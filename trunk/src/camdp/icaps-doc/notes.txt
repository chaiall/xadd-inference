Symbolic Dynamic Programming for Discrete-Continuous MDPs (in abstract)
Exact Solutions to Continuous MDPs with Stochastic Difference Equation Dynamics

Most real-world problems have some form of continuous state
or action:

- resources
- spatial quantity
- time

Previous approaches have been able to exactly solve Continuous MDPs with
fairly restricted reward and transition functions.  Here we show that 
with any algebraic reward function, we can compute an exact, symbolic
solution to MDPs with a variety of complex dynamics specified as
stochastic difference equations (discrete noise, uniform noise, ...).

Additional contribution is the XADD to maintain compactness.

Also the extension to continuous action MDPs.

===

Symbolic Dynamic Programming for Discrete and Continuous State MDPs

Many real-world stochastic planning problems involving resources,
time, or spatial configurations naturally require integer or
continuous state variables in their representation.  While such
problems have been previously formalized as discrete and continuous
state Markov decision processes (DC-MDPs), exact solutions to
DC-MDPs with two or more integer or continuous variables 
have traditionally required extremely restricted dynamics.
In this paper, we formalize DC-MDPs using a stochastic difference
equation transition formalism for the integer and continuous state
variables and introduce an exact symbolic dynamic programming (SDP) 
algorithm that produces a closed-form analytical solution for these
problems.  Aided by the introduction of a novel extended algebraic
decision diagram (XADD) that maintains compact
value functions, we demonstrate that 
our SDP approach can exactly solve DC-MDPs for which no
previous analytical solution has been proposed.

decision-diagrams capable
of representing general algebraic expressions.  We show that symbolic
dynamic programming techniques can be used to compute closed-form
analytical solutions to these 

have integer or
continuous state variables in their representation, e.g., resources,
time, or 

We present an exact symbolic dynamic programming approach for the solution of
.

Previous approaches have been able to exactly solve Continuous MDPs with
fairly restricted reward and transition functions.  Here we show that 
with any algebraic reward function, we can compute an exact, symbolic
solution to MDPs with a variety of complex dynamics specified as
stochastic difference equations

Our approach admits a closed-form analytical solution for 
finite-horizon DCS-MDPs for which no previous analytical solution
has been proposed.




and also in many cases 
with various
guarantees based on the properties of the underlying DCS-MDP.  The
result is an algorithm for finite-horizon planning in factored
CSA-MDPs that we evaluate on an illustrative Mars rover problem.
Empirically, the exact approach outperforms state-of-the-art methods
that require discretization or sampling to perform dynamic programming
with such model expressivity.


Many real-world stochastic planning

Most real-world problems have some form of continuous state
or action:

- resources
- spatial quantity
- time

Previous approaches have been able to exactly solve Continuous MDPs with
fairly restricted reward and transition functions.  Here we show that 
with any algebraic reward function, we can compute an exact, symbolic
solution to MDPs with a variety of complex dynamics specified as
stochastic difference equations (discrete noise, uniform noise, ...).

Additional contribution is the XADD to maintain compactness.

Also the extension to continuous action MDPs.

===

* UAI: Continuous MDP 
  - XADD is a canonical BSP technique
  - start off with example -- inside circle / outside circle
  - n.b., graphical models... x > y, max(x,y), and absolute value are
    piecewise linear... easily integrable; general stochastic variables
    in difference equations 
  - int_{x1,x2} N(x1;mu1,sigma1)*N(x2;mu2,sigma2)*max(x1,x2)
  - systems must simply be **k-Markov** and **causal**: 
      x_{t-1} = f(t,t-1,...,t-k)
    then can always substitute value function with difference
    equation and put integral over stochastic variables on the
    outside
  - so for continuous graphical models, also need
    causal property... guaranteed for decomposing a joint distribution...
      P(a,b,c,d) = P(a|b,c,d) P(b|c,d) P(c|d) P(d)
    need to characterize then what cpfs have closed-form inference
    solutions (that were hard to compute otherwise?)
  - probability in difference equations always seems to be some
    form of conditional noise that is integrated out... does not
    reduce to mean simply because it is multiplied by value function...
    probability over noise variables always sits on outside of value DD
    because not part of value expression... allows it to distribute 
    internally
  - if probability is piecewise linear and XADD is also then 
    integrals are bounded quadratic, otherwise can have arbitrary
    form... need good way to approximate
  - x is a continuous RV
  - need int_x f(x)*p(x) 
  - an integral decomposes over partitions... distributions push down
    with integral bound constraints
  - so paper will currently handle mixtures of difference equations,
    treatment of integration for a separate paper
  - technically limited to delta-like distributions except that
    expressions can be arbitrarily nonlinear and solution is exact
  - another contribution is the XADD
  - future work: continuous actions, continuous noise (piecewise
    linear, nonlinear, and general functions), applications: queueing
    theory, traffic theory... don't need compact solutions and LTI
    systems -- want finite horizon solution rather than functional
    form for *all* future time... need data structures to support
    maximization from control that makes difference equations nonlinear

===

Good questions...

> We plan to solve a TiMDP or a continuous MDP?

Either, or both simultaneously if you want.  

Time can always be *one* of the continuous state variables.  Then you
really need to think of each decision stage in an MDP as a "decision
stage" and not a "fixed-length time step".  In this way, the
decision-stages-to-go view of an MDP remains unchanged and all of the
theory still holds.

One note though: if time is a continuous state variable, you have to
be very careful with how you encode the reward and you likely do not
want to use discounting.  I can explain if you're interested, these
are two lessons I learned the hard way.

> The XADD will be used instead of kd-trees? what is the relation
  between them?

To use kd-trees for continuous MDPs, you would have to put constant
values or expressions at the leaves (a departure from their use in
nearest-neighbor algorithms), but this modification is obvious and
trivial.

kd-trees typically only have axis-aligned decision tests (x > 3 ^ y <
10 ^ z > -5) whereas XADDs can use *any* arithmetic expression for a
test (x^2 + 5yz > 0).  For this simple reason, XADDs represent a
*strict superset* of the functions representable by kd-trees.

Subject to (a) having a canonical form for expressions (similar to
what you did for PADD leaves) and (b) determining whether one
expression implies another (which is undecidable in the general case
but possible in restricted cases) then XADDs are unique, *minimal*,
and canonical for a fixed ordering of tests.

In the context of continuous MDPs...

The most crucial advantage of XADDs for continuous MDPs simply comes
in the expressivity of XADDs: you are heavily restricted in the reward
and transition functions that can be used if you want an exact
solution to a continuous MDP with kd-trees... this is reflected in the
*heavily restricted* set of problems that can be solved exactly by
references [2] and [3] below.

The problem is that Dearden, Littman, et al didn't realize how to
jointly handle *regression* and *maximization* when they relaxed these
restrictions.  And in their integral-based convolution notation I
agree it is not at all obvious.  Dearden does not handle expressive
problems (Mausam has a similar paper with similar restrictions);
Littman approximates because if he didn't, then maximization would
take him outside of the functions representable by kd-trees (i.e., the
decision boundary between two linear functions is linear, but the
decision boundary -- read: decision test -- between two *quadratics*
is itself *quadratic*).

I think Littman knew that he needed to use nonlinear decision tests in
a decision tree to represent exact solutions and for sure he knew how
to do this, but he could not figure out how to do regression... how do
you do convolution of two kd-trees with nonlinear decision tests???
The neat trick of doing a convolution in the intersection of all
rectangles that he currently uses in his paper cannot be applied when
the kd-trees do not have axis-aligned tests.  How to solve this
problem stumped me too.

It took me a long time to notice that if you simply switch to an
*equivalent* difference equation notation, regression is trivial -- it
is just substitution!  The difficult part from an implementation
perspective is that you have to substitute both at the leaves and *in*
the decision tests.  But mathematically the solution is obvious.  I
think the knapsack problem is probably the simplest example.

Furthermore, I note that the use of stochastic difference equations
just requires you to marginalize out noise variables, so stochastic
outcomes do not introduce any additional complications.

I'll say that it's only a matter of time before someone else stumbles
upon the difference equation notation to see that this solution is
obvious, so I think it's rather crucial to get this paper out for
IJCAI or AAAI this year.

The above is a lot to digest, so don't worry if you're still wrapping
your head around all these ideas.  I've been working on this problem
for three years, so I have a head start. :)

In summary though, there are two major contributions for the intended
paper over previous work:

(1) It can exactly solve Continuous MDPs for which no prior analytical
    exact solution existed.

(2) It also gives more compact solutions to the problems previously
    studied (e.g., Mars Rovers) due to the compactness of XADDs.
