\section{Symbolic Dynamic Programming}
\label{sec:sdp}

Symbolic dynamic programming (SDP) \cite{Boutilier_IJCAI_2001} is 
the process of performing dynamic programming via symbolic 
manipulation. In the following sections we present a brief overview
of SDP for zero-sum continuous stochastic games. We refer the reader
to \cite{Sanner_UAI_2011,Zamani_AAAI_2012} for more thorough 
expositions of SDP and its operations.

\subsection{Case Representation}

Symbolic dynamic programming assumes that all symbolic 
functions can be represented in case statement form \cite{Boutilier_IJCAI_2001} as follows:

{\small 
\abovedisplayshortskip=-5pt
\belowdisplayshortskip=0pt
\begin{equation*}
  f = 
    \begin{cases}
      \phi_1: & f_1 \\ 
      \vdots & \vdots\\ 
      \phi_k: & f_k \\ 
    \end{cases} \nonumber
\end{equation*}
}%

Here the $\phi_i$ are logical formulae defined over the state $\vec{x}$ 
and can include arbitrary logical combinations of boolean variables and 
linear inequalities over continuous variables. Each $\phi_i$ is
disjoint from the other $\phi_j$ ($j \neq i$) and may not 
exhaustively cover the state space. Hence, \emph{f} may only be a 
partial function. In this paper we restrict the $f_i$ to be either linear or constant 
functions of the state variable $\vec{x}$. We require $f$ to be continuous.

\subsection{Case Operations}

Unary operations on a case statement \emph{f}, such as scalar 
multiplication $c \cdot f$ where $ c \in \mathbb{R} $ or negation $-f$,
are applied to each $f_i$ ($1 \leq i \leq k$). 

Binary operations on two case statements are executed in two stages.
Firstly, the cross-product of the logical partitions of each case statement 
is taken, producing paired partitions. Finally, the binary operation 
is applied to the resulting paired partitions. The ``cross-sum'' $\oplus$
operation can be performed on two cases in the following manner:

{\small 
\abovedisplayskip=0pt
\belowdisplayskip=0pt
\begin{center}
  \begin{tabular}{r c c c l}
  &
    $\begin{cases}
        \phi_1: \hspace{-1mm} & \hspace{-1mm} f_1  \\ 
        \phi_2: \hspace{-1mm} & \hspace{-1mm} f_2  \\ 
    \end{cases}$
  $\oplus$
  &
  \hspace{-4mm}
    $\begin{cases}
        \psi_1: \hspace{-1mm} & \hspace{-1mm} g_1  \\ 
        \psi_2: \hspace{-1mm} & \hspace{-1mm} g_2  \\ 
    \end{cases}$
  &
  \hspace{-4mm} 
  $ = $
  &
  \hspace{-4mm}
    $\begin{cases}
      \phi_1 \wedge \psi_1: & f_1 + g_1 \\
      \phi_1 \wedge \psi_2: & f_1 + g_2 \\
      \phi_2 \wedge \psi_1: & f_2 + g_1 \\
      \phi_2 \wedge \psi_2: & f_2 + g_2  \\
    \end{cases}$
  \end{tabular}
\end{center}
}%

``cross-subtraction''  $\ominus$ and ``cross-multiplication'' $\otimes$
are defined in a similar manner but with the addition operator replaced
by the subtraction and multiplication operators, respectively.
Some partitions resulting from case operators may be inconsistent and 
are thus removed. 

Minimisation over cases, known as $\casemin$, is defined as:
{\small 
\abovedisplayskip=0pt
\belowdisplayskip=0pt
\begin{center}
  \begin{tabular}{r c c c l}
  &
  \hspace{-7mm} $\casemin \Bigg(
    \begin{cases}
        \phi_1: \hspace{-1mm} & \hspace{-1mm} f_1 \\ 
        \phi_2: \hspace{-1mm} & \hspace{-1mm} f_2 \\ 
    \end{cases}$
  $,$
  &
  \hspace{-4mm}
    $\begin{cases}
        \psi_1: \hspace{-1mm} & \hspace{-1mm} g_1 \\ 
        \psi_2: \hspace{-1mm} & \hspace{-1mm} g_2 \\ 
    \end{cases} \Bigg)$
  &
  \hspace{-4mm} 
  $ = $
  &
  \hspace{-4mm}
    $\begin{cases}
      \phi_1 \wedge \psi_1 \wedge f_1 < g_1    : & \hspace{-2mm} f_1 \\ 
      \phi_1 \wedge \psi_1 \wedge f_1 \geq g_1 : & \hspace{-2mm} g_1 \\ 
      \phi_1 \wedge \psi_2 \wedge f_1 < g_2    : & \hspace{-2mm} f_1 \\ 
      \phi_1 \wedge \psi_2 \wedge f_1 \geq g_2 : & \hspace{-2mm} g_2 \\ 
      \vdots & \vdots
    \end{cases}$
  \end{tabular}
\end{center}
}%

$\casemin$ preserves the linearity of the constraints and the constant 
or linear nature of the $f_i$ and $g_i$. If the $f_i$ or 
$g_i$ are quadratic then the expressions $f_i > g_i$ or 
$f_i \leq g_i$ will be at most univariate quadratic and any such 
constraint can be linearised into a combination of at most two linear 
inequalities by completing the square. 

The $\casemax$ operator, which calculates symbolic case maximisation,
is defined as:
{\small 
\abovedisplayskip=0pt
\belowdisplayskip=0pt
\begin{center}
  \begin{tabular}{r c c c l}
  &
  \hspace{-7mm} $\casemax \Bigg(
    \begin{cases}
        \phi_1: \hspace{-1mm} & \hspace{-1mm} f_1 \\ 
        \phi_2: \hspace{-1mm} & \hspace{-1mm} f_2 \\ 
    \end{cases}$
  $,$
  &
  \hspace{-4mm}
    $\begin{cases}
        \psi_1: \hspace{-1mm} & \hspace{-1mm} g_1 \\ 
        \psi_2: \hspace{-1mm} & \hspace{-1mm} g_2 \\ 
    \end{cases} \Bigg)$
  &
  \hspace{-4mm} 
  $ = $
  &
  \hspace{-4mm}
    $\begin{cases}
      \phi_1 \wedge \psi_1 \wedge f_1 > g_1    : & \hspace{-2mm} f_1 \\ 
      \phi_1 \wedge \psi_1 \wedge f_1 \leq g_1 : & \hspace{-2mm} g_1 \\ 
      \phi_1 \wedge \psi_2 \wedge f_1 > g_2    : & \hspace{-2mm} f_1 \\ 
      \phi_1 \wedge \psi_2 \wedge f_1 \leq g_2 : & \hspace{-2mm} g_2 \\ 
      \vdots & \vdots
    \end{cases}$
  \end{tabular}
\end{center}
}%
$\casemax$ preserves the linearity of the constraints and the constant 
or linear nature of the $f_i$ and $g_i$. 

Other important SDP operations include substitution and continuous maximisation. 
The substitution operation simply takes a set $\theta$ of variables and their
substitutions, e.g. $\theta = \left\{ x'_1/(x_1 + x_2), x'_2/x^2_{1} \text{exp}(x_2) \right\}$,
where the LHS of the / represents the substitution variable and the 
RHS of the / represents the expression that should be substituted in its place.
We can apply the substitution $\theta$ to both non-case functions $f_i$
and case partitions $\phi_i$ as $f_i\theta$ and $\phi_i\theta$, respectively.
Substitution into case statements can therefore be written as:

{\small 
\abovedisplayshortskip=0pt
\belowdisplayshortskip=0pt
\begin{equation*}
  f\theta = 
    \begin{cases}
      \phi_1\theta: & f_1\theta \\ 
      \vdots & \vdots\\ 
      \phi_k\theta: & f_k\theta \\ 
    \end{cases} \nonumber
\end{equation*}
}%

Maximisation can be used to calculate $f_1(\vec{x}, y) = \contmax_{y}f_2(\vec{x}, y) $
with respect to a continuous parameter $y$. This is a complex case operation
whose explanation is beyond the scope of this paper. We refer the reader to 
\cite{Zamani_AAAI_2012} for further elucidation on this operator.

Case statements and their operations are implemented using Extended 
Algebraic Decision Diagrams (XADDs) \cite{Sanner_UAI_2011}.
XADDs provide a compact data structure with which to maintain
compact forms of $Q^{h}(\vec{x}, a_1, a_2)$ and $V^{h}(\vec{x})$. 
XADDs also permit the use of linear constraint feasibility checkers to 
prune unreachable paths in the XADD.

\subsection{SDP for Zero-sum Continuous Stochastic Games}

In this section we show that SDP can be used to calculate optimal closed-form
solutions to zero-sum continuous stochastic games with piecewise
constant rewards and a piecewise linear transition functions. 

We calculate the optimal solution to zero-sum CSGs by first
expressing $R(\vec{x}, a_1, a_2)$, $T(\vec{x}, a_1, a_2, \vec{x}')$, 
$V^0(\vec{x})$ and the $m_{a_{1}}$ as case statements. We can then
compute the optimal solution via the following SDP equations:

{\footnotesize 
\abovedisplayskip=0pt
\belowdisplayskip=0pt
\begin{multline}
\label{eq:sdpdiscqfunc}
  Q^{h}(\vec{x}, a_1, a_2) = R(\vec{x}, a_1, a_2) \quad \oplus \\
  \gamma \otimes \int T(\vec{x}, a_1, a_2, \vec{x}') \otimes V^{h-1}(\vec{x}')\ d\vec{x}' 
\end{multline}
}%

{\footnotesize 
\abovedisplayskip=0pt
\belowdisplayskip=0pt
\begin{multline}
\label{eq:sdpvfunccompact}
  V^{h}(\vec{x}) = \max_{m_{a_{1}} \in \sigma(A_1)} \\ \left[ \left\{ \casemin_{a_2 \in A_2} \left( \sum_{a_1 \in A_1} Q^{h}(\vec{x}, a_1, a_2) \otimes m_{a_{1}} \right)  \right\} \otimes \mathbb{I} \right]
\end{multline}
}%

Equations \eqref{eq:sdpdiscqfunc} and \eqref{eq:sdpvfunccompact} can
be derived from Equations \eqref{eq:csgdiscqfunc} and \eqref{eq:csgvfunccompact}
by replacing all functions by their case statement equivalents and replacing
operations such as $+$, $\times$ and min, by their symbolic equivalents,
$\oplus$, $\otimes$ and $\casemin$, respectively. Equation \eqref{eq:sdpvfunccompact} also includes an ``indicator''
function, $\mathbb{I}$, which serves as a summation constraint 
$\sum_{a_{1} \in A_1} m_{a_{1}} = 1$. The function $\mathbb{I}$
returns 1 when the conjunction of all constraints on each $m_{a_1}$ are satisfied
and $-\infty$, otherwise. That is:

{\small 
\abovedisplayskip=0pt
\belowdisplayskip=0pt
\begin{multline*}
  \mathbb{I} = \\
  {\footnotesize
    \begin{cases}
      \forall a_{1} \in A_1 \hspace{6pt}\left[(m_{a_{1}} \geq 0) \wedge (m_{a_{1}} \leq 1 ) \wedge (\sum m_{a_{1}} = 1) \right]: 1 \\ 
      \forall a_{1} \in A_1 \neg \left[(m_{a_{1}} \geq 0) \wedge (m_{a_{1}} \leq 1 ) \wedge (\sum m_{a_{1}} = 1) \right]: -\infty \\ 
    \end{cases} \nonumber
   }
\end{multline*}
}%

The summation constraint is included to ensure that the subsequent
max operation returns legal values for $m_{a_1}$, since
$\contmax(l, -\infty) = l$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%We also encode the summation constraint $\sum_{a_{1} \in A_1} m_{a_{1}} = 1$
%as an ``indicator'' function $\mathbb{I}$  which returns 1 when the 
%conjunction of all constraints on each $m_{a_1}$ are satisfied
%and $-\infty$, otherwise. That is, 
%
%{\small 
%\abovedisplayskip=0pt
%\belowdisplayskip=0pt
%\begin{equation*}
  %\mathbb{I} = 
    %\begin{cases}
      %\forall a_{1} \in A_1 \left[(m_{a_{1}} \geq 0) \wedge (m_{a_{1}} \leq 1 ) \wedge (\sum m_{a_{1}} = 1) \right]: & 1 \\ 
      %\forall a_{1} \in A_1 \neg \left[(m_{a_{1}} \geq 0) \wedge (m_{a_{1}} \leq 1 ) \wedge (\sum m_{a_{1}} = 1) \right]: & -\infty \\ 
    %\end{cases} \nonumber
%\end{equation*}
%}%
%
%By using the constraint indicator function $\mathbb{I}$
%we ensure that any illegal values of $m_{a_{1}}$ in the resulting
%value function effectively vanish, owing to the identity $\contmax(l, -\infty) = l$.

In Algorithm 1 we present a methodology to calculate the optimal 
$h$-stage-to-go value function, $V^h(\vec{x})$, Equations \eqref{eq:sdpdiscqfunc} and \eqref{eq:sdpvfunccompact}.
For the base case of $h = 0$, we set $V^0(\vec{x})$, expressed in 
case statement form, to zero or to the terminal reward. For all $h > 0$
we apply the previously defined SDP operations in the following steps:
\begin{enumerate}
  \item Prime the Value Function. In line \ref{alg:prime} we set up a 
            substitution $\theta = \left\{ x_1/x'_1, \ldots, x_m/x'_m \right\}$, 
            and obtain $V'^{h} = V^{h}\theta$, the ``next state''.
  \item Discount and add Rewards. We assumed that the reward function
  contains primed variables incorporate it in line \ref{alg:dr1}.
  \item Continuous Marginal Integration. For the continuous state variables in $\vec{x}$
            lines \ref{alg:cmi1} - \ref{alg:cmi2} follow the rules of integration w.r.t. a $\delta$ function 
            \cite{Sanner_UAI_2011} which yields the following symbolic
            substitution: 
            $\int f(x'_j) \otimes \delta\left[ x'_j - g(\vec{z})\right] dx'_j = f(x'_j)\left\{ x'_j/g(\vec{z})\right\}$,
            where $g(\vec{z})$ is a case statement and $\vec{z}$ does not contain $x'_j$.
            The latter operation indicates that any occurrence of $x'_j$ in $f(x'_j)$ is symbolically substituted
            with the case statement $g(\vec{z})$.
  \item Incorporate Agent 1's unknown stochastic strategy. At this point 
            we have calculated $Q^h(\vec{x}, a_1, a_2)$, as shown in
            Equation \eqref{eq:sdpdiscqfunc}. In lines \ref{alg:dm1} - \ref{alg:dm2} 
            we proceed to incorporate each $m_{a_{1}} \in \sigma_1(A_1)$.
            We note that each $m_{a_{1}}$ is a case statement representing
            a free variable.            
  \item Case Minimisation. In lines \ref{alg:cm1} - \ref{alg:cm2} we minimise {\small $Q^h(\vec{x}, a_1, a_2)$ } over all $a_2 \in A_2$ using
            a sequence of $\casemin$ operations.
  \item Incorporate Constraints. In line \ref{alg:constraint} we incorporate constraints on $m_{a_{1}}$ 
            via the constraint indicator function $\mathbb{I}$. The 
            $\casemax$ operator ensures that all case partitions which involve illegal values of $m_{a_{1}}$ are
            mapped to $-\infty$.
  \item Maximisation. In lines \ref{alg:max1} - \ref{alg:max2} we maximise over the $m_{a_{1}} \in \sigma_1(A_1)$ 
            by sequentially applying the maximisation operator as defined 
            previously. At this point we have calculated $V^{h}(\vec{x})$ as shown in 
          Equation \eqref{eq:sdpvfunccompact}.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Algorithm: CSG-VI
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\incmargin{1.5em}
\linesnumbered
\begin{algorithm}[h]
  \vspace{-.5mm}
  \dontprintsemicolon
  \SetKwFunction{remapWithPrimes}{Prime}
  
  \Begin{
  
    $V^0:=0, h:=0$\;
    %\;
    \While{$h < H$}{
      $h := h + 1$\;
      \emph{// Prime the value function}\;
      $Q^h := \remapWithPrimes{$V^{h-1}$} \,$ \; \label{alg:prime}
      \;
       \If{R contains primed variables} {
        $Q^h := R(\vec{x}, a_1, a_2, \vec{x}') \oplus (\gamma \otimes Q^h)$ \; \label{alg:dr1} 
        $\,$
      }
      \emph{// Continuous marginal integration}\;
      \For {all $x'_j$ in $Q^h$}{ \label{alg:cmi1}
        $Q^h := \int Q^h \otimes T(x_j'|a_1, a_2, \vec{x}) d_{x'_j}$\; \label{alg:cmi2}
      }
      \;
%      \If{R does not contain primed variables} {
 %     $Q^h := R(\vec{x}, a_1, a_2) \oplus (\gamma \otimes Q^h)$ \; \label{alg:dr2}     
  %    }
  %    \;
      \emph{// Incorporate Agent 1's unknown stochastic strategy }\;
      \For {all $a_1$ in $A_1$}{ \label{alg:dm1}
        $Q^h := Q^h \oplus \left( Q^h \otimes m_{a_{1}} \right)$ \; \label{alg:dm2}
      }
      \;
      \emph{// Case Minimisation}\;
%      $Q^h := \casemin_{a_2} Q^{h} $\; \label{alg:cm}
      \For {all $a_2$ in $A_2$}{ \label{alg:cm1}
        $Q^h := \casemin(Q^h, a_2) $ \; \label{alg:cm2}
      }
      \;
      \emph{// Incorporate constraints }\;
      $Q^h := \casemin(Q^{h}, \mathbb{I}) $\; \label{alg:constraint}
      \;
      \emph{// Maximisation}\;
      $V^h = Q^h$\;
%      $V^h := \max_{m_{a_{1}}} Q^h $ \; \label{alg:max}
      \For {all $a_1$ in $A_1$}{ \label{alg:max1}
        $V^h := \max\left(V^h, m_{a_{1}} \right)$ \; \label{alg:max2}
      }
      \;
       \emph{// Terminate if early convergence}\;
      \If{$V^h = V^{h-1}$} {
        break 
        $\,$
      }
    }    
    \Return{$(V^h)$} \;
  }
  \caption{
    \footnotesize \texttt{CSG-VI}(CSG, $H$, $\mathbb{I}$) $\longrightarrow$ $(V^h)$ 
    \label{alg:csgvi}
  }
  \vspace{-1mm}
\end{algorithm}
\decmargin{.5em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Algorithm: CSG-VI
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

It can be proved that all SDP operations on case statements result in
a linear piecewise constant $V^h(\vec{x})$, given a linear piecewise constant
$V^0(\vec{x})$ \cite{Sanner_UAI_2011,Zamani_AAAI_2012}.

In the next section we demonstrate how SDP can be used to compute
exact solutions to a variety of zero-sum continuous stochastic games.