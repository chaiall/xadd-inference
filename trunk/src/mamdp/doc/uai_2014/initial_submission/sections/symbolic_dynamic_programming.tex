\section{Symbolic Dynamic Programming}
\label{sec:sdp}

Symbolic dynamic programming (SDP) \cite{Boutilier_IJCAI_2001} is 
the process of performing dynamic programming via symbolic 
manipulation. In the following sections we present a brief overview
of SDP for zero-sum continuous stochastic games. We refer the reader
to \cite{Sanner_UAI_2011,Zamani_AAAI_2012} for more thorough 
expositions of SDP and its operations.

\subsection{Case Representation}

Symbolic dynamic programming assumes that all symbolic 
functions can be represented in case statement form \cite{Boutilier_IJCAI_2001} as follows:

{\small 
\begin{equation*}
  f = 
    \begin{cases}
      \phi_1: & f_1 \\ 
      \vdots & \vdots\\ 
      \phi_k: & f_k \\ 
    \end{cases} \nonumber
\end{equation*}
}%

Here the $\phi_i$ are logical formulae defined over the state $\vec{x}$ 
and can include arbitrary logical combinations of boolean variables and 
linear inequalities over continuous variables. Each $\phi_i$ is
disjoint from the other $\phi_j$ ($j \neq i$) and may not 
exhaustively cover the state space. Hence, \emph{f} may only be a 
partial function. In this paper we restrict the $f_i$ to be either linear or constant 
functions of the state variable $\vec{x}$. We require $f$ to be continuous.

\subsection{Case Operations}

Unary operations on a case statement \emph{f}, such as scalar 
multiplication $c \cdot f$ where $ c \in \mathbb{R} $ or negation $-f$,
are applied to each $f_i$ ($1 \leq i \leq k$). 

Binary operations on two case statements are executed in two stages.
Firstly, the cross-product of the logical partitions of each case statement 
is taken, producing paired partitions. Finally, the binary operation 
is applied to the resulting paired partitions. The ``cross-sum'' $\oplus$
operation can be performed on two cases in the following manner:

{\small 
\begin{center}
  \begin{tabular}{r c c c l}
  &
    $\begin{cases}
        \phi_1: \hspace{-1mm} & \hspace{-1mm} f_1  \\ 
        \phi_2: \hspace{-1mm} & \hspace{-1mm} f_2  \\ 
    \end{cases}$
  $\oplus$
  &
  \hspace{-4mm}
    $\begin{cases}
        \psi_1: \hspace{-1mm} & \hspace{-1mm} g_1  \\ 
        \psi_2: \hspace{-1mm} & \hspace{-1mm} g_2  \\ 
    \end{cases}$
  &
  \hspace{-4mm} 
  $ = $
  &
  \hspace{-4mm}
    $\begin{cases}
      \phi_1 \wedge \psi_1: & f_1 + g_1 \\
      \phi_1 \wedge \psi_2: & f_1 + g_2 \\
      \phi_2 \wedge \psi_1: & f_2 + g_1 \\
      \phi_2 \wedge \psi_2: & f_2 + g_2  \\
    \end{cases}$
  \end{tabular}
\end{center}
}%

``cross-subtraction''  $\ominus$ and ``cross-multiplication'' $\otimes$
are defined in a similar manner but with the addition operator replaced
by the subtraction and multiplication operators, respectively.
Some partitions resulting from case operators may be inconsistent and 
are thus removed. 

Minimisation over cases, known as $\casemin$, is defined as:
{\small 
\begin{center}
  \begin{tabular}{r c c c l}
  &
  \hspace{-7mm} $\casemin \Bigg(
    \begin{cases}
        \phi_1: \hspace{-1mm} & \hspace{-1mm} f_1 \\ 
        \phi_2: \hspace{-1mm} & \hspace{-1mm} f_2 \\ 
    \end{cases}$
  $,$
  &
  \hspace{-4mm}
    $\begin{cases}
        \psi_1: \hspace{-1mm} & \hspace{-1mm} g_1 \\ 
        \psi_2: \hspace{-1mm} & \hspace{-1mm} g_2 \\ 
    \end{cases} \Bigg)$
  &
  \hspace{-4mm} 
  $ = $
  &
  \hspace{-4mm}
    $\begin{cases}
      \phi_1 \wedge \psi_1 \wedge f_1 < g_1    : & \hspace{-2mm} f_1 \\ 
      \phi_1 \wedge \psi_1 \wedge f_1 \geq g_1 : & \hspace{-2mm} g_1 \\ 
      \phi_1 \wedge \psi_2 \wedge f_1 < g_2    : & \hspace{-2mm} f_1 \\ 
      \phi_1 \wedge \psi_2 \wedge f_1 \geq g_2 : & \hspace{-2mm} g_2 \\ 
      \vdots & \vdots
    \end{cases}$
  \end{tabular}
\end{center}
}%

$\casemin$ preserves the linearity of the constraints and the constant 
or linear nature of the $f_i$ and $g_i$. If the $f_i$ or 
$g_i$ are quadratic then the expressions $f_i > g_i$ or 
$f_i \leq g_i$ will be at most univariate quadratic and any such 
constraint can be linearised into a combination of at most two linear 
inequalities by completing the square. 

The $\casemax$ operator, which calculates symbolic case maximisation,
is defined as:
{\small 
\begin{center}
  \begin{tabular}{r c c c l}
  &
  \hspace{-7mm} $\casemax \Bigg(
    \begin{cases}
        \phi_1: \hspace{-1mm} & \hspace{-1mm} f_1 \\ 
        \phi_2: \hspace{-1mm} & \hspace{-1mm} f_2 \\ 
    \end{cases}$
  $,$
  &
  \hspace{-4mm}
    $\begin{cases}
        \psi_1: \hspace{-1mm} & \hspace{-1mm} g_1 \\ 
        \psi_2: \hspace{-1mm} & \hspace{-1mm} g_2 \\ 
    \end{cases} \Bigg)$
  &
  \hspace{-4mm} 
  $ = $
  &
  \hspace{-4mm}
    $\begin{cases}
      \phi_1 \wedge \psi_1 \wedge f_1 > g_1    : & \hspace{-2mm} f_1 \\ 
      \phi_1 \wedge \psi_1 \wedge f_1 \leq g_1 : & \hspace{-2mm} g_1 \\ 
      \phi_1 \wedge \psi_2 \wedge f_1 > g_2    : & \hspace{-2mm} f_1 \\ 
      \phi_1 \wedge \psi_2 \wedge f_1 \leq g_2 : & \hspace{-2mm} g_2 \\ 
      \vdots & \vdots
    \end{cases}$
  \end{tabular}
\end{center}
}%

Other important SDP operations include substitution and continuous maximisation. 
The substitution operation simply takes a set $\theta$ of variables and their
substitutions, e.g. $\theta = \left\{ x'_1/(x_1 + x_2), x'_2/x^2_{1} \text{exp}(x_2) \right\}$,
where the LHS of the / represents the substitution variable and the 
RHS of the / represents the expression that should be substituted in its place.
We can apply the substitution $\theta$ to both non-case functions $f_i$
and case partitions $\phi_i$ as $f_i\theta$ and $\phi_i\theta$, respectively.
Substitution into case statements can therefore be written as:

{\small 
\begin{equation*}
  f\theta = 
    \begin{cases}
      \phi_1\theta: & f_1\theta \\ 
      \vdots & \vdots\\ 
      \phi_k\theta: & f_k\theta \\ 
    \end{cases} \nonumber
\end{equation*}
}%

Maximisation can be used to calculate $f_1(\vec{x}, y) = \contmax_{y}f_2(\vec{x}, y) $
with respect to a continuous parameter $y$. This is a complex case operation
whose explanation is beyond the scope of this paper. We refer the reader to 
\cite{Zamani_AAAI_2012} for further elucidation on this operator.

Case statements and their operations are implemented using Extended 
Algebraic Decision Diagrams (XADDs) \cite{Sanner_UAI_2011}.
XADDs provide a compact data structure with which to maintain
compact forms of $Q^{h}(\vec{x}, a_1, a_2)$ and $V^{h}(\vec{x})$. 
XADDs also permit the use of linear constraint feasibility checkers to 
prune unreachable paths in the XADD.

\subsection{SDP for Continuous Stochastic Games}

In this section we show that SDP can be used to calculate optimal closed-form
solutions to zero-sum continuous stochastic games with piecewise
constant rewards and a piecewise linear transition functions. 

In Algorithm 1 we present a methodology to calculate the optimal 
$h$-stage-to-go value function, $V^h(\vec{x})$, via Equations
\eqref{eq:csgdiscqfunc} and \eqref{eq:csgvfunccompact}, using symbolic
dynamic programming. We begin by expressing the reward and transition
functions, $R(\vec{x}, a_1, a_2)$ and $T(\vec{x}, a_1, a_2, \vec{x}')$, respectively, as case statements. We also encode the summation constraint $\sum_{a_{1} \in A_1} m_{a_{1}} = 1$
as an ``indicator'' function $\mathbb{I}$  which returns 1 when the 
conjunction of all constraints on each $m_{a_1}$ are satisfied
and $-\infty$, otherwise. That is, 

{\small 
\begin{equation*}
  \mathbb{I} = 
    \begin{cases}
      \forall a_{1} \in A_1 \left[(m_{a_{1}} \geq 0) \wedge (m_{a_{1}} \leq 1 ) \right]: & 1 \\ 
      \forall a_{1} \in A_1 \neg \left[(m_{a_{1}} \geq 0) \wedge (m_{a_{1}} \leq 1 ) \right]: & -\infty \\ 
    \end{cases} \nonumber
\end{equation*}
}%

By using the constraint indicator function $\mathbb{I}$
we ensure that any illegal values of $m_{a_{1}}$ in the resulting
value function effectively vanish, owing to the identity $\contmax(l, -\infty) = l$.

For the base case of $h = 0$, we set $V^0(\vec{x})$, expressed in 
case statement form, to zero or to the terminal reward. For all $h > 0$
we apply the previously defined SDP operations in the following steps:
\begin{enumerate}
  \item Prime the Value Function. In line \ref{alg:prime} we set up a 
            substitution $\theta = \left\{ x_1/x'_1, \ldots, x_m/x'_m \right\}$, 
            and obtain $V'^{h} = V^{h}\theta$, the ``next state''.
  \item Discount and add Rewards. If the reward function contains primed state variables
          then it is incorporated in line \ref{alg:dr1}, otherwise it is incorporated in line \ref{alg:dr2}.            
  \item Continuous Marginal Integration. For the continuous state variables in $\vec{x}$
            lines \ref{alg:cmi1} - \ref{alg:cmi2} follow the rules of integration w.r.t. a $\delta$ function 
            \cite{Sanner_UAI_2011} which yields the following symbolic
            substitution: 
            $\int f(x'_j) \otimes \delta\left[ x'_j - g(\vec{z})\right] dx'_j = f(x'_j)\left\{ x'_j/g(\vec{z})\right\}$,
            where $g(\vec{z})$ is a case statement and $\vec{z}$ does not contain $x'_j$.
            The latter operation indicates that any occurrence of $x'_j$ in $f(x'_j)$ is symbolically substituted
            with the case statement $g(\vec{z})$.
  \item Discrete Marginalisation. At this point we have calculated $Q^h(\vec{x}, a_1, a_2)$, as shown in
            Equation \eqref{eq:csgdiscqfunc}. In lines \ref{alg:dm1} - \ref{alg:dm2} we proceed to marginalise over the actions $a_1 \in A_1$.
            Here we note that $m_{a_{1}} \in \sigma_1(A_1)$.
  \item Case Minimisation. In line \ref{alg:cm} we minimise {\small $Q^h(\vec{x}, a_1, a_2)$ } over all $a_2 \in A_2$ using
            a sequence of $\casemin$ operations.
  \item Incorporate Constraints. In line \ref{alg:constraint} we incorporate constraints on $m_{a_{1}}$ 
            via the constraint indicator function $\mathbb{I}$. At this stage
            all case partitions which involve illegal values of $m_{a_{1}}$ are
            mapped to $-\infty$.
  \item Maximisation. In line \ref{alg:max} we maximise over the $m_{a_{1}} \in \sigma_1(A_1)$ 
            by sequentially applying the maximisation operator as defined 
            previously. At this point we have calculated $V^{h}(\vec{x})$ as shown in 
          Equation \eqref{eq:csgvfunccompact}.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Algorithm: CSG-VI
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\incmargin{1.5em}
\linesnumbered
\begin{algorithm}[h]
  \vspace{-.5mm}
  \dontprintsemicolon
  \SetKwFunction{remapWithPrimes}{Prime}
  
  \Begin{
  
    $V^0:=0, h:=0$\;
    \;
    \While{$h < H$}{
      $h := h + 1$\;
      \emph{// Prime the value function}\;
      $Q^h := \remapWithPrimes{$V^{h-1}$} \,$ \; \label{alg:prime}
      \;
       \If{R contains primed variables} {
        $Q^h := R(\vec{x}, a_1, a_2, \vec{x}') \oplus (\gamma \otimes Q^h)$ \; \label{alg:dr1} 
        $\,$
      }
      \emph{// Continuous marginal integration}\;
      \For {all $x'_j$ in $Q^h$}{ \label{alg:cmi1}
        $Q^h := \int Q^h \otimes T(x_j'|a_1, a_2, \vec{x}) d_{x'_j}$\; \label{alg:cmi2}
      }
      \;
      \If{R does not contain primed variables} {
      $Q^h := R(\vec{x}, a_1, a_2) \oplus (\gamma \otimes Q^h)$ \; \label{alg:dr2}     
      }
      \;
      \emph{// Discrete marginalisation}\;
      \For {all $a_1$ in $A_1$}{ \label{alg:dm1}
        $Q^h := Q^h \oplus \left( Q^h \otimes m_{a_{1}} \right)$ \; \label{alg:dm2}
      }
      \;
      \emph{// Case Minimisation}\;
      $Q^h := \casemin_{a_2} Q^{h} $\; \label{alg:cm}
      \;
      \emph{// Incorporate constraints }\;
      $Q^h := Q^{h} \otimes \mathbb{I} $\; \label{alg:constraint}
      \emph{// Maximisation}\;
      $V^h := \max_{m_{a_{1}}} Q^h $ \; \label{alg:max}
      \;
       \emph{// Terminate if early convergence}\;
      \If{$V^h = V^{h-1}$} {
        break 
        $\,$
      }
    }    
    \Return{$(V^h)$} \;
  }
  \caption{
    \footnotesize \texttt{CSG-VI}(CSG, $H$, $\mathbb{I}$) $\longrightarrow$ $(V^h)$ 
    \label{alg:csgvi}
  }
  \vspace{-1mm}
\end{algorithm}
\decmargin{.5em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Algorithm: CSG-VI
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

It can be proved that all SDP operations on case statements result in
a linear piecewise constant $V^h(\vec{x})$, given a linear piecewise constant
$V^0(\vec{x})$ \cite{Sanner_UAI_2011,Zamani_AAAI_2012}.

In the next section we demonstrate how SDP can be used to compute
exact solutions to a variety of zero-sum continuous stochastic games.