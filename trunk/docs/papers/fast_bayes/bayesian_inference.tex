
\section{Bayesian inference on graphical models with piecewise distributions}
\label{sec:inference_piecewise_models}
\paradot{Inference} Although the inference method that will be presented can be used on 
any graphical model, for simplicity, we focus on \emph{Bayesian inference} 
on Bayesian networks factorized as: 
\begin{equation}
\label{e:posterior}
pr(\boldsymbol\theta | \, d_1, \ldots, d_n) 
\propto
pr(\boldsymbol\theta, \, d_1, \ldots, d_n) 
= pr(\boldsymbol\theta) \cdot \prod_{i=1}^{n} pr(d_i | \boldsymbol\theta)
\end{equation} 
where $\boldsymbol\theta$ is a parameter vector and $d_i$ are observed data points. 
A typical inference task is the computation of the probability of a new data $d_{n+1}$ from the observed points:
\begin{equation}
\label{e:prob.outcome}
pr(d_{n+1} | \, d_1, \ldots, d_n) = 
\int_{\boldsymbol\theta} pr(d_{n+1} | \, \boldsymbol\theta) pr(\boldsymbol\theta | \, d_1, \ldots, d_n) d\boldsymbol\theta 
\end{equation}

\paradot{Piecewise models} 
We are interested in the inference on models where prior/likelihood distributions are piecewise.
A function $f(\bvec{x})$ is $n$-piece \emph{piecewise} if it can be represented in form of Equation~(\ref{e:piecewise}) where $\phi_1$ to $\phi_m$ are mutually exclusive and jointly exhaustive Boolean functions (constraints) 
that partition the space of variables $\bvec{x}$. If for a particular variable assignment $\bvec{x}_0$, a constraint $\phi_i(\bvec{x}_0)$ is satisfied, then by definition, the function returns the value of its $i$-th \emph{sub-function}: $f(\bvec{x}_0) = f_i(\bvec{x}_0)$.  
In this case, it is said that sub-function $f_i$ is \emph{activated} by assignment $\bvec{x}_0$.
\begin{equation}
\label{e:piecewise}
f(\bvec{x}) = 
{\footnotesize
\begin{cases}
\phi_1(\bvec{x}) : & f_1(\bvec{x})\\
\vdots\\
\phi_m(\bvec{x}) : & f_m(\bvec{x})
\end{cases}
}%end footnote size
\end{equation}
In the implementation of our proposed algorithm, the constraints are restricted to linear/quadratic (in)equalities while sub-functions are polynomials in which real-exponents (except -1) are allowed.
However, in theory, the algorithm can be applied to any family of piecewise models in which the constraints are solvable and sub-functions (and their products) are integrable. 

%In Example~\ref{example:pref}, BPPL is a \emph{piecewise constant} model in the sense that both prior and likelihood models are piecewise and their corresponding \emph{sub-functions} ($f_i$) are constant. A more general piecewise model where the sub-functions are not constant is presented in  Section~\ref{sect:experiment}. 

\paradot{Complexity of piecewise modes}


If in the model of equation \ref{e:posterior}, the prior $pr(\boldsymbol\theta)$ 
is an $L$-piece distribution and each of the $n$ likelihoods is a piecewise function with number of partitions  bound by $M$, 
then the joint distribution is a piecewise function with number of partitions bound by $LM^n$ (therefore, $O(M^n)$).
The reason, as clarified by the following simple formula,   
is that the number of partitions in the product of two piecewise functions is bound by the product of their number of partitions:\footnote{
If pruning potential inconsistent (infeasible) constraint is possible
(i.e.\ by \emph{linear constraint solvers} for linear constrains) and the imposed extra costs are justified,
the number of partitions can be less.
}%endfootnote
\[
{\footnotesize
\begin{cases}
\phi_1(\bvec{x}) : & f_1(\bvec{x})\\
\phi_2(\bvec{x}) : & f_2(\bvec{x})
\end{cases}
\otimes
\begin{cases}
\psi_1(\bvec{x}) : & g_1(\bvec{x})\\
\psi_2(\bvec{x}) : & g_2(\bvec{x})
\end{cases}
=
\begin{cases}
\phi_1(\bvec{x}) \wedge \psi_1(\bvec{x}) : & f_1(\bvec{x}) g_1(\bvec{x})\\
\phi_1(\bvec{x}) \wedge \psi_2(\bvec{x}) : & f_1(\bvec{x}) g_2(\bvec{x})\\
\phi_2(\bvec{x}) \wedge \psi_1(\bvec{x}) : & f_2(\bvec{x}) g_1(\bvec{x})\\
\phi_2(\bvec{x}) \wedge \psi_2(\bvec{x}) : & f_2(\bvec{x}) g_2(\bvec{x})
\end{cases}
}%end footnote size
\]

\paradot{Exact inference on piecewise models}
In theory, closed form inference on piecewise models (at least piecewise polynomials) with linear constraints is possible \cite{Sanner:12}.
In practice, however, such symbolic methods rapidly become intractable.
For example, by rule of thumb, a few observed data points with simple piecewise likelihood models lead to
joint/posterior distributions with tens of partitions.
For an inference problem formalized by equation~\ref{e:prob.outcome}, 
the number of required marginalization integrals is $|\boldsymbol\theta|$ (dimensionality of the parameter space).
Although that the family of piecewise polynomials are closed under integration, even a single \emph{closed form} integral over such a posterior can lead to a function with thousands of pieces \cite{Sanner:12}.   
   
\paradot{Approximate inference on piecewise modes} 
The second best option is to seek \emph{asymptotically unbiased} inference methods. 
The existing such tools are based on sampling.
As an instance, given a set of $S$ samples (particles) $\{\boldsymbol\theta^{(1)}, \ldots, \boldsymbol\theta^{(S)}\}$ taken from a posterior $pr(\boldsymbol\theta | \, d_1, \ldots, d_n)$, 
inference task of equation~\ref{e:prob.outcome} can be approximated by: 
$\frac{1}{S} \sum_{i=1}^S pr(d_{n+1} | \, \boldsymbol\theta^{(i)})$.
Three widely used sampling methods are as follows:

\emph{Rejection sampling}:
Let $p(\bvec{x})$ and $q(\bvec{x})$ be two distributions 
such that direct sampling from them is respectively hard and easy
and
$p(\bvec{x})/q(\bvec{x})$ is bound by a constant $c>0$. 
To take a sample from $p(\bvec{x})$ by means of \emph{rejection sampling}, 
a sample $\bvec{s}$ is taken from $q(\bvec{x})$ and accepted with probability $p(\bvec{s}) / c q(\bvec{s})$, 
otherwise it is rejected and the process is repeated. 
If $c$ is too small, the speed of this algorithm is low since often lots of samples are taken till one is accepted.

\emph{Metropolis-Hastings (MH)}:
To generate a new sample $\bvec{x}^{(t)}$ of a distribution $p(\bvec{x})$ given a previously taken sample $\bvec{x}^{(t-1)}$, 
firstly, a sample $\bvec{x}'$ is taken 
from a symmetric \emph{proposal density} $q(\bvec{x} |\, \bvec{x}^{(t-1)})$ 
from which samples can be taken efficiently 
(often a \emph{Gaussian} centered at $\bvec{x}^{(t-1)}$). 
With probability $\min \big(1, p(\bvec{x}')/p(\bvec{x}^{(t-1)}) \big)$, 
$\bvec{x}'$ is accepted as the next sample ($\bvec{x}^{(t)} \leftarrow \bvec{x}'$), otherwise, $\bvec{x}^{(t)} \leftarrow \bvec{x}^{(t-1)}$. 
Choosing a `right' \emph{proposal} is problem-dependent and requires extra tunings. 


\emph{Gibbs sampling}:
In this method, to generate a new sample for variables $\bvec{x} = (x_1, \ldots, x_N)$, 
each one of them is sampled conditioned on the others being instantiated with their last sampled values:
$x_i \sim p(x_i | \bvec{x}_{-i})$. 
Therefore,  
computation of $N$ univariate \emph{cumulative distribution functions} (CDFs) 
(one for each $p(x_i | \, \bvec{x}_{-i})$) as well as their inverse functions is required which can be quite time consuming. 
Due to this shortcoming, in practice, Gibbs sampling can be prohibitively expensive.   

\paradot{Gibbs sampling from a piecewise model}
CDF of a piecewise (univariate) function is clearly the summation of the CDFs of each of its partitions.
Therefore Gibbs sampling does not get around the difficulties introduced by the exponential growth of posterior distributions in Bayesian models.\footnote{
The reason is that the number of pieces in a function of form $p(x_i |\, \bvec{x}_{-i})$ is still in $O(M^n)$. 
In practice, many univariate partitions might be empty (i.e.\ associated with infeasible constraints) but in general, detecting them is not such easier than computing CDFs.
} %end footnote  
%Therefore, according to the previous arguments, using Gibbs to take a single sample vector from the posterior distribution of an $N$ dimensional space of a Bayesian inference model with an $L$-piece prior and $n$ $M$-piece likelihood functions, up to $LM^nN$ CDFs (and CDF$^{-1}$s) should be computed.
%With the aim of making Gibbs sampling more scalable, in Section~\ref{sect:mix} we introduce a technique using which this number can be reduced to {\color{red}an order of} $LM(N+n)$.
however,  we are going to propose an equivalent transformation of the model that makes computation linear in the amount of data.
