%-------------------------------%
% Biblography
%------------------------------%
\RequirePackage{filecontents}        % loading package filecontents
% writing file \jobname.bib, for example mb-bibtex.bib.

\begin{filecontents*}{\jobname.bib}

%@conference{,
%[A]
%[B]
@book{Bishop:06,
  title={Pattern recognition and machine learning},
  author={Christopher M Bishop},
  year={2006},
  publisher={Springer New York}
}

%[D]
@inproceedings{Das:08,
  title={Adapting to a Market Shock: Optimal Sequential Market-Making.},
  author={Das, Sanmay and Magdon-Ismail, Malik},
  booktitle={NIPS},
  pages={361--368},
  year={2008}
}
%[E]
%[F]
%[G]
@book{Gelman:13},
  title={Bayesian data analysis},
  author={Gelman, Andrew and Carlin, John B and Stern, Hal S and Dunson, David B and Vehtari, Aki and Rubin, Donald B},
  year={2013},
  publisher={CRC press}
}
%[H]
%[I]
%[J]
%[K]
@book{Keeney:93,
  title={Decisions with multiple objectives: preferences and value trade-offs},
  author={Keeney, Ralph L and Raiffa, Howard},
  year={1993},
  publisher={Cambridge university press.},
% Original publication, Wiley, New York, 
  year={1976}
}

@book{Koller:09,
  title={Probabilistic graphical models: principles and techniques},
  author={Koller, Daphne and Friedman, Nir},
  year={2009},
  publisher={The MIT Press}
}
%[L]
%[N]
%[O]
%[P]
@inproceedings{Poon:06,
  title={Sound and efficient inference with probabilistic and deterministic dependencies},
  author={Poon, Hoifung and Domingos, Pedro},
  booktitle={AAAI},
  volume={6},
  pages={458--463},
  year={2006}
}
%[Q]
%[R]
@article{Roberts:97,
  title={Weak convergence and optimal scaling of random walk Metropolis algorithms},
  author={Roberts, Gareth O. and Gelman, Andrew and Gilks, Walter R.},
  journal={The annals of applied probability},
  volume={7},
  number={1},
  pages={110--120},
  year={1997},
  publisher={Institute of Mathematical Statistics}
}
%[S]
@conference{Sanner:12,
author = {Scott Sanner and Ehsan Abbasnejad},
title = {Symbolic Variable Elimination for Discrete and Continuous Graphical Models},
booktitle = {AAAI},
year = {2012}
}
@article{Shogren:00,
  title={Preference learning in consecutive experimental auctions},
  author={Shogren, Jason F and List, John A and Hayes, Dermot J},
  journal={American Journal of Agricultural Economics},
  volume={82},
  number={4},
  pages={1016--1021},
  year={2000},
  publisher={Oxford University Press}
}

%[T]
%[U]
%[V]
%[W]
%[X]
%[Y]
%[Z]

@book{Gelman:13},
  title={Bayesian data analysis},
  author={Gelman, Andrew and Carlin, John B and Stern, Hal S and Dunson, David B and Vehtari, Aki and Rubin, Donald B},
  year={2013},
  publisher={CRC press}
}


@article{chib1995understanding,
  title={Understanding the metropolis-hastings algorithm},
  author={Chib, Siddhartha and Greenberg, Edward},
  journal={The American Statistician},
  volume={49},
  number={4},
  pages={327--335},
  year={1995},
  publisher={Taylor \& Francis Group}
}


@book{robert2004monte,
  title={Monte Carlo statistical methods},
  author={Robert, Christian P and Casella, George},
  volume={319},
  year={2004},
  publisher={Citeseer}
}


@inproceedings{lerner2002monitoring,
  title={Monitoring a complex physical system using a hybrid dynamic bayes net},
  author={Lerner, Uri and Moses, Brooks and Scott, Maricia and McIlraith, Sheila and Koller, Daphne},
  booktitle={Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence},
  pages={301--310},
  year={2002},
  organization={Morgan Kaufmann Publishers Inc.}
}


@inproceedings{brahma2012bayesian,
  title={A Bayesian market maker},
  author={Brahma, Aseem and Chakraborty, Mithun and Das, Sanmay and Lavoie, Allen and Magdon-Ismail, Malik},
  booktitle={Proceedings of the 13th ACM Conference on Electronic Commerce},
  pages={215--232},
  year={2012},
  organization={ACM}
}


@article{yu2012solving,
  title={Solving inverse problems with piecewise linear estimators: from Gaussian mixture models to structured sparsity},
  author={Yu, Guoshen and Sapiro, Guillermo and Mallat, St{\'e}phane},
  journal={Image Processing, IEEE Transactions on},
  volume={21},
  number={5},
  pages={2481--2499},
  year={2012},
  publisher={IEEE}
}

\end{filecontents*}

\documentclass{article} % For LaTeX2e
\usepackage{nips14submit_e,times}%{nips13submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsthm} %for theorems, examples, etc.
\usepackage{amsfonts} %for matbb font
\usepackage{graphicx}
\usepackage{caption} %for graphics
\usepackage{subcaption} %for graphics
\usepackage{amsmath} %for cases

\usepackage{algorithmic} %for algorithms
% Import an algorithm formatting package
\usepackage[vlined,algoruled,titlenumbered,noend]{algorithm2e}
%\documentstyle[nips13submit_09,times,art10]{article} % For LaTeX 2.09
%\usepackage{algpseudocode} %new tpr
%\usepackage{algorithm} %new tpr

\usepackage{verbatim} %for commenting

\newcommand{\denselist}{\itemsep 0pt\partopsep 0pt}

%\newenvironment{proof}{{\noindent\bf Proof.}}\qed%{\hspace*{\fill}\ensuremath{\diamondsuit\quad}%{\vskip 1ex}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{example}{Example}
\def\fexample#1#2#3{\vspace{-1ex}\begin{example}[#2]\label{#1}\rm #3
\hspace*{\fill} $\diamondsuit\quad$ \end{example}\vspace{-2ex} }
\newcommand{\tuple}[1] {\langle #1 \rangle}
\newcommand{\bvec}[1]{\textbf{#1}}
\newcommand{\indicator}{\mathbb{I}}%{I\!\!I}

\def\eqvsp{}  \newdimen\paravsp  \paravsp=1.3ex
\def\paradot#1{\vspace{\paravsp plus 0.5\paravsp minus 0.5\paravsp}\noindent{\bf\boldmath{#1.}}}

\def\lgap{3.2mm}% little gap. by Hadi
\newcommand{\svdots}{\vspace{-\lgap}.\vspace{-\lgap}\\.\vspace{-\lgap}\\.} %small vdots by Hadi

\title{
%Blocked Gibbs Sampling on Piecewise Bayesian Networks
Fast Bayesian Inference in\\ Piecewise Graphical Models
}


\author{
%David S.~Hippocampus\thanks{ Use footnote for providing further information
%about author (webpage, alternative address)---\emph{not} for acknowledging
%funding agencies.} \\
%Department of Computer Science\\
%Cranberry-Lemon University\\
%Pittsburgh, PA 15213 \\
%\texttt{hippo@cs.cranberry-lemon.edu} \\
%\And
%Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
(if needed)\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

\begin{abstract}
%In many real world probabilistic inference tasks such as \emph{preference learning}, predicting traders behaviors in a financial market, etc. prior/likelihood models are intrinsically piecewise. This work shows that Gibbs sampling can effectively be used on piecewise linear/quadratic polynomial distributions. It happens that in a Bayesian model,  the number of partitions in the \emph{posterior} distribution can grow exponentially if the \emph{likelihood} is piecewise. A major contribution is to show that such networks can be regarded to as mixture models leading to a computation reduction which is exponential to linear in the amount of data. We provide a Blocked Gibbs sampling algorithm which is quite effective on such models. The empirical results show that the performance of this sampling method is order of magnitudes better than candidate algorithms for asymptotically unbiased reasoning. The generalization of the proposed sampling method to any piecewise distribution with a huge number of partitions is straight forward. 
\input abstract
\end{abstract}


\input intro



\input bayesian_inference


\input mixture

\input experiment

\input conclusion


%\subsubsection*{References}
\small{
\bibliographystyle{alpha}%{plain}%{plainnat}  % needs package natbib
\bibliography{\jobname}       % uses \jobname.bib, according to \jobname.tex
}



\end{document}
