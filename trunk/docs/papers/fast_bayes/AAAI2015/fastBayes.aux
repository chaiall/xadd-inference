\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{sanner:aistats10}
\citation{Shogren:00}
\citation{Keeney:93}
\@writefile{toc}{\contentsline {section}{Introduction}{1}{section*.1}}
\newlabel{example:pref}{{1}{1}{Bayesian pairwise preference learning (BPPL)}{example.1}{}}
\newlabel{e:pref1likelihood}{{1}{1}{Bayesian pairwise preference learning (BPPL)}{equation.0.1}{}}
\citation{Sanner:12}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:pref}{{1b}{2}{\relax }{figure.caption.2}{}}
\newlabel{sub@fig:pref}{{b}{2}{\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \relax \fontsize  {9}{10}\selectfont  (a) Graphical model for BPPL problem in Example \ref  {example:pref}. (b) A 2D instance of Example \ref  {example:pref}: (i) An (unnormalized) prior uniform in a hyperrectangle with center (0,0). (ii) Likelihood model $pr(\textbf  {a}_1 \succ \textbf  {b}_1 | \tmspace  +\thinmuskip {.1667em} \boldsymbol  \theta )$ and (iii) $pr(\textbf  {a}_2 \succ \textbf  {b}_2 | \tmspace  +\thinmuskip {.1667em} \boldsymbol  \theta )$ (as in equation\nobreakspace  {}\ref  {e:pref1likelihood}) where $\textbf  {a}_1 = (5, 3)$, $\textbf  {b}_1 = (6, 2)$, $\textbf  {a}_2 = \textbf  {a}_1$ and $\textbf  {b}_2 = (6, 3)$. (iv) A piecewise function proportional to the posterior distribution.\relax }}{2}{figure.caption.2}}
\newlabel{fig:pref-up-down}{{1}{2}{\footnotesize (a) Graphical model for BPPL problem in Example \ref {example:pref}. (b) A 2D instance of Example \ref {example:pref}: (i) An (unnormalized) prior uniform in a hyperrectangle with center (0,0). (ii) Likelihood model $pr(\bvec {a}_1 \succ \bvec {b}_1 | \, \boldsymbol \theta )$ and (iii) $pr(\bvec {a}_2 \succ \bvec {b}_2 | \, \boldsymbol \theta )$ (as in equation~\ref {e:pref1likelihood}) where $\bvec {a}_1 = (5, 3)$, $\bvec {b}_1 = (6, 2)$, $\bvec {a}_2 = \bvec {a}_1$ and $\bvec {b}_2 = (6, 3)$. (iv) A piecewise function proportional to the posterior distribution.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{Bayesian inference on graphical models with piecewise distributions}{2}{section*.3}}
\newlabel{sec:inference_piecewise_models}{{}{2}{Bayesian inference on graphical models with piecewise distributions}{section*.3}{}}
\newlabel{e:posterior}{{3}{2}{Bayesian inference on graphical models with piecewise distributions}{equation.0.3}{}}
\newlabel{e:prob.outcome}{{4}{2}{Bayesian inference on graphical models with piecewise distributions}{equation.0.4}{}}
\newlabel{e:piecewise}{{5}{2}{Bayesian inference on graphical models with piecewise distributions}{equation.0.5}{}}
\newlabel{fig:naive}{{2a}{3}{\relax }{figure.caption.5}{}}
\newlabel{sub@fig:naive}{{a}{3}{\relax }{figure.caption.5}{}}
\newlabel{fig:naive.mix}{{2b}{3}{\relax }{figure.caption.5}{}}
\newlabel{sub@fig:naive.mix}{{b}{3}{\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \relax \fontsize  {9}{10}\selectfont  (a) A Bayesian inference model with parameter (vector) $\boldsymbol  \theta $ and data points $d_1$ to $d_n$. (b) A mixture model with parameter (vector) $\boldsymbol  \theta $ and data points $d_1$ to $d_n$ \relax }}{3}{figure.caption.5}}
\@writefile{toc}{\contentsline {section}{Piecewise models as mixture models}{3}{section*.4}}
\newlabel{sect:mix}{{}{3}{Piecewise models as mixture models}{section*.4}{}}
\newlabel{e:expanded}{{6}{3}{Piecewise models as mixture models}{equation.0.6}{}}
\newlabel{e:aaaax}{{8}{3}{Piecewise models as mixture models}{equation.0.8}{}}
\newlabel{pro:discrete}{{1}{3}{}{proposition.1}{}}
\newlabel{e:piecewise.likelihood22}{{9}{3}{}{equation.0.9}{}}
\newlabel{e:2rel22}{{10}{3}{}{equation.0.10}{}}
\newlabel{e:2rel222}{{11}{3}{}{equation.0.11}{}}
\citation{Poon:06}
\citation{Das:08}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \relax \fontsize  {9}{10}\selectfont  A piecewise joint distribution of $(\theta _1, \theta _2)$ partitioned by bi-valued linear constraints. In the side specified by each arrow its associated auxiliary variable $k_i$ is 1 otherwise 2. A Gibbs sampler started from an initial point $O = (\theta _1^{(0)}, \theta _2^{(0)})$, is trapped in an initial partition (A) where $k_1 = k_2 = k_3 = 1$ and $k_4 = 2$. \relax }}{4}{figure.caption.7}}
\newlabel{fig:simple.example}{{3}{4}{\footnotesize A piecewise joint distribution of $(\theta _1, \theta _2)$ partitioned by bi-valued linear constraints. In the side specified by each arrow its associated auxiliary variable $k_i$ is 1 otherwise 2. A Gibbs sampler started from an initial point $O = (\theta _1^{(0)}, \theta _2^{(0)})$, is trapped in an initial partition (A) where $k_1 = k_2 = k_3 = 1$ and $k_4 = 2$. \relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{Deterministic dependencies and blocked sampling}{4}{section*.6}}
\newlabel{sect:deterministic}{{}{4}{Deterministic dependencies and blocked sampling}{section*.6}{}}
\@writefile{toc}{\contentsline {section}{Experimental results}{4}{section*.8}}
\newlabel{sect:experiment}{{}{4}{Experimental results}{section*.8}{}}
\newlabel{example:market}{{2}{4}{Market maker (MM)}{example.2}{}}
\citation{Roberts:97}
\newlabel{fig:market}{{4a}{5}{\relax }{figure.caption.9}{}}
\newlabel{sub@fig:market}{{a}{5}{\relax }{figure.caption.9}{}}
\newlabel{fig:mmm.prior}{{4b}{5}{\relax }{figure.caption.9}{}}
\newlabel{sub@fig:mmm.prior}{{b}{5}{\relax }{figure.caption.9}{}}
\newlabel{fig:mmm.posterior}{{4c}{5}{\relax }{figure.caption.9}{}}
\newlabel{sub@fig:mmm.posterior}{{c}{5}{\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \relax \fontsize  {9}{10}\selectfont  Instrument type value distribution of Market Maker problem of Example\nobreakspace  {}\ref  {example:market}. (a) prior, (b) posterior given 4 observed data points (trader responses).\relax }}{5}{figure.caption.9}}
\newlabel{fig:mmm}{{4}{5}{\footnotesize Instrument type value distribution of Market Maker problem of Example~\ref {example:market}. (a) prior, (b) posterior given 4 observed data points (trader responses).\relax }{figure.caption.9}{}}
\bibstyle{alpha}
\bibdata{fastBayes}
\bibcite{Das:08}{DMI08}
\bibcite{Keeney:93}{KR93}
\bibcite{Poon:06}{PD06}
\bibcite{Roberts:97}{RGG97}
\bibcite{Sanner:12}{SA12}
\bibcite{Shogren:00}{SLH00}
\@writefile{toc}{\contentsline {section}{Conclusion}{6}{section*.11}}
\newlabel{sect:conclusion}{{}{6}{Conclusion}{section*.11}{}}
\newlabel{fig:error-samples-bppl}{{5a}{7}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig:error-samples-bppl}{{a}{7}{\relax }{figure.caption.10}{}}
\newlabel{fig:error-samples-bppl}{{5b}{7}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig:error-samples-bppl}{{b}{7}{\relax }{figure.caption.10}{}}
\newlabel{fig:mmm_data_analysis}{{5c}{7}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig:mmm_data_analysis}{{c}{7}{\relax }{figure.caption.10}{}}
\newlabel{fig:mmm_dim_analysis}{{5d}{7}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig:mmm_dim_analysis}{{d}{7}{\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Performance of Rejection/Metropolis-Hastings, baseline and augmented Gibbs on (a) \& (b) BPPL and (c) \& (d) MM models against different configurations of the number of observed data and the dimensionality of the parameter space. In almost all cases, Augmented Gibbs takes orders of magnitude less time to achieve the same error as other methods and this performance separation from competing algorithms increases in many cases with the amount of data and dimensionality.\relax }}{7}{figure.caption.10}}
\newlabel{fig:results}{{5}{7}{Performance of Rejection/Metropolis-Hastings, baseline and augmented Gibbs on (a) \& (b) BPPL and (c) \& (d) MM models against different configurations of the number of observed data and the dimensionality of the parameter space. In almost all cases, Augmented Gibbs takes orders of magnitude less time to achieve the same error as other methods and this performance separation from competing algorithms increases in many cases with the amount of data and dimensionality.\relax }{figure.caption.10}{}}
