\label{sec:sdp}

In this section, we describe the \emph{symbolic dynamic programming}
(SDP) work of~\cite{sanner_uai12} and its extension to the case of continuously
parameterized actions by ~\cite{sanner_aaai12}.  We present the general SDP
framework for value iteration in Algorithm~\ref{alg:vi} (\texttt{VI})
and a regression subroutine in Algorithm~\ref{alg:regress}
(\texttt{Regress}) where we have omitted parameters $\vec{b}$ and
$\vec{x}$ from $V$ and $Q$ to avoid notational clutter.  .
%Before we explain this contribution though, we 
But first we recap SDP,
which uses the \emph{case} representation and operations.
%solution, which requires that the CSA-MDP and all other
%functions such as $Q$ and $V$ are represented in \emph{case} form and
%all operations are \emph{case operations}, defined next.

\subsection{Case Representation and Operators}

From here out, we assume that all symbolic functions
can be represented in \emph{case} form~\cite{fomdp}:
{%\footnotesize 
\begin{align}
f = 
\begin{cases}
  \phi_1: & f_1 \\ 
 \vdots&\vdots\\ 
  \phi_k: & f_k \\ 
\end{cases} \label{eq:case}
\end{align}
}
Here the $\phi_i$ are logical formulae defined over the state
$(\vec{b},\vec{x})$ that can include arbitrary logical ($\land,\lor,\neg$)
combinations of (i) boolean variables and (ii) 
\emph{linear} strict inequalities ($>,<$) \footnote{ For purposes of evaluating
a linear function $f$ within a region, it does not matter whether a bound is inclusive ($\leq$ or $\geq$) or exclusive ($<$ or $>$), since it can be arbitrarily near the bounds; for precise evaluation at the bounds, $f$ is required to be continuous, in which case both partitions will give the same result.}
over continuous variables.  
The $\phi_i$ are disjoint from each other;  however they may not exhaustively cover the state space, in which case $f$ is a \emph{partial function}, undefined for some
variable assignments. 
We denote by $S(\phi)$ the region of $\phi$, that is, the set of values where $\phi$ is true. $$S := \{ (\vec{b},\vec{x}) \in \{0,1\}^n\times\mathbb{R}^m  | \phi(\vec{b},\vec{x}) = 1 \}$$.
For $f$ to represent a piecewise linear function, all $f_i$ must be linear in the continuous parameters. 
 In this work, we will refer to a pair $( f_i, \phi_i)$ as a case linear function, which is interpreted as a piecewise linear partial function defined in only one partition, $\phi_i$, and its value is equal to $f_i$ for all points where it is defined.

In the context of SDP, we need to perform operations on symbolic functions represented in case form. The operations we need are: scalar multiplication, addition, subtraction and multiplication of symbolic functions, case maximization, boolean restriction and continuous integration. Next we explain how these operations are defined.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\incmargin{.5em}
\linesnumbered
\begin{algorithm}[t!]
\vspace{-.5mm}
\dontprintsemicolon
\SetKwFunction{regress}{Regress}
\SetKwFunction{approximate}{Approximate}
\Begin{
   $V^0:=0, h:=0$\;
   \While{$h < H$}{
       $h:=h+1$\;
       \ForEach {$a(\vec{y}) \in A$}{
              $Q_a^{h}(\vec{y})\,:=\,$\regress{$V^{h-1},a,\vec{y}$}\;
              $Q_a^{h} := \max_{\vec{y}} \, Q_a^{h}(\vec{y})$ $\,$ \emph{// Continuous $\max$}\;
              $V^{h} := \casemax_a \, Q_a^{h}$ $\,$ \emph{// $\casemax$ all $Q_a$}\;
              $\pi^{*,h} := \argmax_{(a,\vec{y})} \, Q_a^{h}(\vec{y})$\;
       }
       $V^h = $\approximate{$V^{h}$}\; \label{approxline}
       \If{$V^h = V^{h-1}$}
           {break $\,$ \emph{// Terminate if early convergence}\;}
   }
     \Return{$(V^h,\pi^{*,h})$} \;
}
\caption{\footnotesize \texttt{VI}(Hybrid-MDP, $H$) $\longrightarrow$ $(V^h,\pi^{*,h})$ \label{alg:vi}}
\vspace{-1mm}
\end{algorithm}
\decmargin{.5em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\incmargin{.5em}
\linesnumbered
\begin{algorithm}[t!]
\vspace{-.5mm}
\dontprintsemicolon
\SetKwFunction{remapWithPrimes}{Prime}
%\SetKwFunction{sumout}{sumout}


\Begin{
    $Q=$ \remapWithPrimes{$V$} $\,$ \emph{// All $b_i \to b_i'$ and all $ x_i \to x_i'$} \;
    \emph{// Continuous regression marginal integration}\\
    \For {all $x'_j$ in $Q$}{
         $Q := \int Q \otimes P(x_j'|\vec{b},\vec{b}',\vec{x},a,\vec{y}) \, d_{x'_j}$\;
    }
    \emph{// Discrete regression marginal summation}\\
    \For {all $b'_i$ in $Q$}{
         $Q := \left[ Q \otimes P(b_i'|\vec{b},\vec{x},a,\vec{y}) \right]|_{b_i' = 1}$\\
         \hspace{8mm} $\oplus \left[ Q \otimes P(b_i'|\vec{b},\vec{x},a,\vec{y}) \right]|_{b_i' = 0}$\;
    }
    \Return{$R(\vec{b},\vec{x},a,\vec{y}) \oplus (\gamma \otimes Q)$} \;
}
\caption{\footnotesize \texttt{Regress}($V,a,\vec{y}$) $\longrightarrow$ $Q$ \label{alg:regress}}
\vspace{-1mm}
\end{algorithm}
\decmargin{.5em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\emph{Unary operations} such as scalar multiplication $c\cdot f$ (for
some constant $c \in \mathbb{R}$) or negation $-f$ on case statements
$f$ are simply applied to each
$f_i$ ($1 \leq i \leq k$). Intuitively, to perform a \emph{binary
  operation} on two case statements, we simply take the cross-product
of the logical partitions of each case statement and perform the
corresponding operation on the resulting paired partitions.  Letting
each $\phi_i$ and $\psi_j$ denote generic first-order formulae, we can
perform the ``cross-sum'' $\oplus$ of two (unnamed) cases in the
following manner:

{\footnotesize 
\begin{center}
\begin{tabular}{r c c c l}
&
\hspace{-6mm} 
  $\begin{cases}
    \phi_1: & f_1 \\ 
    \phi_2: & f_2 \\ 
  \end{cases}$
$\oplus$
&
\hspace{-4mm}
  $\begin{cases}
    \psi_1: & g_1 \\ 
    \psi_2: & g_2 \\ 
  \end{cases}$
&
\hspace{-2mm} 
$ = $
&
\hspace{-2mm}
  $\begin{cases}
  \phi_1 \wedge \psi_1: & f_1 + g_1 \\ 
  \phi_1 \wedge \psi_2: & f_1 + g_2 \\ 
  \phi_2 \wedge \psi_1: & f_2 + g_1 \\ 
  \phi_2 \wedge \psi_2: & f_2 + g_2 \\ 
  \end{cases}$
\end{tabular}
\end{center}
}
\normalsize

Likewise, we perform $\ominus$ and $\otimes$ by,
respectively, subtracting or multiplying partition values to obtain the result. 
As linear inequalities can be inconsistent (infeasible) some of the generated partitions will be removed from the result. 

%For SDP, we'll also need to perform maximization, restriction,
%and substitution on case statements.  
Next we define \emph{symbolic case maximization}:
\vspace{-2mm}

{\footnotesize
\begin{center}
\begin{tabular}{r c c c l}
&
\hspace{-7mm} $\casemax \Bigg(
  \begin{cases}
    \phi_1: \hspace{-2mm} & \hspace{-1mm} f_1 \\ 
    \phi_2: \hspace{-2mm} & \hspace{-1mm} f_2 \\ 
  \end{cases}$
$,$
&
\hspace{-4mm}
  $\begin{cases}
    \psi_1: \hspace{-2mm} & \hspace{-1mm} g_1 \\ 
    \psi_2: \hspace{-2mm} & \hspace{-1mm} g_2 \\ 
  \end{cases} \Bigg)$
&
\hspace{-4mm} 
$ = $
&
\hspace{-4mm}
  $\begin{cases}
  \phi_1 \wedge \psi_1 \wedge f_1 > g_1    : & \hspace{-2mm} f_1 \\ 
  \phi_1 \wedge \psi_1 \wedge f_1 \leq g_1 : & \hspace{-2mm} g_1 \\ 
  \phi_1 \wedge \psi_2 \wedge f_1 > g_2    : & \hspace{-2mm}f_1 \\ 
  \phi_1 \wedge \psi_2 \wedge f_1 \leq g_2 : & \hspace{-2mm} g_2 \\ 
  \vdots & \vdots
%  \phi_2 \wedge \psi_1 \wedge f_2 > g_1    : & \hspace{-2mm} f_2 \\ 
%  \phi_2 \wedge \psi_1 \wedge f_2 \leq g_1 : & \hspace{-2mm} g_1 \\ 
%  \phi_2 \wedge \psi_2 \wedge f_2 > g_2    : & \hspace{-2mm} f_2 \\ 
%  \phi_2 \wedge \psi_2 \wedge f_2 \leq g_2 : & \hspace{-2mm} g_2 \\ 
  \end{cases}$
\end{tabular}
\end{center}
} If all $f_i$ and $g_i$ are linear,
the $\casemax$ result is clearly still linear and representable in the case format previously described (i.e., linear inequalities in decisions).

 The operation of \emph{boolean restriction} required for regressing boolean variables is obvious and an example is omitted: in this operation
$f|_{b=v}$, anywhere a boolean variable $b$ occurs in $f$, we assign
it the value $v \in \{ 0,1 \}$, and resolve the following logic implications, reducing formulae or removing partitions.  The operation of \emph{continuous integration} $\int Q(x'_j) \otimes P(x'_j|\cdots) dx'_j$ is required for regressing continuous variables. As previously defined, $P(x_j'|\cdots)$
will always be of the form $\delta[x_j' - h(\vec{z})]$ where
$h(\vec{z})$ is a case statement and $\vec{z}$ does not contain
$x'_j$.  Rules of integration then tell us that $\int f(x'_j) \otimes
\delta[x_j' - h(\vec{z})] dx'_j = f(x'_j) \{ x'_j / h(\vec{z}) \}$
where the latter operation indicates that any occurrence of $x'_j$ in
$f(x'_j)$ is \emph{symbolically substituted} with the case statement
$h(\vec{z})$. The appearance of case statement within formulae is solved expanding the cross-product of the logical partitions. A more detailed specification of this operation is available in ~\cite{sanner_uai11}.  The important insight is that this $\int$ operation yields a result that is a
case statement in the form previously outlined.

The only operation for SDP that has not yet been defined is the continuous action maximization.
The continuos maximization over action variables can be wrtitten as $g(\vec{b},\vec{x}) := \max_{\vec{y}} \, f(\vec{b},\vec{x},\vec{y})$. Note that the result of this max, $g(\vec{b},\vec{x})$ is a function of continuous variables, hence requiring \emph{symbolic} constrained optimization.

The definition of this maximization operation was the major contribution of~\ref{zahra}, so refer to this work for a detailed explanation. We outline the basic steps for continuous maximization.

First, due to the commutativity of $\max$, any multivariate $\max_{\vec{y}}$ can be rewritten as a sequence of univariate $\max$ operations $\max_{y_1} \cdots \max_{y_{|\vec{y}|}}$; hence it
suffices to provide just the \emph{univariate} $\max_y$ solution:
$g(\vec{v}) := \max_{y} \, f(\vec{v},y)$, where $\vec{v}$ denotes a vector of boolean and continuous variables, in our case containing $\vec{b}$, $\vec{x}$ and other action parameters.

Second, because of the mutual disjointness of partitions, and the commutativity of $\casemax_i$ and $\max_y$, we can write $\max_y f(\vec{v},y) $ via:
{\footnotesize
\begin{align}
\max_y f(\vec{v},y) & = 
\max_y \casemax_i \, \phi_i(\vec{v},y) f_i(\vec{v},y) \nonumber \\
& = \casemax_i \, \fbox{$\max_y \phi_i(\vec{v},y) f_i(\vec{v},y)$} \label{eq:casemax_max}
\end{align}}

 Thus to complete this operation we need only
show how to symbolically compute the maximum in a single partition 
$\max_y \phi_i(\vec{v},y) f_i(\vec{v},y)$.

In $\phi_i$, we observe that each conjoined constraint serves one of
three purposes: (i) \emph{upper bound on $y$}: it can be written
as $y < l(\vec{v})$; (ii) \emph{lower bound on $y$}: it can be written as $y >
u(\vec{v})$ or (iii) \emph{independent of $y$}: the constraints do not contain $y$
and can be safely factored outside of the $\max_y$, as $f_i(\vec{v},y) =  g_i(\vec{v})$.  
Because there are multiple symbolic upper and lower
bounds on $y$, we apply the $\casemax$ ($\casemin$) operator to determine the highest lower bound $\LB$(lowest upper bound $\UB$):

We know that $\max_y \phi_i(\vec{b},\vec{x},y)
f_i(\vec{b},\vec{x},y)$ for a linear function $f_i$ must occur at either the upper or lower bounds ($\UB$ and $\LB$) of $y$, both of which are symbolic functions of $\vec{v}$; 

Given the \emph{potential} maxima points of $y = \UB$ and $y = \LB$ w.r.t. constraints $\phi_i(\vec{b},\vec{x},y)$ --- which are all symbolic functions --- we must symbolically evaluate which yields the maximizing value $\Max$ for this case partition: {\footnotesize
\begin{align*}
\Max = \casemax( f_i \{ y / \UB \}, f_i \{ y / \LB \})
\end{align*}} 
The substitution operator $\{ y / f \}$ replaces $y$ with case statement $f$, 
defined in~\cite{sanner_uai11}.

At this point, we have almost completed the computation
of the $\max_y \phi_i(\vec{b},\vec{x},y) f_i(\vec{b},\vec{x},y)$
except for one issue: the incorporation of the $\IND$ constraints
(factored out previously) and additional constraints that arise from the
symbolic nature of $\UB$ and $\LB$.  Specifically
for the latter, we need to ensure that indeed $\LB < \UB$
by building a set of constraints $\CONS$ that ensure these conditions hold; to do this,
it suffices to ensure that for each possible expression $e$ used to
construct $\LB$ that $e \leq \UB$.

Now we express the final result as a single case partition:
\begin{equation*}
\max_y \phi_i(\vec{v},y) f_i(\vec{v},y) \;\; = \;\;
\left\{ \CONS \land \IND: \Max \right.
\end{equation*}
Returning to~\eqref{eq:casemax_max}, we find that we have
now specified the inner operation (shown in the $\Box$).  
Hence, to complete the maximization for an entire case statement $f$, we need only apply the
above procedure to each case partition of $f$ and then $\casemax$ all
of these results.  

\subsection {\bf Extended ADDs (XADDs)}
~\cite{sanner_uai11} extension of
ADDs~\cite{bahar93add} provides a compact data structure to support
case statements and operations.  Using XADDs in SDP as a continuous
version of the ADD-based SPUDD~\cite{spudd} algorithm for
discrete MDPs, we maintain compact forms of $Q$ and $V$, e.g., as
shown in $V^2$ for \MarsRover\ in Figure~\ref{fig:opt_val_pol}.  All the operations described in the previous section have been implemented for XADDs in previous works~\cite{sanner_uai11}. In this paper, we use linear XADDs, i.e. XADD with only boolean variables or linear inequalities in decisions and linear functions in terminal nodes. Linear XADDs permit LP solvers to be used in two ways: linear constraint feasibility checking to prune unreachable paths in the XADD and linear constrained optimization of linear functions to obtain maximum and minimum values of a XADD. Besides, our main contribution in this paper is the development of an approximation method for piecewise linear functions that is used in linear XADDs to reduce the number of nodes and provide more efficiency.