In order to compute the equations above, we propose a \emph{Robust symbolic dynamic programming} approach similar to ~\cite{sanner_uai11}.  This requires a general value iteration in Algorithm~\ref{alg:vi} (\texttt{VI})
and a regression subroutine in Algorithm~\ref{alg:regress}
(\texttt{Regress}) %where we have omitted parameters $\vec{b}$ and $\vec{x}$ from $V$ and $Q$ to avoid notational clutter. 
%Note that the noise variables are defined and minimized in the 
%Before we explain this contribution though, we ~\ref{alg:vi} Algorithm. 
In general Symbolic Dynamic Programming (SDP) uses the \emph{case} representation. All operations required for computing $Q$ and $V$ functions are \emph{case operations} and defined next. We then move to the Robust SDP solution. 

\subsection{Case Representation and Operators}

Symbolic functions can generally be represented in \emph{case} form~\cite{fomdp}:
{%\footnotesize 
\begin{align}
f = 
\begin{cases}
  \phi_1: & f_1 \\ 
 \vdots&\vdots\\ 
  \phi_k: & f_k \\ 
\end{cases} \label{eq:case}
\end{align}
}
where $\phi_i$ are logical formulae defined over the state
$(\vec{b},\vec{x})$ and can include arbitrary logical ($\land,\lor,\neg$)
combinations of boolean variables and \emph{linear} inequalities ($\geq,>,\leq,<$) 
over continuous variables.  
%Each $\phi_i$ will be disjoint from the other $\phi_j$ ($j \neq i$);  however the $\phi_i$ may not exhaustively cover the state space, hence $f$ may only be a \emph{partial function} and may be undefined for some variable assignments.

The $f_i$ may be either linear or quadratic in the continuous parameters according to the same restrictions as for 
$R(\vec{b},\vec{x},a,\vec{y})$and to be continuous at partition boundaries. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\incmargin{.5em}
\linesnumbered
\begin{algorithm}[t!]
\vspace{-.5mm}
\dontprintsemicolon
\SetKwFunction{regress}{Regress}
\Begin{
   $V^0:=0, h:=0$\;
   \While{$h < H$}{
       $h:=h+1$\;
       \ForEach {$a(\vec{y}) \in A$}{
              $Q_a^{h}(\vec{y})\,:=\,$\regress{$V^{h-1},a,\vec{y}$}\;
               $Q_a^{h}(\vec{y},\vec{\epsilon}) := \casemax_{\vec{epsilon}} \, Q_a^{h}(\vec{y})$ $\,$ \emph{// $\max$-in noise variables}\;
				$Q_a^{h}(\vec{y}) := \min_{\vec{epsilon}} \, Q_a^{h}(\vec{y},\vec{\epsilon})$ $\,$ \emph{// Stochastic $\min$}\;
				$Q_a^{h} := \max_{\vec{y}} \, Q_a^{h}(\vec{y})$ $\,$ \emph{// Continuous $\max$}\;
              $V^{h} := \casemax_a \, Q_a^{h}$ $\,$ \emph{// $\casemax$ all $Q_a$}\;
              $\pi^{*,h} := \argmax_{(a,\vec{y})} \, Q_a^{h}(\vec{y})$\;
       }
       \If{$V^h = V^{h-1}$}
           {break $\,$ \emph{// Terminate if early convergence}\;}
   }
     \Return{$(V^h,\pi^{*,h})$} \;
}
\caption{\footnotesize \texttt{VI}(CSA-MDP, $H$) $\longrightarrow$ $(V^h,\pi^{*,h})$ \label{alg:vi}}
\vspace{-1mm}
\end{algorithm}
\decmargin{.5em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\incmargin{.5em}
\linesnumbered
\begin{algorithm}[t!]
\vspace{-.5mm}
\dontprintsemicolon
\SetKwFunction{remapWithPrimes}{Prime}
%\SetKwFunction{sumout}{sumout}


\Begin{
    $Q=$ \remapWithPrimes{$V$} $\,$ \emph{// All $b_i \to b_i'$ and all $ x_i \to x_i'$} \;
    \emph{// Continuous regression marginal integration}\\
    \For {all $x'_j$ in $Q$}{
         $Q := \int Q \otimes P(x_j'|\vec{b},\vec{b}',\vec{x},a,\vec{y},\vec{\epsilon}) \, d_{x'_j}$\;
    }
    \emph{// Discrete regression marginal summation}\\
    \For {all $b'_i$ in $Q$}{
         $Q := \left[ Q \otimes P(b_i'|\vec{b},\vec{x},a,\vec{y}) \right]|_{b_i' = 1}$\\
         \hspace{8mm} $\oplus \left[ Q \otimes P(b_i'|\vec{b},\vec{x},a,\vec{y}) \right]|_{b_i' = 0}$\;
    }
    \Return{$R(\vec{b},\vec{x},a,\vec{y}) \oplus (\gamma \otimes Q)$} \;
}
\caption{\footnotesize \texttt{Regress}($V,a,\vec{y}$) $\longrightarrow$ $Q$ \label{alg:regress}}
\vspace{-1mm}
\end{algorithm}
\decmargin{.5em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

For any \emph{Unary operations} such as scalar multiplication $c\cdot f$ (for
a constant $c \in \mathbb{R}$) and negation $-f$ on case statements
$f$, the operation is simply applied to each
$f_i$ ($1 \leq i \leq k$). \emph{Binary operation} on two case statementsare done by taking the cross-product
of the logical partitions of the two case statement and perform the
corresponding operation on the resulting paired partitions.  As a  example the ``cross-sum'' $\oplus$ of two cases are defined as below

{\footnotesize 
\begin{center}
\begin{tabular}{r c c c l}
&
\hspace{-6mm} 
  $\begin{cases}
    \phi_1: & f_1 \\ 
    \phi_2: & f_2 \\ 
  \end{cases}$
$\oplus$
&
\hspace{-4mm}
  $\begin{cases}
    \psi_1: & g_1 \\ 
    \psi_2: & g_2 \\ 
  \end{cases}$
&
\hspace{-2mm} 
$ = $
&
\hspace{-2mm}
  $\begin{cases}
  \phi_1 \wedge \psi_1: & f_1 + g_1 \\ 
  \phi_1 \wedge \psi_2: & f_1 + g_2 \\ 
  \phi_2 \wedge \psi_1: & f_2 + g_1 \\ 
  \phi_2 \wedge \psi_2: & f_2 + g_2 \\ 
  \end{cases}$
\end{tabular}
\end{center}
}
\normalsize

The other operations of $\ominus$ and $\otimes$ are performed by subtracting or multiplying partition values.  Note that some partitions may become inconsistent (infeasible)  which are removed from the final result. 

The other operations required for RSDP are restriction, substitution and maximization on case statements.  

\texttt{Regression} is defined separately for discrete and continuous variables.\emph{Boolean restriction} $f|_{b=v}$ assigns the value $v \in \{ 0,1 \}$ to any occurrence of $b$ in $f$ such as lines 7--8. \emph{Continuous integration} $\int Q(x'_j) \otimes P(x'_j|\cdots) dx'_j$  us performed in line 5 where $P(x_j'|\cdots)$
is in the form of $\delta[x_j' - h(\vec{z})]$ ($h(\vec{z})$ is a case statement and $\vec{z}$ does not contain
$x'_j$) and the result from integration is $\int f(x'_j) \otimes \delta[x_j' - h(\vec{z})] dx'_j = f(x'_j) \{ x'_j / h(\vec{z}) \}$. Note that the latter operation indicates that any occurrence of $x'_j$ in
$f(x'_j)$ is \emph{symbolically substituted} with the case statement
$h(\vec{z})$.  Full specification of this operation is defined in~\cite{sanner_uai11}. 
A \emph{symbolic case maximization} on two case statements is performed below:
\vspace{-4mm}

{\footnotesize
\begin{center}
\begin{tabular}{r c c c l}
&
\hspace{-7mm} $\casemax \Bigg(
  \begin{cases}
    \phi_1: \hspace{-2mm} & \hspace{-2mm} f_1 \\ 
    \phi_2: \hspace{-2mm} & \hspace{-2mm} f_2 \\ 
  \end{cases}$
$,$
&
\hspace{-4mm}
  $\begin{cases}
    \psi_1: \hspace{-2mm} & \hspace{-2mm} g_1 \\ 
    \psi_2: \hspace{-2mm} & \hspace{-2mm} g_2 \\ 
  \end{cases} \Bigg)$
&
\hspace{-4mm} 
$ = $
&
\hspace{-4mm}
  $\begin{cases}
  \phi_1 \wedge \psi_1 \wedge f_1 > g_1    : & \hspace{-2mm} f_1 \\ 
  \phi_1 \wedge \psi_1 \wedge f_1 \leq g_1 : & \hspace{-2mm} g_1 \\ 
  \phi_1 \wedge \psi_2 \wedge f_1 > g_2    : & \hspace{-2mm}f_1 \\ 
  \phi_1 \wedge \psi_2 \wedge f_1 \leq g_2 : & \hspace{-2mm} g_2 \\ 
 % \vdots & \vdots
 \phi_2 \wedge \psi_1 \wedge f_2 > g_1    : & \hspace{-2mm} f_2 \\ 
 \phi_2 \wedge \psi_1 \wedge f_2 \leq g_1 : & \hspace{-2mm} g_1 \\ 
 \phi_2 \wedge \psi_2 \wedge f_2 > g_2    : & \hspace{-2mm} f_2 \\ 
 \phi_2 \wedge \psi_2 \wedge f_2 \leq g_2 : & \hspace{-2mm} g_2 \\ 
  \end{cases}$
\end{tabular}
\end{center}
} If all $f_i$ and $g_i$ are linear,
the $\casemax$ result is clearly still linear.  If the $f_i$ or $g_i$
are quadratic (e.g. in the reward), then $f_i > g_i$ or $f_i \leq
g_i$ will be at most univariate quadratic which can be linearized into two linear inequalities. Hence the result of this $\casemax$
operator will be representable in the case format with linear inequalities in decisions.

For continuous maximization according to ~\cite{sdp_aaai} the $\max_y$ for each case partition
is computed individually that is $\max_y \phi_i(\vec{b},\vec{x},y) f_i(\vec{b},\vec{x},y)$.
In $\phi_i$ each conjoined constraint serves as either the upper bound on $y$, lower bound on $y$ or independent of $y$. The $\casemax$ ($\casemin$) operator is then used to determine the highest lower bound $\LB$
(lowest upper bound $\UB$) for multiple symbolic upper and lower bounds on $y$.
Apart from the lower and upper bound the roots of $\frac{\partial}{\partial y} f_i$ 
w.r.t.\ $y$  are also potential maxima points. These points are then symbolically evaluated to find which yields the
maximizing value $\Max$.  Independent constraints and additional constraints that arise from the
symbolic nature of the $\UB$, $\LB$, and $\Root$ are incorporated in the final result.

To complete the maximization for an entire case statement $f$, the
above procedure is applied to each case partition of $f$ and then $\casemax$ all
of these results.  

To implement the case statements efficiently with continuous variables, extended Algebraic Decision diagrams (XADDs) are used from ~\cite{sanner_uai11} which is extended from ADDs ~\cite{bahar93add}. Unreachable paths can be pruned in XADDs using LP solvers and all operations including the continuous minimization explained in the next section can be defined using XADDs. 

%The only operation that has not been previously defined for XADDs is $\max_y$, but this is easy: treating each XADD path from root to leaf node as a single case partition with conjunctive constraints,  $\max_y$ is performed at each leaf subject to these constraints and all path $\max_y$'s are then accumulated via the $\casemax$ operation to obtain the final result.

\subsection{Symbolic Robust Dynamic Programming}