\section{Related Work and Conclusions}
\label{cha:conclusions}

% See original background content
In short, others have looked at robust solutiosn, but no one
has looked at directly optimizing 0--1 loss with one
exception.

Empirically, we compare our proposed algorithms to logistic
regression, SVM, and the Bayes point machine showing that the proposed
0--1 loss optimization algorithms perform at least as well and offer a
clear advantage in the presence of outliers.  To this end, we believe
this work reiterates the importance of 0--1 loss and its robustness
properties while challenging the notion that it is difficult to
directly optimize.


\COMMENT

\subsection{Future Work}
\label{sec:concl.futurework}

Incrementally maintain half-spaces.

Parallelization.

Use subset of points in SLA close to current boundary... other
points unlikely to change from small shift in boundary.

Currently, the first process of the SLA algorithm uses a modified
gradient descent mechanism. However, there are faster optimization
methods that can be used, such as the pseudo-second order
Levenberg-Marquardt method \cite{Marquardt}, or the trust region
Newton's method \cite{Steihaug}.

Boosting?

\ENDCOMMENT
