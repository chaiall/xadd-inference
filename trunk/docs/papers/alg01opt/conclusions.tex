\section{Related Work and Conclusions}
%\section{Conclusions}
\label{cha:conclusions}

\COMMENT
% See original background content
In short, others have looked at robust solutiosn, but no one
has looked at directly optimizing 0--1 loss with one
exception.
\ENDCOMMENT

We have explored a variety of algorithms for direct 0--1 loss
optimization and have shown that one approach in particular --- smooth
0--1 loss approximation --- is fastest and can perform as well as
state-of-the-art classification algorithms and often better in the
presence of outliers.  While other works have investigated
classification methods that are robust to outliers, including
t-logistic regression \cite{Ding}, robust truncated hinge loss
\cite{wu07}, and SavageBoost \cite{lossdesign}, we are unaware of any
work that has successfully directly optimized 0--1 loss --- our
primary goal in this paper.  We believe this work reiterates the
importance of 0--1 loss for its robustness properties while providing
evidence that it can be effectively optimized in practice.

\COMMENT

\subsection{Future Work}
\label{sec:concl.futurework}

Incrementally maintain half-spaces.

Parallelization.

Use subset of points in SLA close to current boundary... other
points unlikely to change from small shift in boundary.

Currently, the first process of the SLA algorithm uses a modified
gradient descent mechanism. However, there are faster optimization
methods that can be used, such as the pseudo-second order
Levenberg-Marquardt method \cite{Marquardt}, or the trust region
Newton's method \cite{Steihaug}.

Boosting?

\ENDCOMMENT
