\section{Related Work and Conclusions}
%\section{Conclusions}
\label{cha:conclusions}

\MYCOMMENT
% See original background content
In short, others have looked at robust solutiosn, but no one
has looked at directly optimizing 0--1 loss with one
exception.
\ENDMYCOMMENT

We have explored a variety of algorithms for direct 0--1 loss
optimization and have shown one of these approaches --- smooth
0--1 loss approximation (SLA) --- is fastest and performs as well as
state-of-the-art classification algorithms and often better in the
presence of outliers.  While other works have investigated
classification methods that are robust to outliers, including
t-logistic regression \cite{Ding}, robust truncated hinge loss
\cite{wu07}, SavageBoost \cite{lossdesign},
we are unaware of any
work that has successfully directly optimized 0--1 loss --- our
primary goal in this paper. The work of \cite{collobert} showed that
researchers should also explore non-convex approaches as they 
sometimes give clear benefits. We believe this work reiterates the
importance of 0--1 loss for its robustness properties while providing
evidence that it can be effectively optimized in practice.

\MYCOMMENT

\subsection{Future Work}
\label{sec:concl.futurework}

Incrementally maintain half-spaces.

Parallelization.

Use subset of points in SLA close to current boundary... other
points unlikely to change from small shift in boundary.

Currently, the first process of the SLA algorithm uses a modified
gradient descent mechanism. However, there are faster optimization
methods that can be used, such as the pseudo-second order
Levenberg-Marquardt method \cite{Marquardt}, or the trust region
Newton's method \cite{Steihaug}.

Boosting?

\ENDMYCOMMENT
