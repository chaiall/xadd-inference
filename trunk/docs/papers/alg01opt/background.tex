\section{Background}
\label{cha:background}

%=================================================
\subsection{Linear Binary Classification}
\label{sec:bgr.formulation}

Generally, the space of observations with $P$ properties is called the \emph{configuration space}, which is mapped to $D$ components of a vector $\x \in \R^D$ ($D$ for dimension), where each component is called a \emph{feature}, the space of $\x$ is called the \emph{feature space}, and the mapping process is called \emph{feature mapping}. Note that often $D \not= P$, because one property can be mapped to more than one features as illustrated in Figure \ref{fig:featuresspace}.  

The goal of binary classification is to predict the class $\hat{t} \in \{ -1, 1 \}$ of a new observation $\x$. Linear binary classification achieves this by making a classification decision based on the value of a linear combination of the features. Specifically, the class prediction for $\x$ is based on the value of the following (so--called) predictor function \cite{bishop06}: 
\begin{equation} 
\label{eq:predictor}
f_{\w}(\x) = \sum_{j=1}^D w_j x_j + w_0 = \w^T \x + w_0,
\end{equation}
where $w_j$ is the weight corresponding to the $j-$th feature, $\w$ is the weight vector consisting of these individual weights, and $w_0$ is a bias (the negative of the bias is sometimes called a $threshold$). If $f_{\w}(\x) \geq 0$, the class prediction for $\x$ is $\hat{t} = 1$, otherwise $\hat{t} = -1$. Thus, the equation of the decision boundary that separates the two classes is $f_{\w}(\x) = \w^T \x + w_0 = 0,$
which is a hyperplane in this linear model (hence the decision boundary is also called \emph{decision hyperplane}). For convenience, the bias term is often included in the weight vector as $\begin{pmatrix}
w_0\\ \w \end{pmatrix}$, and the feature space is extended appropriately by a constant feature as $\begin{pmatrix} 1\\ \x \end{pmatrix}$. In this new notation, both $\w$ and $\x$ are in $\R^{D+1}$, and the predictor function is now $$f_{\w}(\x) = \w^T \x.$$ The equation of the decision hyperplane is now $$\w^T \x = 0,$$ which is a homogeneous system of equations. For this reason, this notation is called \emph{homogeneous}. For latter reference we call the former notation (with a separate bias $w_0$) \emph{non-homogeneous}. 

Note that the linear classification model may seem simple, but it can solve complicated problems as shown by Figure \ref{fig:featuresspace}, and is arguably the most commonly used classification model in practice. 

In machine learning, the \emph{training dataset} contains some $N$ observations $\X = \{\boldsymbol{x_1, x_2, \dots, x_N}\}$ and their corresponding class $\t = \{t_1, t_2, \dots, t_N\}$. To measure the degree of belief in class prediction for an observation $\xi \in \X$, the so--called \emph{margin} is defined as follows:
\begin{equation} 
\label{eq:margin}
m_i(\w) = t_i f_{\w}(x_i),
\end{equation}
where $m_i(\w)$ is the margin corresponding to $\xi$. It can be seen that if $m_i(\w) < 0$ then the sign of the predictor function $f_{\w}(x_i)$ is opposite to the sign of then known class $t_i$ of point $\xi$, hence $\xi$ is misclassified. In this case, $m_i$ is a measure of the margin by which the class prediction for $\xi$ given by $f_{\w}(x_i)$ is wrong. Otherwise, if $m_i(\w) \geq 0$, then $\xi$ is correctly classified and $m_i$ represents the ``margin of safety'' by which the prediction is correct \cite{McAllester}.  

The task of classification is now boiled down to determine $\w$ based on the training dataset so as to minimize the sum of losses, where each loss quantifies the penalty for an incorrect classification in the training dataset. There are different types of losses used by different algorithms, such as 0--1 loss, log loss, squared loss, hinge loss, etc. Naturally, all these losses depend on the margin defined above, so the task is mathematically summarized as:
\begin{equation}
\label{eq:objective}
 \w^* = \arg\min_{\w} \; \sum_{i=1}^{N} L(m_i(\w)) + \lambda R(\w),
\end{equation}
where $L(m_i(\w))$ is some loss defined as a function of $m_i(\w)$, and the term $\lambda R(\w)$ is called the \emph{regularization term}, which prevents overfitting and extreme values of $\w$. In this term, $\lambda > 0$ is the \emph{regularization parameter}, which is tuned so as to maximize empirical performance of the resulting value of $\w^*$, and $R(\w)$ is the \emph{regularization function}, often defined as the $p-$norm of $\w$: 
$$R(\w) = \| \w \|_p = (\sum_{i=1}^D |w_i|^p)^{1/p}.$$
For instance, $p=1$ corresponds to the so--called $L_1$ regularizer: $$L_1(\w) = \sum_{i=1}^D |w_i|,$$ 
and $p=2$ corresponds to $L_2$ regularizer: $$L_2(\w) = \frac{1}{2} \w^T \w,$$ where the constant $\frac{1}{2}$ is added just for convenience, etc. 

We next review 0--1 loss and formulate the 0--1 loss optimization problem for linear binary classification. Other types of losses as well as their advantages and disadvantages shall be discussed in Section \ref{sec:bgr.surrogates}.   


%======================
\subsubsection{0--1 Loss and Problem Definition}
\label{ssec:01loss}

The 0--1 loss is defined for some margin $m$ as 
$$
L_{01}(m) = \left\{
     \begin{array}{ll}
       0 & : \text{if }m > 0\\
       1 & : \text{otherwise}
     \end{array}
   \right.
   \; = \; \mathbb{I} [m \leq 0],
$$
where the indicator function, $\mathbb{I} [.]$, is used to simplify the piecewise function. In the context of linear binary classification given in previous subsection, the margin is given by Equation \ref{eq:margin}, and the objective given by Equation \ref{eq:objective} now becomes
\begin{equation}
\label{eq:objective01}
 \w^* = \arg\min_{\w} \; \sum_{i=1}^{N} \mathbb{I} [t_i \w^T \xi \leq 0] + \lambda R(\w),
\end{equation}

It can be seen that in the context of 0--1 loss, the regularization term of the above objective function has no meaning as it can be always driven to zero by scaling down the weight vector $\w$ to $\epsilon \w$ with an arbitrarily small real positive constant $\epsilon$ without changing the 0--1 loss value, because the indicator function is invariant to such scaling. We can therefore omit this term, noting that in practice we can avoid overfitting by checking the components of $\w$ directly. The objective function now becomes
\begin{equation}
\label{eq:objective01}
 \w^* = \arg\min_{\w} \; \sum_{i=1}^{N} \mathbb{I} [t_i \w^T \xi \leq 0],
\end{equation}
where the sum of all losses $\sum_{i=1}^{N} \mathbb{I} [t_i \w^T \xi \leq 0]$ is called the 0--1 loss function. It can be seen, that the 0--1 loss function essentially counts the number of misclassifications. In this sense, 0--1 loss is the most natural loss function, because minimizing it is equivalent to minimizing the misclassification rate for the training data.

The subsequent chapters of this thesis study different approaches to optimize the 0--1 loss function. It is therefore convenient to give a unified problem definition for later reference as follows.

\begin{framed}
\label{definition}
{\bf The 0--1 Loss Optimization Problem Definition:}

\line(1,0){217}

Given a training dataset of $N$ observations $\X = \{\boldsymbol{x_1, x_2, \dots, x_N}\}$ and their corresponding targets $\t = \{t_1, t_2, \dots, t_N\}$, where  $\xi =  (1, x_{i1}, x_{i2}, \dots, x_{iD})^T \in \R^{D+1}$ and $t_i \in \{ -1, 1 \}$ for $i=1,2,\dots, N$. 

Find a weight vector $\w^*$, which minimizes the following 0--1 loss function:
\begin{equation}
\label{01loss}
L(\w) = \sum_{i=1}^N \mathbb{I} [t_i \w^T \xi \leq 0]
\end{equation}
where $\mathbb{I}$ is the indicator function, ${\w} = (w_0, w_1, \dots, w_D)^T \in \R^{D+1}$ is the (vector) variable of the 0--1 loss function $L(\w)$, and the task is to find (or approximate) 
$${\w}^* = \arg\min_{\w} L({\w})$$
\end{framed}

%=================================================
\subsection{Surrogates of 0--1 Loss for Binary Classification}
\label{sec:bgr.surrogates}

In previous section we have reviewed the binary classification problem and 0--1 loss, minimizing which is equivalent to minimizing the error rate in the training dataset. However, direct optimization of 0--1 loss is NP--hard \cite{Feldman,nphard}, hence existing algorithms use convex surrogates of 0--1 loss. In this section, we first discuss the advantages and disadvantages of using a surrogate of 0--1 loss, and then review some of the most popular  surrogates and algorithms using them\cite{McAllester,bishop06}.

{\bf Note somewhere: All can be optimized efficiently and optimally by gradient descent.}

%======================
\subsubsection{Advantages and Disadvantages of Surrogates}
\label{ssec:disadvantages}

Most state-of-the-art algorithms use some convex surrogate of 0--1 loss to solve the binary classification problem, because the convexity makes them computationally efficient \cite{Bartlett} and non-convexity often results in NP-hard problems. However, this choice comes at a cost as the disadvantages of convex objectives are well understood. Intuitively, loss should be bounded as we should not pay an infinite cost for misclassifying any one observation. It can be seen in Figure \ref{fig:losses}, that 0--1 loss is bounded, while other convex surrogates are not bounded with large negative margins. Because of this reason, \cite{outliers} showed that convex loss functions cannot be made robust in the presence of  noise. \cite{outliers2} further characterized this effect by analyzing various convex losses and found that risk minimization under 0-1 loss function has impressive noise tolerance properties and that under squared error loss is tolerant only to uniform noise; risk minimization under other loss functions is not noise tolerant.

%======================
\subsubsection{Squared Loss and Linear Regression}

The squared loss is defined as follows:

\[ \begin{split}
L_2(m_i(\w)) &= \frac{1}{2}[m_i(\w) - 1]^2 \\
& = \frac{1}{2}[t_i f_{\w}(\xi) - t_i^2]^2 \\
& = \frac{1}{2}t_i^2 [f_{\w}(\xi) - t_i]^2 \\
& = \frac{1}{2}[\w^T\xi - t_i]^2,
\end{split} \] 
where we used the fact that $t_i^2 = 1$, and the constant $\frac{1}{2}$ is used for later convenience. We see that minimizing (the sum of) squared loss is equivalent to minimizing the (total squared) difference between the value given by the predictor function $\w^T\xi$ and the training target $t_i$. By applying this loss to the objective function of binary classification given by Equation \ref{eq:objective01} we have the following:
\begin{equation}
\label{eq:squareobj}
\w^* = \arg\min_{\w} \; \sum_{i=1}^{N}  \frac{1}{2}[\w^T\xi - t_i]^2 + \lambda R(\w).
\end{equation}
Squared loss is often used with $L_2$ or $L_1$ regularizers. It is called \emph{ridge regression} if used with $L_2$ regularizer, or \emph{lasso} if used with $L_1$. 

%======================
\subsubsection{Log Loss and Logistic Regression}

We have discussed in Subsection \ref{ssec:binclass}, that if $m_i < 0$ then $|m_i|$ is a measure of the margin by which the class prediction for $\xi$ given by $f_{\w}(x_i)$ is wrong, otherwise the prediction is correct. In this sense, the log loss is defined as 
\[ \begin{split}
L_{log}(m_i(\w)) &= \ln (1 + e^{-m_i(\w)}) \\
 &= \ln (1 + e^{-t_i\w^T\xi}) 
\end{split} \] 
It can be easily verified, that this loss is convex, and for $m_i >> 1$ we have $L_{log}(m_i) \approx 0$, on the other hand, for $m_i << -1$ we have $L_{log} \approx -m_i = |m_i|$. In other words, log loss is a convex model of the scheme, where misclassification is penalized by the absolute value of the margin, and correct classification has zero penalty. We note that both log loss and quadratic loss can be viewed as an attempt to estimate $P(t|\x)$ and can be derived under assumption that $P(\x|t)$ has (multivariate) Gaussian distribution. However, for the purpose here we will not go into this detail. By applying log loss to the objective function of binary classification given by Equation \ref{eq:objective01} we have the following:
\begin{equation}
\label{eq:logobj}
\w^* = \arg\min_{\w} \; \sum_{i=1}^{N} \ln(1 + e^{-t_i\w^T\xi})  + \lambda R(\w).
\end{equation}

\emph{Logistic regression} is the combination of log loss with $L_2$ regularizer, where the objective function becomes 
\[ \begin{split}
\w^* & = \arg\min_{\w} \; \sum_{i=1}^{N} \ln(1 + e^{-t_i\w^T\xi}) + \frac{\lambda}{2} \w^T\w \\
\end{split} \]

%======================
\subsubsection{Hinge Loss and Support Vector Machine}

The hinge loss is defined as
\[ \begin{split}
L_{hinge}(m_i(\w)) &= \max(0, 1 - m_i(\w)) \\
&= \max(0, 1 - t_i \w^T \xi),
\end{split} \] 
and it can be viewed as a piecewise linear approximation of log loss, because it has the same properties as given for log loss for cases when $m_i >> 1$ and $m_i << -1$. Although being convex, this loss is difficult to optimize directly using the traditional procedure. To be more specific, applying this loss with $L_2$ regularizer to Equation \ref{eq:objective} yields 
\begin{equation}
\label{eq:svmobj}
\w^* = \arg\min_{\w} \; \sum_{i=1}^{N} \max(0,1 - t_i\w^T\xi)  + \frac{\lambda}{2} \w^T\w,
\end{equation}
which is non--differentiable, so typical optimization methods cannot be directly applied. However, the support vector machines (SVM) method proposed by \cite{Vapnik} minimizes this loss by using the maximum-margin classification approach, which we briefly review the key points of this method as follows.

First, suppose that the two classes of the discussed linear binary classification problem are linearly separable by some weight vector $\w$. Then, SVM chooses the decision boundary which maximizes the smallest distance to samples in both classes, which is mathematically defined as
$$ \w^* = \arg \max_{\w} \{ \frac{1}{\|\w\|} \min_i [t_i\w^T \xi] \}. $$

%=================================================
\subsection{Other Methods for Binary Classification}
\label{sec:bgr.others}


%======================
\subsubsection{Bayes Point Machine}

Bayes point machine (BPM) was proposed by \cite{Herbrich}. This is a Bayesian approach to linear classification and is briefly summarized as follows. From our discussion in Subsection \ref{ssec:binclass} we see that, in linear classification, a point $\x$ is classified by $t = \sign(\w^T \x)$ for some weight vector $\w$. Given the training dataset $\D = \{\X, \t \}$, the likelihood for $\w$ is given by
$$p(\D | \w) = \prod_{i=1}^N p(t_i | \xi, \w) = \prod_{i=1}^N \phi(\frac{t_i\w^T\xi}{\epsilon}),$$
where 
$$\phi(z) = \int_{-\infty}^z N(z; 0, 1) dz,$$
and $\epsilon$ controls the allowed `slack'. By using $\phi$ instead of a step function, this likelihood tolerates small errors. Under the Bayesian approach, we also have a prior distribution of $\w$, which is taken to be the zero-mean unit-variance normal distribution: $N(\boldsymbol{0, I})$. Given this model, the optimal way to classify a new data point $\x$ is to vote all classifiers according to their posterior probability: $\mathbb{E}[\sign(\w^T \x) | \D]$. This is, however, expensive to compute. As an approximation to this, BPM uses the output of the average classifier: $\sign(\mathbb{E}[\w] ^T \x)$. This can be estimated by the billiard ball algorithm proposed in the original BPM paper, which is based on the Monte Carlo method. Another (better) way to estimate this is by using expectation propagation as proposed by \cite{bpm}. This thesis uses the Matlab implementation of expectation propagation algorithm for BPM of the latter author in all comparisons where BPM is present. 

%======================
\subsubsection{Other Methods}

Apart from mentioned algorithms, there are quite a few other algorithms that can solve the binary classification problem. Some of the most notable are, for example, the following. Neural networks \cite{bishop2} consist of one or more interconnected layers of neurons, each neuron having its own weight vector. It can be viewed as an adaptive system that changes its structure during learning phase. In contrast to the introduced linear binary classification model, neural networks is generally not linear, except for perceptrons \cite{perceptron}, which can be considered as the simplest kind of neural network. Perceptron, however, may not converge if the two classes are not linearly separable. $k-$ Nearest Neighbor \cite{kNN} is one of the simplest classification algorithms. It assigns a new data point to the class most common amongst its $k$ nearest training data points (neighbors). Adaboost \cite{adaboost}, short for Adaptive Boosting, is an algorithm that coordinate several weak classifiers to improve the classification quality, where subsequent classifiers are tweaked in favor of those instances misclassified by previous classifiers. Adaboost uses exponential loss, which is $L_{exp} = \exp(-m_i(\w)) = \exp(-t_i\w^T\xi)$, hence can be sensitive to noisy data and outliers. Decision trees are also used for classification as shown by \cite{tree1}, and by \cite{tree2}. 

There are also many works that are more or less related to the work of this thesis. We can categorize them into those developing classifiers that are robust to outliers and those optimizing 0--1 loss directly. The number of works in the first category is quite abundant, including t-logistic regression \cite{Ding}, robust truncated hinge loss \cite{robusthinge}, SavageBoost \cite{lossdesign}, and more. There are, however, almost no works in the second category. At the time of finishing this thesis, we can only find the work by \cite{ling}, which proposes an algorithm optimizing 0--1 loss for perceptrons by random coordinate descent. 
