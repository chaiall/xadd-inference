While convex losses for binary classification are attractive due to
the existence of numerous (provably) efficient methods for finding
their global optima, they are sensitive to outliers.  On the other
hand, while the non-convex 0--1 loss is robust to outliers, it is
NP-hard to optimize and hence rarely optimized in practice.  In this
paper, however, we do just that: we explore a variety of practical
methods for direct (approximate) optimization of the 0--1 loss based
on branch and bound search, combinatorial search, and coordinate
descent on smooth, differentiable 0--1 loss relaxations.  Empirically,
we compare our proposed algorithms to logistic regression, SVM, and
the Bayes point machine showing that the proposed 0--1 loss
optimization algorithms perform at least as well and offer a clear
advantage in the presence of outliers.  To this end, we believe this
work reiterates the potential importance of non-convex losses and
their properties while partially challenging long-held notions that
non-convex losses are difficult to optimize.
