\section{Combinatorial Search}
\label{cha:combinatorialsearch}

This section introduces the idea behind the combinatorial search
approach, which is illustrated by Figure~\ref{fig:cs_intro}. In short,
we observe that the hyperplane passing through a set of $D$ points
maps to an equivalance class of hyperplanes all with the same 0--1
loss.  This suggests a simple combinatorial search algorithm to
enumerate all $\choose{N}{D}$ representatives in order to find the one
with minimal 0--1 loss.

\begin{figure}[here]
\includegraphics[width=0.50\textwidth]{images/fig41_intro.eps}
\caption{ \footnotesize This plot illustrates the basic idea behind
  the combinatorial search approach. In the plot, {\bf H} is the
  optimal decision hyperplane to separate the two classes. {\bf H'} is
  obtained from {\bf H} by tuning its direction with changing the 0--1
  loss value, so that it goes through the two nearest points A and B.}
\label{fig:cs_intro}
\end{figure}

To formulate the approach discussed above, 
recall that using homogenous notation, the 0--1 loss for $\xi$ is 
\begin{equation}\label{eq:loss}
L(w_0,\w) = \sum_{i=1}^N \mathbb{I} [w_0 + t_i (\w^T \xi) < 0].
\end{equation}
%In this definition, the non-strict inequality inside the
%indicator function is replaced by a strict inequality.  The only
%difference this makes is in the way how to treat points lying directly
%on the decision hyperplane: before these are deemed to be
%misclassified, now correctly classified. As it is expected that there
%are none or only a few points lying on the decision hyperplane, this
%difference is negligible.
%
%A decision hyperplane that separates the two classes is now given by
%$w_0 + \w^T\boldsymbol{x} = 0$, and the optimal solution $\w_{opt} =
%(w_0^*, \w^*)$ consists of the bias and the weight vector that
%minimizes the 0--1 loss function $L(w_0, \w)$ above. If the bias term
%$w_0$ is zero, the decision hyperplane must go through the origin,
%which is very rare in practice. Thus, it is reasonable to assume that
%$w_0 \not= 0$. 
As noted previously, if both $w_0$ and $w$ are scaled by $|w_0|$, the
solution is unchanged.  To be more specific, there are two
cases. First, if $w_0>0$, then the loss function is:
\[ \begin{split}
L(w_0,\w) &= \sum_{i=1}^N \mathbb{I} [w_0 + t_i (\w^T \xi) < 0] \\
&=\sum_{i=1}^N \mathbb{I} [t_i (\frac{w_0}{|w_0|} + \frac{1}{|w_0|}\w^T \xi) < 0]  \\
&= \sum_{i=1}^N \mathbb{I} [t_i (1 + \w'^T \xi) < 0] \\
&= L(1, \w'),\\
\end{split} \]
where $\w'$ is defined as $\w' = \frac{1}{w_0}\w$. The equation of the
decision hyperplane is as follows:
\[ \begin{split} 
&w_0 + \w^T\boldsymbol{x} = 0 \\
\Leftrightarrow \quad
&\frac{w_0}{|w_0|} + \frac{1}{|w_0|} \w^T \boldsymbol{x}  = 0 \\ \Leftrightarrow \quad
&1 + \w'^T \boldsymbol{x} = 0
\end{split} \]
Second, if $w_0 < 0$, then $\w' = \frac{1}{w_0}\w = -\frac{1}{|w_0|}\w$, and the loss function is 
\[ \begin{split}
L(w_0,\w) &= \sum_{i=1}^N \mathbb{I} [w_0 + t_i (\w^T \xi) < 0] \\
&=\sum_{i=1}^N \mathbb{I} [t_i (\frac{w_0}{|w_0|} + \frac{1}{|w_0|}\w^T \xi) < 0]  \\
&= \sum_{i=1}^N \mathbb{I} [t_i (-1 - \w'^T \xi) < 0] 
= L(-1, -\w'),\\
\end{split} \]
and the equation of the decision hyperplane is 
\[ \begin{split} 
&w_0 + \w^T\boldsymbol{x} = 0 \\
\Leftrightarrow \quad
&\frac{w_0}{|w_0|} + \frac{1}{|w_0|} \w^T \boldsymbol{x}  = 0 \\ \Leftrightarrow \quad
&-1 - \w'^T \boldsymbol{x} = 0 \\
\Leftrightarrow \quad
&1 + \w'^T \boldsymbol{x} = 0
\end{split} \]
It can be seen, that the equation of the decision hyperplane ($1 +
\w'^T \boldsymbol{x} = 0$) is the same in both cases, and the loss
function is either $L(1, \w')$ or $L(-1, -\w')$, i.e., the bias term
is now a either $1$ or $-1$. As shall be
seen next, this fact is critically important for the purpose of the
combinatorial search approach. The discussion at the beginning of this
section pointed out, that to find the optimal solution, it suffices to
check all hyperplanes that go through $D$ points of the training
dataset and find the one that has minimum 0--1 loss. So, assuming
$\boldsymbol{x_1}, \dots, \boldsymbol{x_D}$ are (any) $D$ distinct
data points from the training dataset, then for the combinatorial
search to work, the two following tasks must be solved:
\begin{itemize}
\item Find the weight vector $\w'=(w'_1, \dots, w'_D)^T$ of the
  decision hyperplane that goes through these $D$ selected points.
\item Calculate the 0--1 loss value corresponding to $\w'$.
\end{itemize}

For the first task, because the hyperplane goes through the given $D$
data points, at each point the hyperplane equation must be
satisfied. So,
$$ 1+ \w'^T\xi = 0, \quad\quad \text{for } i = 1, 2, \dots, D.$$
Now, let $A$ be a matrix, whose rows are formed by these points: 
$A =(\boldsymbol{x_1}\; \boldsymbol{x_2}\; \dots \; \boldsymbol{x_D})^T$,
then the weight vector $\w'$ must satisfy the following matrix equation 
\begin{equation}\label{eq:matrix}
\boldsymbol{1} + A \w' = 0 \Longrightarrow \w' = -A^{-1}\boldsymbol{1},
\end{equation}
where $\boldsymbol{1}$ is the unit vector in $\R^D$, because each row
of this matrix equation corresponds to an equation for one point given
above. Here, one sees that if the bias term $w_0$ was still present,
the above equation would be undetermined.

Now, with $\w'$ specified, the second task becomes easy, as the 0--1
loss value is obviously the smaller value of $L(1,\w')$ and $L(-1,
-\w')$. Thus, if $L(1,\w') \leq L(-1,-\w')$, the 0--1 loss value
corresponding to decision hyperplane $1+\w'^T\boldsymbol{x} = 0$ is
$L(1,\w')$, and the solution vector (including bias and weights) is
$(1, \w')$, otherwise, the 0--1 loss value is $L(-1,-\w')$, and the
solution vector is $(-1,-\w')$.

The above discussion represents all necessary knowledge for the
combinatorial search approach and it is now possible to build
algorithms based on the foundation presented here.  We present
two variants: one provably optimal and one approximate.

\noindent\emph{Prioritized Combinatorial Search (PCS)}: This algorithm
exploits the fact that combinations of data points lying closer to an
initial approximated decision hyperplane (e.g., given by an SVM) are
more likely to produce the optimal hyperplane than combinations of
points lying far away.  The algorithm in
Figure~\ref{alg:cs.prioritized} captures the above idea by considering
combinations of $D$ points in the increasing order of their distance
to the approximated decision hyperplane, where the distance of a set
of points is the minimal distance from these points to the
approximated decision hyperplane. We remark that in the worst case,
this algorithm can find an optimal solution in $O(D^3 \choose{N}{D})$ 
time ($\choose{N}{D}$ iterations requiring a $D \times D$ matrix inversion)
and observe that for small $D$, this will be much more efficient than \BB.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[tp!]
\vspace{-3mm}
\caption{
Prioritized Combinatorial Search (PCS).\hfill\; \\
%\text{\hspace{2.1cm}} $Input$: Dataset of training data points $ \boldsymbol{X}$, their labelled class targets $\t$. \\
%\text{\hspace{2.1cm}} $Output$: Optimal weight vector $\w^*$ minimizing 0--1 loss.
}
\label{alg:cs.prioritized}
{\footnotesize 
\begin{algorithmic}[1]
\Function{Find-Optimal-01Loss-Solution}{$\X, \t$} \Comment{returns $\w^*$}
\State $\w^* \gets \w^*_{\mathit{SVM}}$ from SVM solution for $(\boldsymbol{X},\t)$
\State $loss_{min} \gets$ 0--1 loss implied by $\w^*$
\State $\boldsymbol{i} \gets$ indices of $\xi$ ordered by $|{\w^*}^T\boldsymbol{x_k}|$, for $k=1..N$.
\State $\p \gets [1,2,\dots,D]$
\Comment{first combination of $D$ points}
\While{$\p \not= \emptyset$}
   \State $(\w,loss) \gets$ \Call{Get-Solution}{$\p$}
   \If {$loss < loss_{min}$}
      \State $(\w^*, loss_{min}) \gets (\w, loss)$
   \EndIf
   \State $\p \gets $ next combination of ${N \choose D}$, or $\emptyset$ if no more.
\EndWhile
\State \Return $\w^*$
\Statex
\Function{Get-Solution}{$\p$} 
   \State $A \gets (x_{i[p_1]} \, x_{i[p_2]} \, \dots \, x_{i[p_D]})^T$
   \State $\w' \gets -A^{-1} \boldsymbol{1}$
   \If {$L(1,\w') \leq L(-1,-\w')$}
      \State $\w \gets (1, \w')$
      \State $loss \gets L(1,\w')$
   \Else
      \State $\w \gets (-1, -\w')$
      \State $loss \gets L(-1,-\w')$
   \EndIf
   \State \Return {$(\w, loss)$} \Comment{corresponding to $\p$}
\EndFunction
\Statex
\EndFunction
\end{algorithmic}}
\vspace{-4mm}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\COMMENT

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[tp!]
\vspace{-3mm}
\caption{
Combinatorial Search Approximation (CSA).\hfill \; \\
%\text{\hspace{2.1cm}} $Input$: Dataset of training data points $ \boldsymbol{X}$, their labelled class targets $\t$. \\
%\text{\hspace{2.1cm}} $Output$: Optimal weight vector $\w^*$ minimizing 0--1 loss.
}
\label{alg:cs.approximation}
{\footnotesize 
\begin{algorithmic}[1]
\Function{Approximate-01Loss-Solution}{$\X, \t$} \Comment{returns $\w^*$}
\State $\w^* \gets$ approximated weight vector given by an SVM
\State $\boldsymbol{i} \gets$ indices of points ordered by $|{\w^*}^T\boldsymbol{x_k}|$, for $k=1, \dots, N$.
\State $(\p, \w^*, loss_{min}) \gets$ \Call{Get-Best-Initial-Solution}{$30$}
\Loop
   \For {$k=1$ to $N$}
      \If {$k \not\in \p$}
         \For {j=1 to D} 
            \State $\p' \gets \p$
            \State $p'_j \gets k$
            \Comment{replace $j-$th component by $k$}
            \State $(\w, loss) \gets$ \Call{Get-Solution}{$\p'$}
            \If {$loss < loss_{min}$}
               \State $(\p, \w^*, loss_{min}) \gets (\p', \w, loss)$
               \State {\bf go back to step 5}
            \EndIf
         \EndFor
      \EndIf
   \EndFor
   \State \Return $\w^*$
 \EndLoop
\Statex
\Function{Get-Best-Initial-Solution}{$N'$} \Comment{returns $(\p, \w, loss)$}
   \State $loss \gets +\infty$
   \State $\p_t \gets [1,2,\dots,D]$
   \Comment{initialize the first combination of $D$ points}
   \While{$\p_t \not= \emptyset$}
      \State $(\w_t, loss_t) \gets$ \Call{Get-Solution}{$\p_t$}
      \If {$loss_t < loss$}
         \State $(\p, \w, loss) \gets (\p_t, \w_t, loss_t)$
      \EndIf
      \State $\p_t \gets $ next combination of ${N' \choose D}$, or $\emptyset$ if there is no more.
   \EndWhile
   \State \Return $(\p, \w, loss)$
\EndFunction
\Statex
\EndFunction
\end{algorithmic}}
\vspace{-4mm}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\ENDCOMMENT

\noindent \emph{Combinatorial Search Approximation (CSA)}: Rather than
systematically enumerating all combinations as in prioritized search,
we start from an initial ``best'' combination of $D$ points near an
approximated decision hyperplane given by an SVM, then at each
iteration, we swap two points $(\boldsymbol{x_k, x_j})$ in/out of the
current combination. The algorithm stops when it cannot find any more
points to swap.  We do not present the full algorithm here due to
space limitations but note that it is a slight variation on PCS 
discussed previously.
