\label{sec:csamdp}

We first introduce general continuous state and action Markov decision
processes (CSA-MDPs) and then our specific restricted version of
CSA-MDPs with piecewise linear continuous transitions,
arbitrary stochastic discrete transitions, and quadratic (or
lower-order) reward.  We then provide an algorithm their
finite-horizon solution via symbolic dynamic programming by
extending~\cite{sanner_uai11}.

\subsection{Factored Representation}

In a CSA-MDP, states will be represented by vectors of variables
$(\vec{b},\vec{x}) = ( b_1,\ldots,b_n,x_{1},\ldots,x_m )$.  We assume
that each state variable $b_i$ ($1 \leq i \leq n$) is
boolean$\,$ s.t. $b_i \in \{ 0,1 \}$ and each $x_j$ ($1 \leq j \leq m$) is 
continuous s.t. $x_j \in [L_j,U_j]$ for $L_j,U_j \in
\mathbb{R}; L_j \leq U_j$.  We also assume a finite set of $p$ actions $A
= \{ a_1(\vec{y}_1), \ldots, a_p(\vec{y}_p) \}$, where the $\vec{y}$ denote
\emph{continuous action parameters} for the corresponding action.

A CSA-MDP is defined by the following: (1) a state transition model
$P(\vec{b}',\vec{x}'|\cdots,a,\vec{y})$, which specifies the
probability of the next state $(\vec{b}',\vec{x}')$ conditioned on a
subset of the previous and next state and action $a(\vec{y})$; (2) a
reward function $R(\vec{b},\vec{x},a,\vec{y})$, which specifies the
immediate reward obtained by taking action $a(\vec{y})$ in state
$(\vec{b},\vec{x})$; and (3) a discount factor $\gamma, \; 0 \leq
\gamma \leq 1$.\footnote{If time is explicitly included as one of the
continuous state variables, $\gamma = 1$ is typically used, unless
discounting by horizon (different from the state variable time) is
still intended.}  A policy $\pi$ specifies the action $a(\vec{y}) =
\pi(\vec{b},\vec{x})$ to take in each state $(\vec{b},\vec{x})$.  Our
goal is to find an optimal sequence of horizon-dependent policies
$\Pi^* = (\pi^{*,1},\ldots,\pi^{*,H})$ that maximizes the expected sum
of discounted rewards over a horizon $h \in H; H \geq
0$:\footnote{$H=\infty$ is allowed if an optimal policy has a finitely
bounded value (guaranteed if $\gamma < 1$); for $H=\infty$, the
optimal policy is independent of horizon, i.e., $\forall h \geq 0,
\pi^{*,h} = \pi^{*,h+1}$.}
\begin{align}
V^{\Pi^*}(\vec{x}) & = E_{\Pi^*} \left[ \sum_{h=0}^{H} \gamma^h \cdot r^h \Big| \vec{b}_0,\vec{x}_0 \right]. \label{eq:vfun_def}
\end{align}
Here $r^h$ is the reward obtained at horizon $h$ following $\Pi^*$ where 
we assume starting state $(\vec{b}_0,\vec{x}_0)$ at $h=0$.
 
CSA-MDPs as defined above are naturally factored~\cite{boutilier99dt}
in terms of state variables $(\vec{b},\vec{x},\vec{y})$; as such
transition structure can be exploited in the form of a dynamic Bayes
net (DBN)~\cite{dbn} where the individual conditional probabilities
$P(b_i'|\cdots)$ and $P(x_j'|\cdots)$ for each next state variable
condition on the action and (a subset of) the variables in the action,
current and next state.  For simplicity of exposition, we assume there
are no \emph{synchronic arcs} (variables that condition on each other
in the same time slice) within the binary $\vec{b}$ and continuous
variables $\vec{x}$, but we allow synchronic arcs from variables in
$\vec{b}$ to variables in $\vec{x}$.\footnote{Synchronic arcs between
variables within $\vec{b}$ or within $\vec{x}$ can be accommodated if
the forthcoming Algorithm~\ref{alg:regress} (\texttt{Regress}) 
is modified to
eliminate child variables before parent variables in the directed
acyclic graph (DAG) formed by the DBN.}  We write the joint transition
model as
\begin{align}
P(\vec{b}',&\vec{x}'|\vec{b},\vec{x},a,\vec{y}) = \label{eq:dbn} \\
& \prod_{i=1}^n P(b_i'|\vec{b},\vec{x},a,\vec{y}) \prod_{j=1}^m P(x_j'|\vec{b},\vec{b}',\vec{x},a,\vec{y}). \nonumber 
\end{align}

% can always linearize a quadratic decision (give example), 
% more general reward: parameterized quadratically constrained 
%   quadratic programs
% transition: stochasticity would mean int_n V(x,n) (= { x + n) P(n)

We call the conditional probabilities
$P(b_i'|\vec{b},\vec{x},a,\vec{y})$ for \emph{binary} variables $b_i$
($1 \leq i \leq n$) conditional probability functions (CPFs) --- not
tabular enumerations, because in general these functions can condition
on both discrete and continuous state as
in~\eqref{eq:mr_discrete_trans}.  For the \emph{continuous} variables
$x_j$ ($1 \leq j \leq m$), we represent the CPFs
$P(x_j'|\vec{b},\vec{b'},\vec{x},a,\vec{y})$ with \emph{piecewise
linear equations} (PLEs) satisfying three properties: (1) PLEs are
first-order \emph{Markov}, meaning that they can only condition on the
previous state, (2) PLEs are deterministic, and (2) PLEs are piecewise
linear, where the piecewise conditions may be arbitrary logical
combinations of $\vec{b}$ and \emph{linear inequalities} over $\vec{x}$.  An
example PLE has been provided in~\eqref{eq:mr_cont_trans} where the
use of the Dirac $\delta[\cdot]$ function ensures that this is a
conditional probability function that integrates to 1 over $x'$; In
more intuitive terms, one can see that this $\delta[\cdot]$ is a
simple way to encode the PLE transition $x' = \begin{cases} \ldots
\end{cases}$ as a probability distribution, which is required in
general when PLEs are used to encode the
$P(x_j'|\vec{b},\vec{b'},\vec{x},a,\vec{y})$ as in this work.

It is important at this point to qualify the extent of stochasticity
permitted in this restriction of CSA-MDPs.  While it will be clear
that our restrictions do not permit general stochastic transition noise
(e.g., Gaussian white noise as in LQR), they do permit discrete noise
in the sense that $P(x_j'|\vec{b},\vec{b'},\vec{x},a,\vec{y})$
may condition on $\vec{b'}$, which are stochastically sampled according
to their CPFs.  
We note that this representation effectively allows modeling of
continuous variable transitions as a mixture of $\delta$ functions,
which has been used heavily in previous exact continuous state MDP 
solutions~\cite{feng04,hao09}.
% TODO: Not li05 here (allowed PWC probability), another?

% Future work: To incorporate more general
% forms of noise would generally lead to increase in order
% of polynomials

We allow the reward function $R_a(\vec{b},\vec{x},\vec{y})$ to be either
(1) a general piecewise linear function (boolean or linear conditions
and linear valuations) such as
\begin{align}
R_a(\vec{b},\vec{x},\vec{y}) = \begin{cases}
x_1 \leq x_2 + 1 : & 1 - x_1 + 2x_2 \\
x_1 > x_2 + 1:     & 3x_1 + 2x_2 \\
\end{cases} \label{eq:linear_reward}
\end{align}
or (2) a piecewise quadratic function restricted to be quadratic in
one state variable and linear in *** such as~\eqref{eq:mr_reward}.  In
the concluding remarks, we discuss the implications of
relaxing the above restrictions on the transition and reward
functions.  For now, with our CSA-MDP defined as above, we proceed
to discuss how to solve for it's optimal finite horizon value
function and policy.

\subsection{Solution Methods}

\label{sec:soln}

Now we provide a continuous state generalization of {\it
value iteration}~\cite{bellman}, which is a dynamic programming
algorithm for constructing optimal policies.  It proceeds by
constructing a series of $h$-stage-to-go value functions
$V^h(\vec{b},\vec{x})$.  Initializing $V^0(\vec{b},\vec{x})$ 
(e.g., to $V^0(\vec{b},\vec{x}) = 0$) 
we define the quality of taking action $a$ in state
$(\vec{b},\vec{x})$ and acting so as to obtain $V^{h}(\vec{b},\vec{x})$ 
thereafter as the following:
\vspace{-3mm}

{\footnotesize
\begin{align}
& Q_a^{h+1}(\vec{b},\vec{x}) = \max_{\vec{y}} \Bigg( R_a(\vec{b},\vec{x}) + \gamma \cdot \label{eq:qfun} \\ 
& \sum_{\vec{b}'} \int_{\vec{x}'} \left( \prod_{i=1}^n P(b_i'|\vec{b},\vec{x},a,\vec{y}) \prod_{j=1}^m P(x_j'|\vec{b},\vec{b}',\vec{x},a,\vec{y}) \right) V^h(\vec{b}',\vec{x}') d\vec{x}' \Bigg) \nonumber
\end{align}}

Given $Q_a^h(\vec{b},\vec{x})$ for each $a \in A$, we can proceed
to define the $h+1$-stage-to-go value function as follows:
\begin{align}
V^{h+1}(\vec{b},\vec{x}) & = \max_{a \in A} \left\{ Q^{h+1}_a(\vec{b},\vec{x}) \right\} \label{eq:vfun}
\end{align}

If the horizon $H$ is finite, then the optimal value function is
obtained by computing $V^H(\vec{b},\vec{x})$ and the optimal
horizon-dependent policy $\pi^{*,h}$ at each stage $h$ can be easily
determined via 
$\pi^{*,h}(\vec{b},\vec{x}) = \argmax_a Q^h_a(\vec{b},\vec{x})$.  
If the horizon 
$H = \infty$ and the optimal policy has finitely bounded value, 
then value iteration can terminate at horizon $h+1$ if 
$V^{h+1} = V^{h}$; then 
$\pi^*(\vec{b},\vec{x}) = \argmax_a Q^{h+1}_a(\vec{b},\vec{x})$.

Of course this is simply the \emph{mathematical} definition, next
we show each of these algebraic operations can be computed for
CSA-MDPs.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\incmargin{.5em}
\linesnumbered
\begin{algorithm}[t!]
\dontprintsemicolon
\SetKwFunction{regress}{Regress}
\Begin{
   $V^0:=0, h:=0$\;
   \While{$h < H$}{
       $h:=h+1$\;
       \ForEach {$a(\vec{y}) \in A$}{
              $Q_a^{h}(\vec{y})\,:=\,$\regress{$V^{h-1},a,\vec{y}$}\;
              $Q_a^{h} := \max_{\vec{y}} \, Q_a^{h}(\vec{y})$ $\,$ \emph{// Continuous $\max$}\;
              $V^{h} := \max_a Q_a^{h}$ $\,$ \emph{// Discrete $\max$}\;
              $\pi^{*,h} := \argmax_{(a,\vec{y})} \, Q_a^{h}(\vec{y})$\;
       }
       \If{$V^h = V^{h-1}$}
           {break $\,$ \emph{// Terminate if early convergence}\;}
   }
     \Return{$V^h$} \;
}
\caption{\footnotesize \texttt{Solve}(CSA-MDP, $H$) \label{alg:vi}}
\end{algorithm}
\decmargin{.5em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\incmargin{.5em}
\linesnumbered
\begin{algorithm}[t!]
\dontprintsemicolon
\SetKwFunction{remapWithPrimes}{Prime}
%\SetKwFunction{sumout}{sumout}


\Begin{
    $Q=$ \remapWithPrimes{$V$} $\,$ \emph{// All $b_i \to b_i'$ and all $ x_i \to x_i'$} \;
    \emph{// Continuous regression marginal}\\
    \For {all $x'_j$ in $Q$}{
         $Q := \int Q \otimes P(x_j'|\vec{b},\vec{b}',\vec{x},a,\vec{y}) \, d_{x'_j}$\;
    }
    \emph{// Discrete regression marginal}\\
    \For {all $b'_i$ in $Q$}{
         $Q := \left[ Q \otimes P(b_i'|\vec{b},\vec{x},a,\vec{y}) \right]|_{b_i' = 1}$\\
         \hspace{8mm} $\oplus \left[ Q \otimes P(b_i'|\vec{b},\vec{x},a,\vec{y}) \right]|_{b_i' = 0}$\;
    }
    \Return{$R(\vec{b},\vec{x},a,\vec{y}) \oplus (\gamma \otimes Q)$} \;
}
\caption{\footnotesize \texttt{Regress}($V,a,\vec{y}$)\label{alg:regress}}
\end{algorithm}
\decmargin{.5em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
