\label{sec:sdp}

In this section, we extend the symbolic dynamic programming (SDP) work
of~\cite{sanner_uai11} to the case of continuously parameterized
actions for CSA-MDPs.  We present the general SDP framework for value
iteration in Algorithm~\ref{alg:vi} (\texttt{VI}) and a Q-function
regression subroutine~\ref{alg:regress} (\texttt{Regress}) where we
have omitted parameters $\vec{b}$ and $\vec{x}$ from $V$ and $Q$ to
avoid notational clutter.  We note the single difference between this
algorithm and that described in~\cite{sanner_uai11} comes in the
continuous action parameter maximization in line 7 of \texttt{VI}.
Before we explain this contribution though, we first recap the SDP
solution, which requires that the CSA-MDP and all other
functions such as $Q$ and $V$ are represented in \emph{case} form and
all operations are \emph{case operations}, defined next.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\incmargin{.5em}
\linesnumbered
\begin{algorithm}[t!]
\dontprintsemicolon
\SetKwFunction{regress}{Regress}
\Begin{
   $V^0:=0, h:=0$\;
   \While{$h < H$}{
       $h:=h+1$\;
       \ForEach {$a(\vec{y}) \in A$}{
              $Q_a^{h}(\vec{y})\,:=\,$\regress{$V^{h-1},a,\vec{y}$}\;
              $Q_a^{h} := \max_{\vec{y}} \, Q_a^{h}(\vec{y})$ $\,$ \emph{// Continuous $\max$}\;
              $V^{h} := \casemax_a \, Q_a^{h}$ $\,$ \emph{// $\casemax$ all $Q_a$}\;
              $\pi^{*,h} := \argmax_{(a,\vec{y})} \, Q_a^{h}(\vec{y})$\;
       }
       \If{$V^h = V^{h-1}$}
           {break $\,$ \emph{// Terminate if early convergence}\;}
   }
     \Return{$(V^h,\pi^{*,h})$} \;
}
\caption{\footnotesize \texttt{VI}(CSA-MDP, $H$) $\longrightarrow$ $(V^h,\pi^{*,h})$ \label{alg:vi}}
\end{algorithm}
\decmargin{.5em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\incmargin{.5em}
\linesnumbered
\begin{algorithm}[t!]
\dontprintsemicolon
\SetKwFunction{remapWithPrimes}{Prime}
%\SetKwFunction{sumout}{sumout}


\Begin{
    $Q=$ \remapWithPrimes{$V$} $\,$ \emph{// All $b_i \to b_i'$ and all $ x_i \to x_i'$} \;
    \emph{// Continuous regression marginal integration}\\
    \For {all $x'_j$ in $Q$}{
         $Q := \int Q \otimes P(x_j'|\vec{b},\vec{b}',\vec{x},a,\vec{y}) \, d_{x'_j}$\;
    }
    \emph{// Discrete regression marginal summation}\\
    \For {all $b'_i$ in $Q$}{
         $Q := \left[ Q \otimes P(b_i'|\vec{b},\vec{x},a,\vec{y}) \right]|_{b_i' = 1}$\\
         \hspace{8mm} $\oplus \left[ Q \otimes P(b_i'|\vec{b},\vec{x},a,\vec{y}) \right]|_{b_i' = 0}$\;
    }
    \Return{$R(\vec{b},\vec{x},a,\vec{y}) \oplus (\gamma \otimes Q)$} \;
}
\caption{\footnotesize \texttt{Regress}($V,a,\vec{y}$) $\longrightarrow$ $Q$ \label{alg:regress}}
\end{algorithm}
\decmargin{.5em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Case Representation and Operators}

From here out, we assume that all symbolic functions
can be represented in \emph{case} form~\cite{fomdp}:
{%\footnotesize 
\begin{align}
f = 
\begin{cases}
  \phi_1: & f_1 \\ 
 \vdots&\vdots\\ 
  \phi_k: & f_k \\ 
\end{cases} \label{eq:case}
\end{align}
}
Here the $\phi_i$ are logical formulae defined over the state
$(\vec{b},\vec{x})$ that can include arbitrary logical ($\land,\lor,\neg$)
combinations of (i) boolean variables and (ii) 
\emph{linear} inequalities ($\geq,>,\leq,<$) 
over continuous variables.  
Each $\phi_i$ will be disjoint from the other $\phi_j$ ($j \neq i$); 
however the $\phi_i$ may not exhaustively cover the state space, hence
$f$ may only be a \emph{partial function} and may be undefined for some
variable assignments.
%\footnote{In the context of SDP, states whose value
%at horizon $h$ is undefined correspond to states that cannot reach any
%defined state of the reward in horizon $h$.}
The $f_i$ may be either linear or quadratic in the continuous parameters
according to the same restrictions as for 
$R(\vec{b},\vec{x},a,\vec{y})$.

\emph{Unary operations} such as scalar multiplication $c\cdot f$ (for
some constant $c \in \mathbb{R}$) or negation $-f$ on case statements
$f$ are straightforward; the unary operation is simply applied to each
$f_i$ ($1 \leq i \leq k$). Intuitively, to perform a \emph{binary
  operation} on two case statements, we simply take the cross-product
of the logical partitions of each case statement and perform the
corresponding operation on the resulting paired partitions.  Letting
each $\phi_i$ and $\psi_j$ denote generic first-order formulae, we can
perform the ``cross-sum'' $\oplus$ of two (unnamed) cases in the
following manner:

{\footnotesize 
\begin{center}
\begin{tabular}{r c c c l}
&
\hspace{-6mm} 
  $\begin{cases}
    \phi_1: & f_1 \\ 
    \phi_2: & f_2 \\ 
  \end{cases}$
$\oplus$
&
\hspace{-4mm}
  $\begin{cases}
    \psi_1: & g_1 \\ 
    \psi_2: & g_2 \\ 
  \end{cases}$
&
\hspace{-2mm} 
$ = $
&
\hspace{-2mm}
  $\begin{cases}
  \phi_1 \wedge \psi_1: & f_1 + g_1 \\ 
  \phi_1 \wedge \psi_2: & f_1 + g_2 \\ 
  \phi_2 \wedge \psi_1: & f_2 + g_1 \\ 
  \phi_2 \wedge \psi_2: & f_2 + g_2 \\ 
  \end{cases}$
\end{tabular}
\end{center}
}
\normalsize

Likewise, we can perform $\ominus$ and $\otimes$ by,
respectively, subtracting or multiplying partition values (as opposed
to adding them) to obtain the result.  Some partitions resulting from
the application of the $\oplus$, $\ominus$, and $\otimes$ operators
may be inconsistent (infeasible); we may simply discard such 
partitions as they are irrelevant to the function value.

For SDP, we'll also need to perform maximization, restriction,
and substitution on case statements.  
\emph{Symbolic case maximization} is fairly straightforward
to define:
\vspace{-4mm}

{\footnotesize
\begin{center}
\begin{tabular}{r c c c l}
&
\hspace{-7mm} $\casemax \Bigg(
  \begin{cases}
    \phi_1: \hspace{-2mm} & \hspace{-2mm} f_1 \\ 
    \phi_2: \hspace{-2mm} & \hspace{-2mm} f_2 \\ 
  \end{cases}$
$,$
&
\hspace{-4mm}
  $\begin{cases}
    \psi_1: \hspace{-2mm} & \hspace{-2mm} g_1 \\ 
    \psi_2: \hspace{-2mm} & \hspace{-2mm} g_2 \\ 
  \end{cases} \Bigg)$
&
\hspace{-4mm} 
$ = $
&
\hspace{-4mm}
  $\begin{cases}
  \phi_1 \wedge \psi_1 \wedge f_1 > g_1    : & \hspace{-2mm} f_1 \\ 
  \phi_1 \wedge \psi_1 \wedge f_1 \leq g_1 : & \hspace{-2mm} g_1 \\ 
  \phi_1 \wedge \psi_2 \wedge f_1 > g_2    : & \hspace{-2mm}f_1 \\ 
  \phi_1 \wedge \psi_2 \wedge f_1 \leq g_2 : & \hspace{-2mm} g_2 \\ 
  \phi_2 \wedge \psi_1 \wedge f_2 > g_1    : & \hspace{-2mm} f_2 \\ 
  \phi_2 \wedge \psi_1 \wedge f_2 \leq g_1 : & \hspace{-2mm} g_1 \\ 
  \phi_2 \wedge \psi_2 \wedge f_2 > g_2    : & \hspace{-2mm} f_2 \\ 
  \phi_2 \wedge \psi_2 \wedge f_2 \leq g_2 : & \hspace{-2mm} g_2 \\ 
  \end{cases}$
\end{tabular}
\end{center}
} We remark that if all $f_i$ and $g_i$ are linear,
the $\casemax$ result is clearly still linear.  If the $f_i$ or $g_i$
are quadratic according to the previous reward restriction, it will
shortly become obvious that the expressions $f_i > g_i$ or $f_i \leq
g_i$ will be at most univariate quadratic and any such quadratic
inequality can be \emph{linearized} into a logical combination of at most 
two linear inequalities by completing the square (e.g., $x^2 + 8x > 0
\equiv [x + 4]^2 > 4 \equiv [x \leq -2] \lor [x > 6]$).  Hence
according to the earlier restrictions, the result of this $\casemax$
operator will be representable in the case format previously described
(i.e., linear inequalities in decisions).

There are two operations in \texttt{Regress} that we have not defined
yet.  The first operation of \emph{boolean restriction} required in
lines 8--9 is obvious and an example is omitted: in this operation
$f|_{b=v}$, anywhere a boolean variable $b$ occurs in $f$, we assign
it the value $v \in \{ 0,1 \}$.  The second operation of
\emph{continuous regression} $\int Q(x'_j) \otimes P(x'_j|\cdots)
dx'_j$ is required in line 5; as previously defined, $P(x_j'|\cdots)$
will always be of the form $\delta[x_j' - h(\vec{z})]$ where
$h(\vec{z})$ is a case statement and $\vec{z}$ does not contain
$x'_j$.  Rules of integration then tell us that $\int f(x'_j) \otimes
\delta[x_j' - h(\vec{z})] dx'_j = f(x'_j) \{ x'_j / h(\vec{z}) \}$
where the latter operation indicates that any occurrence of $x'_j$ in
$f(x'_j)$ is \emph{symbolically substituted} with the case statement
$h(\vec{z})$.  The full specification of this operation was a key
contribution of~\cite{sanner_uai11} so we refer the reader to that
paper for further technical details.  The
important insight is that this $\int$ operation yields a result that is a
case statement in the form previously outlined.

\subsection{Maximization of Continuous Parameters}

The only operation in \texttt{VI} and \texttt{Regress} that has not
yet been defined is the continuous action maximization in line 7 of
\texttt{VI} that forms the key novel contribution of this paper.
Reintroducing suppressed state variables and renaming $Q_a^{h}$ to
$f$, we write this operation as $g(\vec{b},\vec{x}) := \max_{\vec{y}}
\, f(\vec{b},\vec{x},\vec{y})$ --- the key point to realize here is
that \emph{the} maximizing $\vec{y}$ is a function
$g(\vec{b},\vec{x})$.  We work through the derivation of $g$ in this
section, which amounts to \emph{symbolic} constrained optimization 
subject to unknown state parameters $\vec{b}$ and $\vec{x}$.

From here out we assume that all case partition conditions $\phi_i$ of
$f$ consist of conjunctions of non-negated linear inequalities and
possibly negated boolean variables --- a condition easy to enforce
since disjunctions can be split across multiple non-disjunctive, 
disjoint case partitions, e.g., {\footnotesize
\begin{equation}
f = 
\begin{cases}
a \lor b: & f_1 \\ 
\neg a \land \neg b: & f_2\\ 
\end{cases} 
\; \; \; =  \; \; \;
\begin{cases}
a: & f_1 \\ 
\neg a \land b: & f_1 \\ 
\neg a \land \neg b: & f_2\\ 
\end{cases} 
\end{equation}}
and negation
inverts inequalities, e.g., $\neg [x < 2] \equiv [x \geq 2]$.

Exploiting the commutativity of $\max$, we can first rewrite any
multivariate $\max_{\vec{y}}$ as a sequence of univariate $\max$
operations $\max_{y_1} \cdots \max_{y_{|\vec{y}|}}$; hence it suffices
to provide just the \emph{univariate} $\max_y$ solution: 
$g(\vec{b},\vec{x}) := \max_{y} \, f(\vec{b},\vec{x},y)$.

We can rewrite $f(\vec{b},\vec{x},y)$ via 
the following equalities:
{\footnotesize
\begin{align*}
\max_y f(\vec{b},\vec{x},y) & = 
% cannot use summation form... it assigns the value 0 to illegal cases!
% \max_y \sum_i \phi_i(\vec{b},\vec{x},y) f_i(\vec{b},\vec{x},y)\\
\max_y \casemax_i \, \phi_i(\vec{b},\vec{x},y) f_i(\vec{b},\vec{x},y)\\
& = \casemax_i \max_y \phi_i(\vec{b},\vec{x},y) f_i(\vec{b},\vec{x},y).
\end{align*}}
%Because the 
%$\phi_i$ are mutually disjoint and exhaustive, 
%$f(\vec{b},\vec{x},y) = \casemax_i \, \phi_i(\vec{b},\vec{x},y) f_i(\vec{b},\vec{x},y)$.  
The first equality is a consequence of the mutual 
disjointness of the partitions in $f$.  Then because 
$\max_y$ and $\casemax_i$ are commutative and may be reordered,
we can compute $\max_y$ for \emph{each case partition
individually}.  Thus to complete this section we need only
show how to symbolically compute a single partition 
$\max_y \phi_i(\vec{b},\vec{x},y) f_i(\vec{b},\vec{x},y)$.

To make the partition maximization procedure concrete, 
we use an example that arises in the \MarsRover\ problem.  
This partition $i$ (resulting
from applying SDP) has conditions $\phi_i(x,b,y) \equiv \neg b \wedge
(x\geq 2) \wedge (y\leq 10) \wedge (y\geq -10) \wedge (y\leq 2-x) \wedge
(y\geq -2-x)$ and value $f_i(x,y) = 4 - (x+y)^2 $.

In $\phi_i$, we observe that each conjoined constraint serves one of
three purposes: (i) \emph{upper bound on $y$}: it can be written
as $y < \cdots$ or $y \leq \cdots$ (i.e., $y \leq 10$, $y \leq 2 -
x$), (ii) \emph{lower bound on $y$}: it can be written as $y >
\cdots$ or $y \geq \cdots$ (i.e., $d \geq -10$, $d \geq x - 2$), or
(iii) \emph{independent of $y$}: the constraints do not contain $y$
and can be safely factored outside of the $\max_y$ (i.e., 
$\IND = \neg b \land (x \geq 2)$).  
Because there are multiple symbolic upper and lower
bounds on $y$, in general we will need to apply the $\casemax$
($\casemin$) operator to determine the highest lower bound $\LB$
(lowest upper bound $\UB$):
%\footnote{We could have also written
%the $\LB$ with respective constraints $x < 8$ and $x \geq 8$ since
%the $\casemax$ ensures that the two adjoining case partitions
%have the same valuation on their mutual boundary (by definition
%the boundary is where the two functions meet).  Likewise for
%$\UB$ and $\casemin$.  This note is important here because then we
%can safely take limits and replace $<$ with $\leq$ and  
%$>$ with $\geq$ when examining the lower and upper bounds on $y$.}
{\footnotesize
\begin{align*}
\LB = \casemax(-10,-2 -x) & = \begin{cases}
x \leq 8: & -2 -x \\ 
x > 8: &-10\\ 
\end{cases}\\
\UB = \casemin(10, 2-x) & = \begin{cases}
x \geq -8: & 2 -x \\ 
x < -8: &10\\ 
\end{cases}
\end{align*}
} We know that $\max_y \phi_i(\vec{b},\vec{x},y)
f_i(\vec{b},\vec{x},y)$ for a continuous function $f_i$ (here at most
quadratic) must occur at the critical points of the function --- 
either the upper or lower bounds ($\UB$ and $\LB$) of $y$, 
or the $\Root$ (i.e., zero) of $\frac{\partial}{\partial y} f_i$ 
w.r.t.\ $y$ (because $f_i$ is at most quadratic, there exists 
at most one $\Root$).  Here each of $\UB$, $\LB$, and $\Root$
is a symbolic function of $\vec{b}$ and $\vec{x}$.  $\UB$ and $\LB$
have already been computed, here we show the computation of $\Root$:
{\footnotesize 
\begin{align*}
\frac{\partial}{\partial y} f_i = - 2y - 2d = 0 \;\;\; \Longrightarrow \;\;\; \Root = y = -d
\end{align*}}
Finally, we have the potential maxima $y = \UB$, $y = \LB$, and $y = \Root$
of $f_i(\vec{b},\vec{x},y)$ w.r.t. constraints $\phi_i(\vec{b},\vec{x},y)$,
but since each is symbolically represented as a case statement, we cannot
numerically evaluate them to determine which is the true maxima. 
Instead, we need to symbolically evaluate this maximum $\Max$ as follows:
\footnotesize{
\begin{align*}
\Max \hspace{-1mm} =  \hspace{-1mm} \begin{cases}
\mbox{has $\Root$}  \hspace{-1mm}: \hspace{-1mm} & \hspace{-2mm} \casemax( f_i \{ y / \Root \}, f_i \{ y / \UB \}, f_i \{ y / \LB \})\\
\mbox{otherwise}  \hspace{-1mm}:  \hspace{-1mm} & \hspace{-2mm} \casemax( f_i \{ y / \UB \}, f_i \{ y / \LB \})
\end{cases}
\end{align*}}
Here a ternary $\casemax$ is just the successive application of
the previously defined binary $\casemax$.

\textbf{UP TO HERE, TODO: show result and add in constraints}

A lower bound on action $d$ occurs when the inequalities of $\geq ,>$ have action $d$ on their LHS; then any expression on the RHS is considered a lower bound on action $d$. Similar to this, an upper bound occurs for inequalities of $\leq , <$ and the RHS expression is considered an upper bound for action $d$. 
The roots of the function are also defined by taking the first derivative of $f$ with respect to $d$ and setting it to zero.

To enforce correctness of bounds symbolically we must have $\LB \leq \Root \leq \UB$, so we add the pair of constraints for the bounds to the root constraint:

{\footnotesize
\begin{center}
\begin{tabular}{r c c c l}
&
\hspace{-9mm} $
  \begin{cases}
-10 \leq -x \\ 
-2 -x \leq -x \\ 
-x \leq 10 \\ 
-x \leq 2 -x\\ 
  \end{cases}$
$=$
&
\hspace{-4mm}
$ \begin{cases}
x \leq 10 \\ 
x \geq -10\\ 
  \end{cases}$
\end{tabular}
\end{center}
}
The other two statements can be removed as a tautology, to leave the final root partition as: 

\begin{align*}\Root:= 
 \begin{cases}
x \leq 10 \wedge x \geq -10:              & -x\\ 
\neg (x \leq 10 \wedge x \geq -10):    & 0\\ 
  \end{cases}
\end{align*}

Having the $\UB$,$\LB$ and root of each case partition, we can maximize the value of each case function according to the bounds. We want to factor out the action variable $d$ (we show later that this is equal to maximizing the Q-function values in order to obtain the value function). This means replacing the continuous action with the possible maximum points; $\LB$, $\UB$ and roots. Replacing the action with a constant, variable or another function is equal to applying the \emph{substitute} operator. Each substitution forms a partition of the final maximization over the three posibble maximum points: 

\begin{align*}
\max_{d}
\begin{cases}
4 - (x+ \UB)^2 \\ 
4- (x+\LB)^2 \\ 
4 - (x+\Root)^2\\ 
  \end{cases}
\end{align*}

We take the maximum of the $\UB$ and $\LB$  substitutions and then maximum the result with the root substitution. Below shows the substituted maximum points:

{\footnotesize
\begin{center}
\begin{tabular} {r c c c l}
\hspace{-4mm}
$
\begin{cases}
x \leq -8 : & -96 + 20 \times x - x^2 \\ 
x \geq -8 : & 0 \\ 
\end{cases} $

\\
\hspace{-4mm}
$ \begin{cases}
x \leq 8 : & 0 \\ 
x \geq 8: & -96 -x^2 -20\times x\\
\end{cases}$
\hspace{-4mm} 
\\
\hspace{-4mm}
$\begin{cases}
x \leq 10 \wedge x \geq -10 : & 4 \\ 
x \leq 10 \wedge x \geq -10 : & 4 \\ 
\end{cases})$
\end{tabular}
\end{center}
}

The main partitions after the maximization over action $d$ is as below, the rest of the partitions hold the value of zero: 
\begin{align*}
\max_{d}
\begin{cases}
-10 \leq x \leq 10 : 4 \\ 
10 \leq x \leq 12  \wedge  -96-x^2-20\times x \geq 0: \\ -96-x^2-20\times x \geq 0 \\ 
-12 \leq x \leq -10  \wedge  -96-x^2+20\times x \geq 0:\\ -96-x^2+20\times x \geq 0 \\
  \end{cases}
\end{align*}

% make sure (iii) constraints get added back in

% mention how to derive policy (refer to Figure 2 and parens)

\subsection{Extended ADDs (XADDs)}

% mention feasibility checking, all ops apply directly

In practice, it can be prohibitively expensive to maintain a case
statement representation of a value function with explicit partitions.
Motivated by the SPUDD~\cite{spudd} algorithm which maintains compact
value function representations for finite discrete factored MDPs using
algebraic decision diagrams (ADDs)~\cite{bahar93add}, Sanner {\it et
al}~\cite{sanner_uai11} introduced \emph{extended ADDs} (XADDs) to handle
continuous variables in an ADD-like data structure.  An
example XADD for the optimal \MarsRover value function has already
been provided in Figure~\ref{fig:opt_val_pol} --- partition
constraints are simply represented as internal node decisions and
partition values are simply represented as leaf nodes.

It is fairly straightforward for XADDs to support all case operations
required for SDP.  Standard operations like unary multiplication,
negation, $\oplus$, and $\otimes$ are implemented exactly as they are
for ADDs.  The fact that the decision nodes have internal structure
means that certain paths in the XADD may be inconsistent or infeasible
(due to parent decisions).  To remedy this, when the XADD has only
linear decision nodes (as it will in this work), we can use a 
feasibility checker of a linear programming solver
to prune out unreachable paths~\cite{sanner_uai11}, which substantially
improves scalability as we show later.

Sanner {\it et al}~\cite{sanner_uai11} outlined a number of issues
that arise in XADDs along with efficient solutions; we refer the
reader to that work for more details.  In this work, we need only
extend the XADD to support the previously defined continuous variable
maximization operation --- indeed, considering any path from root to
leaf in an XADD as a case partition and value, the mapping of this
operation from cases to XADDs is trivial and directly exploits
the compact partition structure of the XADD.

