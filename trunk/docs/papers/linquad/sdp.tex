\label{sec:sdp}

In this section, we extend the symbolic dynamic programming (SDP) work
of~\cite{sanner_uai11} to the case of continuously parameterized
actions for CSA-MDPs.  We present the general SDP framework for value
iteration in Algorithm~\ref{alg:vi} (\texttt{VI})
and a Q-function regression subroutine~\ref{alg:regress} (\texttt{Regress}) 
where we have omitted parameters $\vec{b}$ and $\vec{x}$ from $V$ and $Q$
to avoid notational clutter.  We note the single difference between
this algorithm and that described in~\cite{sanner_uai11} comes in 
the continuous action parameter maximization in line 7
of \texttt{VI}.  Before we explain this contribution though, we
first recap the SDP solution, which relies on the \emph{case} representation
and symbolic case operators to represent all functions
and perform all operations in these algorithms.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\incmargin{.5em}
\linesnumbered
\begin{algorithm}[t!]
\dontprintsemicolon
\SetKwFunction{regress}{Regress}
\Begin{
   $V^0:=0, h:=0$\;
   \While{$h < H$}{
       $h:=h+1$\;
       \ForEach {$a(\vec{y}) \in A$}{
              $Q_a^{h}(\vec{y})\,:=\,$\regress{$V^{h-1},a,\vec{y}$}\;
              $Q_a^{h} := \max_{\vec{y}} \, Q_a^{h}(\vec{y})$ $\,$ \emph{// Continuous $\max$}\;
              $V^{h} := \casemax_a \, Q_a^{h}$ $\,$ \emph{// $\casemax$ all $Q_a$}\;
              $\pi^{*,h} := \argmax_{(a,\vec{y})} \, Q_a^{h}(\vec{y})$\;
       }
       \If{$V^h = V^{h-1}$}
           {break $\,$ \emph{// Terminate if early convergence}\;}
   }
     \Return{$(V^h,\pi^{*,h})$} \;
}
\caption{\footnotesize \texttt{VI}(CSA-MDP, $H$) $\longrightarrow$ $(V^h,\pi^{*,h})$ \label{alg:vi}}
\end{algorithm}
\decmargin{.5em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\incmargin{.5em}
\linesnumbered
\begin{algorithm}[t!]
\dontprintsemicolon
\SetKwFunction{remapWithPrimes}{Prime}
%\SetKwFunction{sumout}{sumout}


\Begin{
    $Q=$ \remapWithPrimes{$V$} $\,$ \emph{// All $b_i \to b_i'$ and all $ x_i \to x_i'$} \;
    \emph{// Continuous regression marginal integration}\\
    \For {all $x'_j$ in $Q$}{
         $Q := \int Q \otimes P(x_j'|\vec{b},\vec{b}',\vec{x},a,\vec{y}) \, d_{x'_j}$\;
    }
    \emph{// Discrete regression marginal summation}\\
    \For {all $b'_i$ in $Q$}{
         $Q := \left[ Q \otimes P(b_i'|\vec{b},\vec{x},a,\vec{y}) \right]|_{b_i' = 1}$\\
         \hspace{8mm} $\oplus \left[ Q \otimes P(b_i'|\vec{b},\vec{x},a,\vec{y}) \right]|_{b_i' = 0}$\;
    }
    \Return{$R(\vec{b},\vec{x},a,\vec{y}) \oplus (\gamma \otimes Q)$} \;
}
\caption{\footnotesize \texttt{Regress}($V,a,\vec{y}$) $\longrightarrow$ $Q$ \label{alg:regress}}
\end{algorithm}
\decmargin{.5em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Case Representation and Operators}

From here out, we assume that all symbolic functions
can be represented in \emph{case} form~\cite{fomdp}:
{%\footnotesize 
\begin{align}
f = 
\begin{cases}
  \phi_1: & f_1 \\ 
 \vdots&\vdots\\ 
  \phi_k: & f_k \\ 
\end{cases} \label{eq:case}
\end{align}
}
Here the $\phi_i$ are logical formulae defined over the state
$(\vec{b},\vec{x})$ that can include arbitrary logical ($\land,\lor,\neg$)
combinations of (i) boolean variables and (ii) 
\emph{linear} inequalities ($\geq,>,\leq,<$) 
over continuous variables.  
Each $\phi_i$ will be disjoint from the other $\phi_j$ ($j \neq i$); 
however the $\phi_i$ may not exhaustively cover the state space, hence
$f$ may only be a \emph{partial function} and may be undefined for some
variable assignments.
%\footnote{In the context of SDP, states whose value
%at horizon $h$ is undefined correspond to states that cannot reach any
%defined state of the reward in horizon $h$.}
The $f_i$ may be either linear or quadratic in the continuous parameters
according to the same restrictions as for 
$R(\vec{b},\vec{x},a,\vec{y})$.

\emph{Unary operations} such as scalar multiplication $c\cdot f$ (for
some constant $c \in \mathbb{R}$) or negation $-f$ on case statements
$f$ are straightforward; the unary operation is simply applied to each
$f_i$ ($1 \leq i \leq k$). Intuitively, to perform a \emph{binary
  operation} on two case statements, we simply take the cross-product
of the logical partitions of each case statement and perform the
corresponding operation on the resulting paired partitions.  Letting
each $\phi_i$ and $\psi_j$ denote generic first-order formulae, we can
perform the ``cross-sum'' $\oplus$ of two (unnamed) cases in the
following manner:

{\footnotesize 
\begin{center}
\begin{tabular}{r c c c l}
&
\hspace{-6mm} 
  $\begin{cases}
    \phi_1: & f_1 \\ 
    \phi_2: & f_2 \\ 
  \end{cases}$
$\oplus$
&
\hspace{-4mm}
  $\begin{cases}
    \psi_1: & g_1 \\ 
    \psi_2: & g_2 \\ 
  \end{cases}$
&
\hspace{-2mm} 
$ = $
&
\hspace{-2mm}
  $\begin{cases}
  \phi_1 \wedge \psi_1: & f_1 + g_1 \\ 
  \phi_1 \wedge \psi_2: & f_1 + g_2 \\ 
  \phi_2 \wedge \psi_1: & f_2 + g_1 \\ 
  \phi_2 \wedge \psi_2: & f_2 + g_2 \\ 
  \end{cases}$
\end{tabular}
\end{center}
}
\normalsize

Likewise, we can perform $\ominus$ and $\otimes$ by,
respectively, subtracting or multiplying partition values (as opposed
to adding them) to obtain the result.  Some partitions resulting from
the application of the $\oplus$, $\ominus$, and $\otimes$ operators
may be inconsistent (infeasible); we may simply discard such 
partitions as they are irrelevant to the function value.

For SDP, we'll also need to perform maximization, restriction,
and substitution on case statements.  
\emph{Symbolic case maximization} is fairly straightforward
to define:
\vspace{-4mm}

{\footnotesize
\begin{center}
\begin{tabular}{r c c c l}
&
\hspace{-7mm} $\casemax \Bigg(
  \begin{cases}
    \phi_1: \hspace{-2mm} & \hspace{-2mm} f_1 \\ 
    \phi_2: \hspace{-2mm} & \hspace{-2mm} f_2 \\ 
  \end{cases}$
$,$
&
\hspace{-4mm}
  $\begin{cases}
    \psi_1: \hspace{-2mm} & \hspace{-2mm} g_1 \\ 
    \psi_2: \hspace{-2mm} & \hspace{-2mm} g_2 \\ 
  \end{cases} \Bigg)$
&
\hspace{-4mm} 
$ = $
&
\hspace{-4mm}
  $\begin{cases}
  \phi_1 \wedge \psi_1 \wedge f_1 > g_1    : & \hspace{-2mm} f_1 \\ 
  \phi_1 \wedge \psi_1 \wedge f_1 \leq g_1 : & \hspace{-2mm} g_1 \\ 
  \phi_1 \wedge \psi_2 \wedge f_1 > g_2    : & \hspace{-2mm}f_1 \\ 
  \phi_1 \wedge \psi_2 \wedge f_1 \leq g_2 : & \hspace{-2mm} g_2 \\ 
  \phi_2 \wedge \psi_1 \wedge f_2 > g_1    : & \hspace{-2mm} f_2 \\ 
  \phi_2 \wedge \psi_1 \wedge f_2 \leq g_1 : & \hspace{-2mm} g_1 \\ 
  \phi_2 \wedge \psi_2 \wedge f_2 > g_2    : & \hspace{-2mm} f_2 \\ 
  \phi_2 \wedge \psi_2 \wedge f_2 \leq g_2 : & \hspace{-2mm} g_2 \\ 
  \end{cases}$
\end{tabular}
\end{center}
} A few remarks are required here.  If all $f_i$ and $g_i$ are linear,
the $\casemax$ result is clearly still linear.  If the $f_i$ or $g_i$
are quadratic according to the previous reward restriction, it will
shortly become obvious that the expressions $f_i > g_i$ or $f_i \leq
g_i$ will be at most univariate quadratic and any such quadratic
inequality can be \emph{linearized} into a logical combination of two
linear inequalities by completing the square (e.g., $x^2 + 8x > 0
\equiv [x + 4]^2 > 4 \equiv [x \leq -2] \lor [x > 6]$).  Hence
according to the earlier restrictions, the result of this $\casemax$
operator will be representable in the case format previously described
(i.e., linear inequalities in decisions).

% define boolean restriction

% define substitution then explain the integral is substitution,
% refer to UAI paper for how to substitute a case into a case

% now all operations defined except line 7!

The next operation of \emph{boolean restriction} is fairly simple: in this
operation, we want to restrict a function $f$ to apply only in cases
that satisfy some boolean variable $b$, which we write as $f|_{\phi}$.  
This can be done by simply appending $\phi$ to each case partition
as follows:
{\footnotesize
\begin{center}
\begin{tabular}{r c c l}
&
\hspace{-6mm} 
  $f = \begin{cases}
    \phi_1: & f_1 \\ 
   \vdots&\vdots\\ 
    \phi_k: & f_k \\ 
  \end{cases}$
&

&
\hspace{-2mm}
  $f|_{\phi} = \begin{cases}
    \phi_1 \land \phi : & f_1 \\ 
   \vdots&\vdots\\ 
    \phi_k \land \phi : & f_k \\ 
  \end{cases}$
\end{tabular}
\end{center}
}
Clearly $f|_{\phi}$ only applies when $\phi$ holds and is
undefined otherwise, hence $f|_{\phi}$ is a partial function
unless $\phi \equiv \top$.

The final operation that we need to define for case statements is
substitution.  \emph{Symbolic substitution} simply takes a set
$\sigma$ of variables and their substitutions, e.g., $\sigma = \{ x_1'
/ (x_1 \hspace{-.8mm} + \hspace{-.8mm} x_2), x_2' / x_1 \}$ where the
LHS of the $/$ represents the substitution variable and the RHS of the
$/$ represents the expression that should be substituted in its place.
No variable occurring in any RHS expression of $\sigma$ can also occur
in any LHS expression of $\sigma$.  We write the substitution of a
non-case function $f_i$ with $\sigma$ as $f_i\sigma$; as an example,
for the $\sigma$ defined previously and $f_i = x_1' + x_2'$ then
$f_i\sigma = x_1 + x_2 + x_1$ as would be expected.  We can also
substitute into case partitions $\phi_j$ by applying $\sigma$ to each
inequality operand; as an example, if $\phi_j \equiv x_1' \leq 2x_2'$ 
then $\phi_j \sigma \equiv x_1 + x_2 \leq 2x_1$.  
Having now defined substitution of $\sigma$ for non-case
functions $f_i$ and case partitions $\phi_j$ we can define it for case
statements in general:

{\footnotesize
\begin{center}
\begin{tabular}{r c c l}
&
\hspace{-6mm} 
  $f = \begin{cases}
    \phi_1: & f_1 \\ 
   \vdots&\vdots\\ 
    \phi_k: & f_k \\ 
  \end{cases}$
&

&
\hspace{-2mm}
  $f\sigma = \begin{cases}
    \phi_1\sigma: & f_1\sigma \\ 
   \vdots&\vdots\\ 
    \phi_k\sigma: & f_k\sigma \\ 
  \end{cases}$
\end{tabular}
\end{center}
}
\normalsize

One property of substitution is that
if $f$ has mutually exclusive partitions $\phi_i$ ($1 \leq i \leq k$)
then $f\sigma$ must also have mutually exclusive partitions ---
this follows from the logical consequence that 
if $\phi_1 \land \phi_2 \models \bot$
then $\phi_1\sigma \land \phi_2\sigma \models \bot$.
%We will exploit this property next in SDP for DC-MDPs.

%%zz: entire subsection
\subsection{Maximization of Continuous Parameters}

In the previous section, we covered the operations required for the
SDP algorithm defined in~\cite{sanner_uai11}.  As the primary
technical contribution of this paper, we now address the
issue of maximizing a function (e.g., Q-function) w.r.t. continuous
parameters (e.g., action parameters).

It is perhaps easiest to demonstrate how to compute $\max_d f(d)$ in exact,
closed-form for $f(d)$ in case format with a running example.
If $f(d)$ has mutually exclusive and exhaustive 
case partitions w.r.t.\ a fixed $d$ (a property that will be guaranteed 
whenever this operator is applied in SDP), the following sequence of
equalities hold:
\begin{align*}
\max_d f(d) & = \max_d \sum_i \phi_i(d) f_i(d) & = \max_d \max_i \phi_i(d) f_i(d)\\
            & = \max_i \max_d \phi_i(d) f_i(d).
\end{align*}
The critical step where $\sum_i$ is replaced with $\max_i$ is permitted
since the $\phi_i$ are mutually disjoint and exhaustive --- only one $\phi_i$
will ever be true for any given assignment to the case statement variables.
This step is crucial because the outer $\max_i$ is the ``case maximization''
operation already discussed while the inner $\max_d$ is a continuous variable
maximization that we can now compute for just one case partition, i.e.,
one $\phi_i(d)$ and $f_i(d)$, and the apply a case maximization to all
of the results for each partition.

To continue with maximization in one partition, we use our
\MarsRoverNL example in one of the case partitions of the first iteration for
continuous maximization.  This partition $i$ (resulting from applying SDP)
contains conditions $\phi_i(d) \equiv \neg b\wedge
(x\geq 2)\wedge (d\leq 10) \wedge (d\geq -10)\wedge (x+d\leq 2) \wedge
(x+d\geq -2) $ and value $f_i(d) = 4 - (x+d)^2 $.

To perform $\max_d \phi_i f_i(d)$, we consider some mathematical
definitions necessary for the correctness of this operation:
\begin{enumerate}
\item A real-valued function $f$ defined on the real domain is said to
have a global (or absolute) maximum point at the point x, if for all $x$:
$ f(x^{*}) \geq f(x) $. The value of the function at this point is
the maximum of the function.
\item By \emph{Fermat's theorem}, the local maxima (and minima) of a function
can occur only at its critical points: where either the function is not
differentiable (which only happens at the upper and lower boundaries) or its
first derivative is 0.
\item The \emph{Boundedness theorem} states that a continuous function
$f$ in the closed interval (a,b) is bounded on that interval. That is,
there exist real numbers m and M such that:
\begin{align*}
\forall x \in \{a,b\} : m \leq f(x) \leq M 
\end{align*}
\end{enumerate}

Based on these definitions, the maximum value of $\phi_i f_i$ must occur
at either the boundaries or the points where its first derivative is zero.
Because we only consider up to quadratic $f_i$ in this paper, we need only
consider the single root of $f_i$ if it is quadratic.
%If this function were linear and a polynomial of a certain degree k, then the maximum for that function would occur on any of the (k-1) roots of the function or at the lower and upper bounds defined for it.
We can write the explicit functions for these bounds in
\emph{piecewise linear case form} for the lower bounds $\mathit{LB}$ 
given by $\phi_i$ (i.e., $d \geq -10$, $d \geq x - 2$), upper bounds 
$\mathit{UB}$ given by $\phi_i$ (i.e., $d \leq 10$, $d \leq 2 - x$), 
and the roots of $\frac{\partial}{\partial d} f_i = - 2x - 2d = 0$
w.r.t.\ $d$ (i.e., $d = -x$).  Since we have multiple lower and upper bounds,
we must consider the maximum of the lower bounds and the minimum of
the upper bounds which we can represent in case form as follows:
{\footnotesize
\begin{center}
\begin{tabular}{r c c l}
$\mathit{LB}:= \max(-10,-2 -x) = \begin{cases}
x \leq 8: & -2 -x \\ 
x \geq 8: &-10\\ 
\end{cases}$
&
\hspace{-4mm}
$\mathit{UB}:= \min(10, 2-x) = \begin{cases}
x \geq -8: & 2 -x \\ 
x \leq -8: &10\\ 
\end{cases}$
&
\hspace{-4mm}
$\mathit{Root}: d = -x$.
\end{tabular}
\end{center}
}
We know that the maximum of the $f_i$ could occur at any of the above
points, but we do not know which.  Hence the true maximum is simply
the case maximization of the substitution of the $\mathit{UB}$, $\mathit{LB}$, and root
into $f(d)$.  


A lower bound on action $d$ occurs when the inequalities of $\geq ,>$ have action $d$ on their LHS; then any expression on the RHS is considered a lower bound on action $d$. Similar to this, an upper bound occurs for inequalities of $\leq , <$ and the RHS expression is considered an upper bound for action $d$. 
The roots of the function are also defined by taking the first derivative of $f$ with respect to $d$ and setting it to zero.

To enforce correctness of bounds symbolically we must have $\mathit{LB} \leq \mathit{Root}s \leq \mathit{UB}$, so we add the pair of constraints for the bounds to the root constraint:

{\footnotesize
\begin{center}
\begin{tabular}{r c c c l}
&
\hspace{-9mm} $
  \begin{cases}
-10 \leq -x \\ 
-2 -x \leq -x \\ 
-x \leq 10 \\ 
-x \leq 2 -x\\ 
  \end{cases}$
$=$
&
\hspace{-4mm}
$ \begin{cases}
x \leq 10 \\ 
x \geq -10\\ 
  \end{cases}$
\end{tabular}
\end{center}
}
The other two statements can be removed as a tautology, to leave the final root partition as: 

\begin{align*}\mathit{Root}:= 
 \begin{cases}
x \leq 10 \wedge x \geq -10:              & -x\\ 
\neg (x \leq 10 \wedge x \geq -10):    & 0\\ 
  \end{cases}
\end{align*}

Having the $\mathit{UB}$,$\mathit{LB}$ and root of each case partition, we can maximize the value of each case function according to the bounds. We want to factor out the action variable $d$ (we show later that this is equal to maximizing the Q-function values in order to obtain the value function). This means replacing the continuous action with the possible maximum points; $\mathit{LB}$, $\mathit{UB}$ and roots. Replacing the action with a constant, variable or another function is equal to applying the \emph{substitute} operator. Each substitution forms a partition of the final maximization over the three posibble maximum points: 

\begin{align*}
\max_{d}
\begin{cases}
4 - (x+ \mathit{UB})^2 \\ 
4- (x+\mathit{LB})^2 \\ 
4 - (x+\mathit{Root})^2\\ 
  \end{cases}
\end{align*}

We take the maximum of the $\mathit{UB}$ and $\mathit{LB}$  substitutions and then maximum the result with the root substitution. Below shows the substituted maximum points:

{\footnotesize
\begin{center}
\begin{tabular} {r c c c l}
\hspace{-4mm}
$
\begin{cases}
x \leq -8 : & -96 + 20 \times x - x^2 \\ 
x \geq -8 : & 0 \\ 
\end{cases} $

\\
\hspace{-4mm}
$ \begin{cases}
x \leq 8 : & 0 \\ 
x \geq 8: & -96 -x^2 -20\times x\\
\end{cases}$
\hspace{-4mm} 
\\
\hspace{-4mm}
$\begin{cases}
x \leq 10 \wedge x \geq -10 : & 4 \\ 
x \leq 10 \wedge x \geq -10 : & 4 \\ 
\end{cases})$
\end{tabular}
\end{center}
}

The main partitions after the maximization over action $d$ is as below, the rest of the partitions hold the value of zero: 
\begin{align*}
\max_{d}
\begin{cases}
-10 \leq x \leq 10 : 4 \\ 
10 \leq x \leq 12  \wedge  -96-x^2-20\times x \geq 0: \\ -96-x^2-20\times x \geq 0 \\ 
-12 \leq x \leq -10  \wedge  -96-x^2+20\times x \geq 0:\\ -96-x^2+20\times x \geq 0 \\
  \end{cases}
\end{align*}

After taking the symbolic maximum over the partitions above, we also add all the constraints that did not effect the $d$ boundaries (i.e, $x\leq 2:false $ and $b:false$) which means only considering the partition values for $x \geq 2$ which takes out the final branch of the case partition above. 

\subsection{Extended ADDs (XADDs)}

% mention feasibility checking, all ops apply directly

In practice, it can be prohibitively expensive to maintain a case
statement representation of a value function with explicit partitions.
Motivated by the SPUDD~\cite{spudd} algorithm which maintains compact
value function representations for finite discrete factored MDPs using
algebraic decision diagrams (ADDs)~\cite{bahar93add}, Sanner {\it et
al}~\cite{sanner_uai11} introduced \emph{extended ADDs} (XADDs) to handle
continuous variables in an ADD-like data structure.  An
example XADD for the optimal \MarsRover value function has already
been provided in Figure~\ref{fig:opt_val_pol} --- partition
constraints are simply represented as internal node decisions and
partition values are simply represented as leaf nodes.

It is fairly straightforward for XADDs to support all case operations
required for SDP.  Standard operations like unary multiplication,
negation, $\oplus$, and $\otimes$ are implemented exactly as they are
for ADDs.  The fact that the decision nodes have internal structure
means that certain paths in the XADD may be inconsistent or infeasible
(due to parent decisions).  To remedy this, when the XADD has only
linear decision nodes (as it will in this work), we can use a 
feasibility checker of a linear programming solver
to prune out unreachable paths~\cite{sanner_uai11}, which substantially
improves scalability as we show later.

Sanner {\it et al}~\cite{sanner_uai11} outlined a number of issues
that arise in XADDs along with efficient solutions; we refer the
reader to that work for more details.  In this work, we need only
extend the XADD to support the previously defined continuous variable
maximization operation --- indeed, considering any path from root to
leaf in an XADD as a case partition and value, the mapping of this
operation from cases to XADDs is trivial and directly exploits
the compact partition structure of the XADD.

\subsection{Symbolic Dynamic Programming (SDP)}

In the SDP solution for CSA-MDPs, our objective will be to take
a CSA-MDP as defined in Section~\ref{sec:dcmdps}, apply value
iteration as defined in Section~\ref{sec:soln}, and produce
the final value optimal function $V^h$ at horizon $h$ in the form
of a case statement.

For the base case of $h=0$, we note that setting $V^0(\vec{b},\vec{x}) = 0$
(or to the reward case statement, if not action dependent)
is trivially in the form of a case statement.  

Next, $h > 0$ requires the application of SDP.  
Fortunately, given our previously defined
operations, SDP is straightforward and can be divided into four 
steps: 
\begin{enumerate}
\item {\it Prime the Value Function}: Since $V^{h}$ will become
the ``next state'' in value iteration, we setup a substitution
$\sigma = \{ b_1 / b_1', \ldots, b_n / b_n', x_1 / x_1', \ldots, x_m / x_m' \}$
and obtain $V'^{h} = V^{h}\sigma$.
%where $V'^{h}$ is over $(\vec{b}',\vec{x}')$.

\item {\it Continuous Integration}: Now that we have our primed value
function $V'^{h}$ in case statement format defined over next state
variables $(\vec{b}',\vec{x}')$, we first evaluate the integral
marginalization $\int_{\vec{x}'}$ over the continuous variables
in~\eqref{eq:qfun}.  Because the lower and upper integration bounds
are respectively $-\infty$ and $\infty$ and we have disallowed
synchronic arcs between variables in $\vec{x}'$ in the transition DBN,
we can marginalize out each $x_j'$ independently, and in any order.
Using \emph{variable elimination}~\cite{varelim}, when marginalizing
over $x_j'$ we can factor out any functions independent of $x_j'$ ---
that is, for $\int_{x_j'}$ in~\eqref{eq:qfun}, one can see that
initially, the only functions that can include $x_j'$ are $V'^{h}$ and
$P(x_j'|\vec{b},\vec{b}',\vec{x},a,\vec{y}) = \delta[x_j' -
g(\vec{x})]$; hence, the first marginal over $x_j'$ need only be
computed over $\delta[x_j' - g(\vec{x})] V'^{h}$.

The key insight of the SDP algorithm introduced in~\cite{sanner_uai11}
is that the integration $\int_{x_j'} \delta[x_j' - g(\vec{x})] V'^{h}
dx_j'$ simply \emph{triggers the substitution} $\sigma = \{ x_j' /
g(\vec{x}) \}$ on $V'^{h}$, that is
\begin{align}
\int_{x_j'} \delta[x_j' - g(\vec{x})] V'^{h} dx_j' \; = \; V'^{h} \{x_j' / g(\vec{x}) \} . \label{eq:one_int}
\end{align}
Thus we can perform the
operation in~\eqref{eq:one_int} repeatedly in sequence \emph{for each}
$x_j'$ ($1 \leq j \leq m$) for every action $a$.

Hence to perform~\eqref{eq:one_int} on this more general
representation, we obtain that $\int_{x_j'} P(x_j'|\vec{b},\vec{x},a,\vec{y}) V'^{h} dx_j'$
\begin{align*}
    = \begin{cases}
    \phi_1: & V'^{h} \{ x_j' = f_1 \} \\ 
   \vdots&\vdots\\ 
    \phi_k: & V'^{h} \{ x_j' = f_k \}  \\ 
  \end{cases}.
\end{align*}
This operation may be complicated in some cases for
when the form of 
$P(x_j'|\vec{b},\vec{x},a,\vec{y})$ is a \emph{conditional} 
equation, but the result is still in a closed-form case
statement as outlined in~\cite{sanner_uai11}.

To perform the full continuous integration, 
if we initialize 
$\tilde{Q}_a^{h+1} := V'^{h}$ for each action $a \in A$, and repeat
the above integrals for all $x_j'$, updating $\tilde{Q}_a^{h+1}$ each time,
then after elimination of all $x_j'$ ($1 \leq j \leq m$), we will have 
the partial regression of $V'^{h}$ for the continuous variables for
each action $a$ denoted by $\tilde{Q}_a^{h+1}$.

\item {\it Discrete Marginalization}: Now that we have our partial
regression $\tilde{Q}_a^{h+1}$ for each action $a$, we proceed
to derive the full backup $Q_a^{h+1}$ from $\tilde{Q}_a^{h+1}$
by evaluating the discrete 
marginalization $\sum_{\vec{b}'}$ in~\eqref{eq:qfun}.
Because we previously disallowed synchronic arcs
between the variables in $\vec{b}'$ 
in the transition DBN, we can sum out each variable $b_i'$ ($1 \leq i \leq n$) 
independently.  Hence, initializing
$Q_a^{h+1} := \tilde{Q}_a^{h+1}$
we perform the discrete regression by applying the following iterative
process \emph{for each} $b_i'$ in any order
for each action $a$:
\begin{align}
Q_a^{h+1} := & \left[ Q_a^{h+1} \otimes P(b_i'|\vec{b},\vec{x},a,\vec{y}) \right]|_{b_i' = 1} \nonumber \\
 & \oplus \left[ Q_a^{h+1} \otimes P(b_i'|\vec{b},\vec{x},a,\vec{y}) \right]|_{b_i' = 0}.
\end{align}
This requires a variant of the earlier restriction operator $|_v$ that
actually \emph{sets} the variable $v$ to the given value if present.
Note that both $Q_a^{h+1}$ and $P(b_i'|\vec{b},\vec{x},a,\vec{y})$ can be represented
as case statements (discrete CPTs \emph{are} case statements), 
and each operation produces a case statement.
Thus, once this process is complete, we have marginalized over
all $\vec{b}'$ and $Q_a^{h+1}$ is the symbolic representation
of the intended Q-function.

\item {\it Continuous Parameter Maximization}: Now that we have
$Q_a^{h+1}(\vec{y})$ in the case format with free parameters $\vec{y}$
for the action variables, we simply need to compute $Q_a^{h+1} =
\max_{\vec{y}} Q_a^{h+1}(\vec{y})$.  This continuous variable
maximization can be done sequentially for each element of $\vec{y}$ as
outlined previously.  At the same time, if record the maximal
substitutions $\vec{y}$ made at each leaf, we can also recover
the parameterized policy that led to these optimal values, e.g.,
as shown in Figure~\ref{fig:opt_val_pol}.

\item {\it Maximization}: Now that we have $Q_a^{h+1}$ in
case format for each action $a \in \{a_1,\ldots,a_p\}$, obtaining
$V^{h+1}$ in case format as defined in~\eqref{eq:vfun} requires
sequentially applying
\emph{symbolic maximization} as defined previously:
\begin{align*}
V^{h+1} & = 
\max(Q_{a_1}^{h+1},\max(\ldots,\max(Q_{a_{p-1}}^{h+1},Q_{a_p}^{h+1})))
\end{align*}
\end{enumerate}
By induction, because $V^0$ is a case statement and applying
SDP to $V^h$ in case statement form produces $V^{h+1}$ in case
statement form, we have achieved our intended
objective with SDP.  On the issue of correctness,
we note that each operation above simply implements one of the
dynamic programming operations in \eqref{eq:qfun} or \eqref{eq:vfun}, 
so correctness simply follows from verifying (a) that each case
operation produces the correct result and that (b) each case operation
is applied in the correct sequence as defined in \eqref{eq:qfun} or 
\eqref{eq:vfun}.  
