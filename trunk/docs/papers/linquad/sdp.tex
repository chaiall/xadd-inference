\label{sec:sdp}

In this section, we extend the symbolic dynamic programming (SDP) work
of~\cite{sanner_uai11} to the case of continuously parameterized
actions for CSA-MDPs.  We present the general SDP framework for value
iteration in Algorithm~\ref{alg:vi} (\texttt{VI}) and a Q-function
regression subroutine~\ref{alg:regress} (\texttt{Regress}) where we
have omitted parameters $\vec{b}$ and $\vec{x}$ from $V$ and $Q$ to
avoid notational clutter.  We note the single difference between this
algorithm and that described in~\cite{sanner_uai11} comes in the
continuous action parameter maximization in line 7 of \texttt{VI}.
Before we explain this contribution though, we first recap the SDP
solution, which requires that the CSA-MDP and all other
functions such as $Q$ and $V$ are represented in \emph{case} form and
all operations are \emph{case operations}, defined next.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\incmargin{.5em}
\linesnumbered
\begin{algorithm}[t!]
\dontprintsemicolon
\SetKwFunction{regress}{Regress}
\Begin{
   $V^0:=0, h:=0$\;
   \While{$h < H$}{
       $h:=h+1$\;
       \ForEach {$a(\vec{y}) \in A$}{
              $Q_a^{h}(\vec{y})\,:=\,$\regress{$V^{h-1},a,\vec{y}$}\;
              $Q_a^{h} := \max_{\vec{y}} \, Q_a^{h}(\vec{y})$ $\,$ \emph{// Continuous $\max$}\;
              $V^{h} := \casemax_a \, Q_a^{h}$ $\,$ \emph{// $\casemax$ all $Q_a$}\;
              $\pi^{*,h} := \argmax_{(a,\vec{y})} \, Q_a^{h}(\vec{y})$\;
       }
       \If{$V^h = V^{h-1}$}
           {break $\,$ \emph{// Terminate if early convergence}\;}
   }
     \Return{$(V^h,\pi^{*,h})$} \;
}
\caption{\footnotesize \texttt{VI}(CSA-MDP, $H$) $\longrightarrow$ $(V^h,\pi^{*,h})$ \label{alg:vi}}
\end{algorithm}
\decmargin{.5em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\incmargin{.5em}
\linesnumbered
\begin{algorithm}[t!]
\dontprintsemicolon
\SetKwFunction{remapWithPrimes}{Prime}
%\SetKwFunction{sumout}{sumout}


\Begin{
    $Q=$ \remapWithPrimes{$V$} $\,$ \emph{// All $b_i \to b_i'$ and all $ x_i \to x_i'$} \;
    \emph{// Continuous regression marginal integration}\\
    \For {all $x'_j$ in $Q$}{
         $Q := \int Q \otimes P(x_j'|\vec{b},\vec{b}',\vec{x},a,\vec{y}) \, d_{x'_j}$\;
    }
    \emph{// Discrete regression marginal summation}\\
    \For {all $b'_i$ in $Q$}{
         $Q := \left[ Q \otimes P(b_i'|\vec{b},\vec{x},a,\vec{y}) \right]|_{b_i' = 1}$\\
         \hspace{8mm} $\oplus \left[ Q \otimes P(b_i'|\vec{b},\vec{x},a,\vec{y}) \right]|_{b_i' = 0}$\;
    }
    \Return{$R(\vec{b},\vec{x},a,\vec{y}) \oplus (\gamma \otimes Q)$} \;
}
\caption{\footnotesize \texttt{Regress}($V,a,\vec{y}$) $\longrightarrow$ $Q$ \label{alg:regress}}
\end{algorithm}
\decmargin{.5em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Case Representation and Operators}

From here out, we assume that all symbolic functions
can be represented in \emph{case} form~\cite{fomdp}:
{%\footnotesize 
\begin{align}
f = 
\begin{cases}
  \phi_1: & f_1 \\ 
 \vdots&\vdots\\ 
  \phi_k: & f_k \\ 
\end{cases} \label{eq:case}
\end{align}
}
Here the $\phi_i$ are logical formulae defined over the state
$(\vec{b},\vec{x})$ that can include arbitrary logical ($\land,\lor,\neg$)
combinations of (i) boolean variables and (ii) 
\emph{linear} inequalities ($\geq,>,\leq,<$) 
over continuous variables.  
Each $\phi_i$ will be disjoint from the other $\phi_j$ ($j \neq i$); 
however the $\phi_i$ may not exhaustively cover the state space, hence
$f$ may only be a \emph{partial function} and may be undefined for some
variable assignments.
%\footnote{In the context of SDP, states whose value
%at horizon $h$ is undefined correspond to states that cannot reach any
%defined state of the reward in horizon $h$.}
The $f_i$ may be either linear or quadratic in the continuous parameters
according to the same restrictions as for 
$R(\vec{b},\vec{x},a,\vec{y})$.

\emph{Unary operations} such as scalar multiplication $c\cdot f$ (for
some constant $c \in \mathbb{R}$) or negation $-f$ on case statements
$f$ are straightforward; the unary operation is simply applied to each
$f_i$ ($1 \leq i \leq k$). Intuitively, to perform a \emph{binary
  operation} on two case statements, we simply take the cross-product
of the logical partitions of each case statement and perform the
corresponding operation on the resulting paired partitions.  Letting
each $\phi_i$ and $\psi_j$ denote generic first-order formulae, we can
perform the ``cross-sum'' $\oplus$ of two (unnamed) cases in the
following manner:

{\footnotesize 
\begin{center}
\begin{tabular}{r c c c l}
&
\hspace{-6mm} 
  $\begin{cases}
    \phi_1: & f_1 \\ 
    \phi_2: & f_2 \\ 
  \end{cases}$
$\oplus$
&
\hspace{-4mm}
  $\begin{cases}
    \psi_1: & g_1 \\ 
    \psi_2: & g_2 \\ 
  \end{cases}$
&
\hspace{-2mm} 
$ = $
&
\hspace{-2mm}
  $\begin{cases}
  \phi_1 \wedge \psi_1: & f_1 + g_1 \\ 
  \phi_1 \wedge \psi_2: & f_1 + g_2 \\ 
  \phi_2 \wedge \psi_1: & f_2 + g_1 \\ 
  \phi_2 \wedge \psi_2: & f_2 + g_2 \\ 
  \end{cases}$
\end{tabular}
\end{center}
}
\normalsize

Likewise, we can perform $\ominus$ and $\otimes$ by,
respectively, subtracting or multiplying partition values (as opposed
to adding them) to obtain the result.  Some partitions resulting from
the application of the $\oplus$, $\ominus$, and $\otimes$ operators
may be inconsistent (infeasible); we may simply discard such 
partitions as they are irrelevant to the function value.

For SDP, we'll also need to perform maximization, restriction,
and substitution on case statements.  
\emph{Symbolic case maximization} is fairly straightforward
to define:
\vspace{-4mm}

{\footnotesize
\begin{center}
\begin{tabular}{r c c c l}
&
\hspace{-7mm} $\casemax \Bigg(
  \begin{cases}
    \phi_1: \hspace{-2mm} & \hspace{-2mm} f_1 \\ 
    \phi_2: \hspace{-2mm} & \hspace{-2mm} f_2 \\ 
  \end{cases}$
$,$
&
\hspace{-4mm}
  $\begin{cases}
    \psi_1: \hspace{-2mm} & \hspace{-2mm} g_1 \\ 
    \psi_2: \hspace{-2mm} & \hspace{-2mm} g_2 \\ 
  \end{cases} \Bigg)$
&
\hspace{-4mm} 
$ = $
&
\hspace{-4mm}
  $\begin{cases}
  \phi_1 \wedge \psi_1 \wedge f_1 > g_1    : & \hspace{-2mm} f_1 \\ 
  \phi_1 \wedge \psi_1 \wedge f_1 \leq g_1 : & \hspace{-2mm} g_1 \\ 
  \phi_1 \wedge \psi_2 \wedge f_1 > g_2    : & \hspace{-2mm}f_1 \\ 
  \phi_1 \wedge \psi_2 \wedge f_1 \leq g_2 : & \hspace{-2mm} g_2 \\ 
  \phi_2 \wedge \psi_1 \wedge f_2 > g_1    : & \hspace{-2mm} f_2 \\ 
  \phi_2 \wedge \psi_1 \wedge f_2 \leq g_1 : & \hspace{-2mm} g_1 \\ 
  \phi_2 \wedge \psi_2 \wedge f_2 > g_2    : & \hspace{-2mm} f_2 \\ 
  \phi_2 \wedge \psi_2 \wedge f_2 \leq g_2 : & \hspace{-2mm} g_2 \\ 
  \end{cases}$
\end{tabular}
\end{center}
} We remark that if all $f_i$ and $g_i$ are linear,
the $\casemax$ result is clearly still linear.  If the $f_i$ or $g_i$
are quadratic according to the previous reward restriction, it will
shortly become obvious that the expressions $f_i > g_i$ or $f_i \leq
g_i$ will be at most univariate quadratic and any such quadratic
inequality can be \emph{linearized} into a logical combination of at most 
two linear inequalities by completing the square (e.g., $x^2 + 8x > 0
\equiv [x + 4]^2 > 4 \equiv [x \leq -2] \lor [x > 6]$).  Hence
according to the earlier restrictions, the result of this $\casemax$
operator will be representable in the case format previously described
(i.e., linear inequalities in decisions).

There are two operations in \texttt{Regress} that we have not defined
yet.  The first operation of \emph{boolean restriction} required in
lines 8--9 is obvious and an example is omitted: in this operation
$f|_{b=v}$, anywhere a boolean variable $b$ occurs in $f$, we assign
it the value $v \in \{ 0,1 \}$.  The second operation of
\emph{continuous regression} $\int Q(x'_j) \otimes P(x'_j|\cdots)
dx'_j$ is required in line 5; as previously defined, $P(x_j'|\cdots)$
will always be of the form $\delta[x_j' - h(\vec{z})]$ where
$h(\vec{z})$ is a case statement and $\vec{z}$ does not contain
$x'_j$.  Rules of integration then tell us that $\int f(x'_j) \otimes
\delta[x_j' - h(\vec{z})] dx'_j = f(x'_j) \{ x'_j / h(\vec{z}) \}$
where the latter operation indicates that any occurrence of $x'_j$ in
$f(x'_j)$ is \emph{symbolically substituted} with the case statement
$h(\vec{z})$.  The full specification of this operation was a key
contribution of~\cite{sanner_uai11} so we refer the reader to that
paper for further technical details.  The
important insight is that this $\int$ operation yields a result that is a
case statement in the form previously outlined.

\subsection{Maximization of Continuous Parameters}

The only operation in \texttt{VI} and \texttt{Regress} that has not
yet been defined is the continuous action maximization in line 7 of
\texttt{VI} that forms the key novel contribution of this paper.
Reintroducing suppressed state variables and renaming $Q_a^{h}$ to
$f$, we write this operation as $g(\vec{b},\vec{x}) := \max_{\vec{y}}
\, f(\vec{b},\vec{x},\vec{y})$ --- the key point to realize here is
that \emph{the} maximizing $\vec{y}$ is a function
$g(\vec{b},\vec{x})$.  We work through the derivation of $g$ in this
section, which amounts to \emph{symbolically} solving a linear program
subject to unknown state parameters $\vec{b}$ and $\vec{x}$.

Exploiting the commutativity of $\max$, we can first rewrite any
multivariate $\max_{\vec{y}}$ as a sequence of univariate $\max$
operations $\max_{y_1} \cdots \max_{y_{|\vec{y}|}}$; hence it suffices
to provide the univariate $\max_y$ solution: 
$g(\vec{b},\vec{x}) := \max_{y} \, f(\vec{b},\vec{x},y)$.

If $f(\vec{b},\vec{x},y)$ has mutually disjoint and exhaustive
case partitions $\phi_i$ (a property that is 
guaranteed whenever this operator is applied in SDP), the following
equalities hold:
{\footnotesize
\begin{align*}
\max_y f(\vec{b},\vec{x},y) & = \max_y \sum_i \phi_i(\vec{b},\vec{x},y) f_i(\vec{b},\vec{x},y)\\
& = \max_y \casemax_i \, \phi_i(\vec{b},\vec{x},y) f_i(\vec{b},\vec{x},y)\\
& = \casemax_i \max_y \phi_i(\vec{b},\vec{x},y) f_i(\vec{b},\vec{x},y).
\end{align*}}
The critical step where $\sum_i$ is replaced with $\max_i$ is
permitted since the $\phi_i$ are mutually disjoint and exhaustive ---
only one $\phi_i$ will ever be true for any given assignment to the
case statement variables.  This step is crucial because the outer
$\max_i$ is the $\casemax$ operation already discussed
while the inner $\max_y$ is a continuous variable maximization that we
can now compute for \emph{each case partition individually}.  Hence we
need only define $\max_y \phi_i(\vec{b},\vec{x},y) f_i(\vec{b},\vec{x},y)$.

{\bf STOPPED HERE}

From here out we assume that all partitions $\phi_i$ consist of
conjunctions of linear inequalities and (possibly negated) 
boolean variables --- a condition easy to enforce since disjunctions
$\lor$ can be broken up across multiple case partitions and negated
inequalities are still inequalities ($\neg [x < 2] \equiv [x \geq 2]$).
To make the partition maximization procedure concrete, 
we use an example that arises in the \MarsRover problem.  
This partition $i$ (resulting
from applying SDP) has conditions $\phi_i(x,b,y) \equiv \neg b\wedge
(x\geq 2)\wedge (y\leq 10) \wedge (y\geq -10)\wedge (x+y\leq 2) \wedge
(x+y\geq -2)$ and value $f_i(x,y) = 4 - (x+d)^2 $.

To perform $\max_y \phi_i f_i(d)$, we make a few remarks on the
structure of this operation.


we consider some mathematical
definitions necessary for the correctness of this operation:
\begin{enumerate}
\item A real-valued function $f$ defined on the real domain is said to
have a global (or absolute) maximum point at the point x, if for all $x$:
$ f(x^{*}) \geq f(x) $. The value of the function at this point is
the maximum of the function.
\item By \emph{Fermat's theorem}, the local maxima (and minima) of a function
can occur only at its critical points: where either the function is not
differentiable (which only happens at the upper and lower boundaries) or its
first derivative is 0.
\item The \emph{Boundedness theorem} states that a continuous function
$f$ in the closed interval (a,b) is bounded on that interval. That is,
there exist real numbers m and M such that:
\begin{align*}
\forall x \in \{a,b\} : m \leq f(x) \leq M 
\end{align*}
\end{enumerate}

Based on these definitions, the maximum value of $\phi_i f_i$ must occur
at either the boundaries or the points where its first derivative is zero.
Because we only consider up to quadratic $f_i$ in this paper, we need only
consider the single root of $f_i$ if it is quadratic.
%If this function were linear and a polynomial of a certain degree k, then the maximum for that function would occur on any of the (k-1) roots of the function or at the lower and upper bounds defined for it.
We can write the explicit functions for these bounds in
\emph{piecewise linear case form} for the lower bounds $\mathit{LB}$ 
given by $\phi_i$ (i.e., $d \geq -10$, $d \geq x - 2$), upper bounds 
$\mathit{UB}$ given by $\phi_i$ (i.e., $d \leq 10$, $d \leq 2 - x$), 
and the roots of $\frac{\partial}{\partial d} f_i = - 2x - 2d = 0$
w.r.t.\ $d$ (i.e., $d = -x$).  Since we have multiple lower and upper bounds,
we must consider the maximum of the lower bounds and the minimum of
the upper bounds which we can represent in case form as follows:
{\footnotesize
\begin{center}
\begin{tabular}{r c c l}
$\mathit{LB}:= \max(-10,-2 -x) = \begin{cases}
x \leq 8: & -2 -x \\ 
x \geq 8: &-10\\ 
\end{cases}$
&
\hspace{-4mm}
$\mathit{UB}:= \min(10, 2-x) = \begin{cases}
x \geq -8: & 2 -x \\ 
x \leq -8: &10\\ 
\end{cases}$
&
\hspace{-4mm}
$\mathit{Root}: d = -x$.
\end{tabular}
\end{center}
}
We know that the maximum of the $f_i$ could occur at any of the above
points, but we do not know which.  Hence the true maximum is simply
the case maximization of the substitution of the $\mathit{UB}$, $\mathit{LB}$, and root
into $f(d)$.  


A lower bound on action $d$ occurs when the inequalities of $\geq ,>$ have action $d$ on their LHS; then any expression on the RHS is considered a lower bound on action $d$. Similar to this, an upper bound occurs for inequalities of $\leq , <$ and the RHS expression is considered an upper bound for action $d$. 
The roots of the function are also defined by taking the first derivative of $f$ with respect to $d$ and setting it to zero.

To enforce correctness of bounds symbolically we must have $\mathit{LB} \leq \mathit{Root}s \leq \mathit{UB}$, so we add the pair of constraints for the bounds to the root constraint:

{\footnotesize
\begin{center}
\begin{tabular}{r c c c l}
&
\hspace{-9mm} $
  \begin{cases}
-10 \leq -x \\ 
-2 -x \leq -x \\ 
-x \leq 10 \\ 
-x \leq 2 -x\\ 
  \end{cases}$
$=$
&
\hspace{-4mm}
$ \begin{cases}
x \leq 10 \\ 
x \geq -10\\ 
  \end{cases}$
\end{tabular}
\end{center}
}
The other two statements can be removed as a tautology, to leave the final root partition as: 

\begin{align*}\mathit{Root}:= 
 \begin{cases}
x \leq 10 \wedge x \geq -10:              & -x\\ 
\neg (x \leq 10 \wedge x \geq -10):    & 0\\ 
  \end{cases}
\end{align*}

Having the $\mathit{UB}$,$\mathit{LB}$ and root of each case partition, we can maximize the value of each case function according to the bounds. We want to factor out the action variable $d$ (we show later that this is equal to maximizing the Q-function values in order to obtain the value function). This means replacing the continuous action with the possible maximum points; $\mathit{LB}$, $\mathit{UB}$ and roots. Replacing the action with a constant, variable or another function is equal to applying the \emph{substitute} operator. Each substitution forms a partition of the final maximization over the three posibble maximum points: 

\begin{align*}
\max_{d}
\begin{cases}
4 - (x+ \mathit{UB})^2 \\ 
4- (x+\mathit{LB})^2 \\ 
4 - (x+\mathit{Root})^2\\ 
  \end{cases}
\end{align*}

We take the maximum of the $\mathit{UB}$ and $\mathit{LB}$  substitutions and then maximum the result with the root substitution. Below shows the substituted maximum points:

{\footnotesize
\begin{center}
\begin{tabular} {r c c c l}
\hspace{-4mm}
$
\begin{cases}
x \leq -8 : & -96 + 20 \times x - x^2 \\ 
x \geq -8 : & 0 \\ 
\end{cases} $

\\
\hspace{-4mm}
$ \begin{cases}
x \leq 8 : & 0 \\ 
x \geq 8: & -96 -x^2 -20\times x\\
\end{cases}$
\hspace{-4mm} 
\\
\hspace{-4mm}
$\begin{cases}
x \leq 10 \wedge x \geq -10 : & 4 \\ 
x \leq 10 \wedge x \geq -10 : & 4 \\ 
\end{cases})$
\end{tabular}
\end{center}
}

The main partitions after the maximization over action $d$ is as below, the rest of the partitions hold the value of zero: 
\begin{align*}
\max_{d}
\begin{cases}
-10 \leq x \leq 10 : 4 \\ 
10 \leq x \leq 12  \wedge  -96-x^2-20\times x \geq 0: \\ -96-x^2-20\times x \geq 0 \\ 
-12 \leq x \leq -10  \wedge  -96-x^2+20\times x \geq 0:\\ -96-x^2+20\times x \geq 0 \\
  \end{cases}
\end{align*}

After taking the symbolic maximum over the partitions above, we also add all the constraints that did not effect the $d$ boundaries (i.e, $x\leq 2:false $ and $b:false$) which means only considering the partition values for $x \geq 2$ which takes out the final branch of the case partition above. 

% mention how to derive policy (refer to Figure 2 and parens)

\subsection{Extended ADDs (XADDs)}

% mention feasibility checking, all ops apply directly

In practice, it can be prohibitively expensive to maintain a case
statement representation of a value function with explicit partitions.
Motivated by the SPUDD~\cite{spudd} algorithm which maintains compact
value function representations for finite discrete factored MDPs using
algebraic decision diagrams (ADDs)~\cite{bahar93add}, Sanner {\it et
al}~\cite{sanner_uai11} introduced \emph{extended ADDs} (XADDs) to handle
continuous variables in an ADD-like data structure.  An
example XADD for the optimal \MarsRover value function has already
been provided in Figure~\ref{fig:opt_val_pol} --- partition
constraints are simply represented as internal node decisions and
partition values are simply represented as leaf nodes.

It is fairly straightforward for XADDs to support all case operations
required for SDP.  Standard operations like unary multiplication,
negation, $\oplus$, and $\otimes$ are implemented exactly as they are
for ADDs.  The fact that the decision nodes have internal structure
means that certain paths in the XADD may be inconsistent or infeasible
(due to parent decisions).  To remedy this, when the XADD has only
linear decision nodes (as it will in this work), we can use a 
feasibility checker of a linear programming solver
to prune out unreachable paths~\cite{sanner_uai11}, which substantially
improves scalability as we show later.

Sanner {\it et al}~\cite{sanner_uai11} outlined a number of issues
that arise in XADDs along with efficient solutions; we refer the
reader to that work for more details.  In this work, we need only
extend the XADD to support the previously defined continuous variable
maximization operation --- indeed, considering any path from root to
leaf in an XADD as a case partition and value, the mapping of this
operation from cases to XADDs is trivial and directly exploits
the compact partition structure of the XADD.

