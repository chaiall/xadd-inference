\label{sec:intro}

Many real-world stochastic planning problems involving resources,
time, or spatial configurations naturally use continuous variables in
both their state and action representation.  For example, in a
\MarsRover\ problem~\cite{bresina02}, a rover must navigate within a
continuous spatial environment and carry out assigned scientific
discovery tasks; in \InventoryControl\ problems~\cite{Mahootchi2009}
for continuous resources such as petroleum products, a business must
must decide what quantity of each item to order subject to uncertain
demand, (joint) capacity constraints, and reordering costs; and in
\WaterReservoir\ problems~\cite{reservoir}, a utility must manage continuous
reservoir water levels in continuous time to avoid underflow 
while maximizing electricity generation revenue.

Previous work on \emph{exact} solutions to multivariate continuous
state \emph{and} action settings has been quite limited.  There are
well-known exact solutions in the control theory literature for the
case of linear-quadratic Gaussian (LQG) control~\cite{lqgc}, i.e.,
minimizing a quadratic cost function subject to linear dynamics with
Gaussian noise in a partially observed setting.  However, the
transition dynamics and reward (or cost) for such problems 
cannot be piecewise --- a crucial limitation preventing the application
of such solutions to planning and operations research problems. 

In this paper, we provide an exact symbolic dyanmic programming (SDP)
solution to a subset of continuous state and action Markov decision
processes (CSA-MDPs), namely CSA-MDPs with \emph{multivariate}
continuous state and actions, discrete noise, \emph{piecewise} linear
dynamics, and \emph{piecewise} linear (or restricted \emph{piecewise}
quadratic) reward.  To be concrete about the form of CSA-MDPs we can
solve with our SDP approach, let us formalize a simple \MarsRover\
problem:\footnote{One should note that for purposes of concise
exposition and explanation of the optimal value function and policy,
this CSA-MDP example contains only univariate state and action and
deterministic transitions; the empirical results will later define a
range of CSA-MDPs with multivariate state and action and discrete
noise.}

\begin{example*}[\MarsRover]
\label{ex:knapsack}
A Mars Rover state consists of its continuous position $x$ along a
given route.  In a given time step, the rover may move a 
continuous distance $d \in [-10,10]$.  The rover receives its greatest
reward for taking a picture at $x=0$, which quadratically decreases
to zero at the boundaries of the range $x \in [-2,2]$.  The rover will
automatically take a picture when it starts a time step within the
range $x \in [-2,2]$ and it only receives this reward once.
\end{example*}
Using boolean variable $b$ to indicate whether the picture has
already been taken and $x'$ and $b'$ to denote the
post-action state and $R$ to denote the reward function, we obtain a
simple instance of a \MarsRover\ CSA-MDP:
\begin{align} 
P(b'|x) & = 
\begin{cases}
b \lor (x \geq -2 \land x \leq 2): & 1.0\\
\neg b \land (x < -2 \lor x > 2):  & 0.0
\end{cases} \label{eq:mr_discrete_trans} \\
P(x'|x,d) & = \delta \left( x' - \begin{cases}
d \geq -10 \land d \leq 10 : & \hspace{-2mm} x + d \\
d < -10 \lor d > 10 : & \hspace{-2mm} x
\end{cases}
\right) \label{eq:mr_cont_trans} \\
R(x,b) & = \begin{cases}
\neg b \land x \geq -2 \land x \leq 2 : & 4 - x^2 \\
b \lor x < -2 \lor x > 2 : & 0
\end{cases} \label{eq:mr_reward}
\end{align}
Then there are two natural questions that we want to ask:
\begin{enumerate}
\item[(a)] What is the optimal form of value that can be 
obtained from any state over a fixed time horizon?
\item[(b)] What is the corresponding closed-form optimal policy?
\end{enumerate}
% Be nice to include that this is a breakthrough for deterministic
% systems as well

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t!]
\centering
%\subfigure{
\includegraphics[width=0.4\textwidth]{Figures1/v1_mr.pdf}\\
\includegraphics[width=0.4\textwidth]{Figures1/v2_mr.pdf}\\
\includegraphics[width=0.4\textwidth]{Figures1/v3_mr.pdf}
\vspace{-2mm}
\caption{\footnotesize Optimal value functions $V^t$ (for $b =
\false$) for time horizons (i.e., decision stages remaining) $t=0$,
$t=1$, and $t=2$ on the \MarsRover\ problem.  For $x \in [-2,2]$, the
rover automatically takes a picture and receives a reward quadratic in
$x$.  For $V^1$, the rover may move up to 10 units in
either direction, reaching the full reward of $4$ up
to $x = \pm 10$ and non-zero reward up to $x = \pm 12$. 
For $V^2$, the rover can move up to 20 units in two time steps,
allowing it to achieve non-zero reward up to $x = \pm 22$.}
\label{fig:opt_graph}
%\vspace{-2mm}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t!]
\centering
%\subfigure{
\includegraphics[width=0.5\textwidth]{Figures1/v2_mr_dd.pdf}
%\vspace{-3mm}
\caption{\footnotesize Optimal value function $V^1$ for the
\MarsRover\ problem represented as an extended algebraic decision
diagram (XADD).  Here the solid lines represent the $\true$ branch for
the decision and the dashed lines the $\false$ branch.  To evaluate
$V^1(x)$ for any state $x$, one simply traverses the diagram in a
decision-tree like fashion until a leaf is reached where the
expression provides the value.  The second expression in parentheses
in each leaf provides the optimal action \emph{policy} for $d$ as a
function of the state that allows one to obtain $V^1(x)$.  This
closed-form policy can be derived as a byproduct of symbolic dynamic
programming as we discuss later.}
\label{fig:opt_val_pol}
%\vspace{-3mm}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To get a sense of the form of the optimal solution to problems such as
\MarsRover, we present the 0-, 1-, and 2-step time horizon solutions
for this problem in Figure~\ref{fig:opt_graph}; further, in symbolic
form, we display both the 1-step time horizon value function (the
2-step is too large to display) \emph{and} corresponding optimal
policy in Figure~\ref{fig:opt_val_pol}.  Here, the piecewise nature of
the transition and reward function lead to piecewise
structure in the value function and policy.  Despite the intuitive and
simple nature of this result, we are unaware of prior methods that can
produce such exact solutions.

To this end, we extend the previous SDP framework
of~\cite{sanner_uai11} to the case of continuous actions to obtain the
\emph{optimal closed-form} value function and policy for the
class of CSA-MDPs previously described.

As the fundamental technical contribution of the paper, we
show how the \emph{continuous action maximization} step in the dynamic
programming backup can be evaluated optimally and symbolically and how
it can be efficiently realized in the extended algebraic decision diagram (XADD)
we use to perform all SDP operations.  This
allows us to obtain the \emph{first} algorithm to derive exact closed-form
solutions to this class of CSA-MDPs along with a closed-form
representation of the optimal policy
(cf. Figure~\ref{fig:opt_val_pol}).  
%- extension to XADDs
%- solutions for multivariate actions and quadratic reward
%- automatic derivation of the policy
%- first exact solution to variants of multi-inventory control
%  problem, water reservoir, and quadratic reward problems with
%  piecewise dynamics
We empirically evaluate the time and space required to compute exact
solutions to the \MarsRover\ problem for different time horizons, as
well as \WaterReservoir\ and multivariate action \InventoryControl\
studied in operations research showing the first exact solutions for
these problems can be practically computed for reasonable horizons.
