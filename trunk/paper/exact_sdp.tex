\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{array}
\usepackage{amsmath,amssymb}
\usepackage{epsfig,subfigure}
\usepackage[vlined,algoruled,titlenumbered,noend]{algorithm2e}
\frenchspacing

\newcommand{\MarsRover}{\textsc{Mars Rover}}
\newcommand{\MarsRoverL}{\textsc{Mars Rover Linear}}
\newcommand{\MarsRoverNL}{\textsc{Mars Rover Nonlinear}}
\newcommand{\InventoryControl}{\textsc{Inventory Control}}
\newcommand{\WaterReservoir}{\textsc{Water Reservoir}}
\newcommand{\MultiWaterReservoir}{\textsc{Multi-Reservoir}}
\newtheorem*{example*}{Example}

\newcommand{\true}{\mathit{true}}
\newcommand{\false}{\mathit{false}}
\def\argmax{\operatornamewithlimits{arg\,max}}
\def\argmin{\operatornamewithlimits{arg\,min}}

\setcounter{secnumdepth}{0}  

 \begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Exact Solutions to Continuous State and Action MDPs}
\author{Anonymous}
%\author{Zahra Zamani\\
%NICTA \& the ANU\\
%Canberra, Australia\\
%{\tt zahra.zamani@anu.edu.au}
%\And
%Scott Sanner\\
%NICTA \& the ANU\\
%Canberra, Australia\\
%{\tt ssanner@nicta.com.au}
%\And
%Cheng Feng\\
%Department of Aeronautics and\\
%Astronautics, MIT, USA\\
%{\tt cfang@mit.edu}
%}
\maketitle
\begin{abstract}
\begin{quote}
Many real-world decision-theoretic planning problems are naturally
modeled using both continuous state and action (CSA) spaces.  Previous
work on such problems has only provided exact solutions in limited
settings, leaving one to resort to discretization or sampling as
approximate solution approaches.  In this work, we propose a symbolic
dynamic programming solution to obtain the \emph{optimal closed-form}
value function and policy for CSA-MDPs with discrete noise, piecewise
linear dynamics, and piecewise quadratic rewards.  Crucially we show
how the continuous action maximization step in the dynamic programming
backup can be evaluated optimally and symbolically for this case;
further we extend this maximization operation to work with an
efficient and compact data structure, the extended algebraic decision
diagram (XADD).  We demonstrate empirical results for CSA-MDPs on a
range of domains from planning and operations research to demonstrate
the first \emph{exact solution} to these problems --- even in the case
of multivariate actions and nonlinear reward.
\end{quote}
\end{abstract}

\section{Introduction}

\label{sec:intro}

Many real-world stochastic planning problems involving resources,
time, or spatial configurations naturally use continuous variables in
both their state and action representation.  For example, in a variant
of the
%\InventoryControl problem, a business must must decide how many
%of each item to reorder subject to uncertain demand, inventory holding
%constraints, and quantity-dependent reordering costs.
\MarsRover\ problem~\cite{bresina02}, a rover must navigate within a
continuous environment and carry out assigned scientific discovery
tasks.  In oversubscribed cases where the \MarsRover\ may not be able
to visit all of its assigned landmarks (e.g., to take pictures) within a
given time horizon, it may still be rewarded to some degree for
partial objective fulfillment (e.g, taking pictures within the
vicinity of the assigned landmarks).  

Previous work on \emph{exact} solutions to continuous state \emph{and}
action settings has been quite limited.  There are well-known exact
solutions in the control theory literature for the case of linear
quadratic Gaussian (LQG) control~\cite{lqgc}, i.e., minimizing a quadratic cost
function subject to linear dynamics with Gaussian additive noise in a
partially observable setting.  However, the transition dynamics and
reward (alternately cost) of such problems are not allowed to be
piecewise --- a restriction that prevents us from solving
problems like the aforementioned \MarsRover\ problem.

To be concrete about the modeling power and contributions of this 
paper, let us formalize a \MarsRover\ problem to serve as a
running example:

\begin{example*}[\MarsRover]
\label{ex:knapsack}
A Mars Rover state consists of its continuous position $x$ along a
given route.  In a given time step, the rover may move an arbitrary
continuous distance $d \in [-10,10]$.  The rover receives its greatest
reward for snapping a picture at $x=0$, which quadratically decreases
to zero at the boundaries of the range $x \in [-2,2]$.  The rover will
automatically take a picture when it starts a time step within the
range $x \in [-2,2]$ and it only receives this reward once; we use
boolean variable $b$ to indicate whether the picture has already been
taken.  Formally, using $x'$ and $b'$ to denote the post-action state
and $R$ to denote the reward function, we obtain a simple instance of
a CSA-MDP:\footnote{One will note that this CSA-MDP example is
deterministic for purposes of minimal exposition; the solution in the
paper will generally allow for CSA-MDPs with discrete noise.}
\begin{align*}
P(x'|x,d) & = \delta \left( x' - \begin{cases}
d \geq -10 \land d \leq 10 : & x + d \\
d < -10 \lor d > 10 : & x
\end{cases}
\right)\\
P(b'|x) & = 
\begin{cases}
b \lor (x \geq -2 \land x \leq 2) & 1.0\\
\neg b \land (x < -2 \lor x > 2)  & 0.0
\end{cases}\\
R(x,b) & = \begin{cases}
\neg b \land x \geq -2 \land x \leq 2 : & 4 - x^2 \\
b \lor x < -2 \lor x > 2 : & 0
\end{cases}
\end{align*}
\end{example*}
There are two natural questions that we want to ask in CSA-MDP settings
such as this one:
\begin{enumerate}
\item[(a)] What is the optimal value that can be obtained from any state 
over a fixed time horizon?
\item[(b)] What is the corresponding policy one should execute to
achieve this optimal value?
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t!]
\centering
%\subfigure{
\includegraphics[width=0.4\textwidth]{Figures1/v1_mr.pdf}\\
\includegraphics[width=0.4\textwidth]{Figures1/v2_mr.pdf}\\
\includegraphics[width=0.4\textwidth]{Figures1/v3_mr.pdf}
\vspace{-2mm}
\caption{\footnotesize Optimal value functions $V^t$ (for $b =
\false$) for time horizons (i.e., decision stages remaining) $t=0$,
$t=1$, and $t=2$ on the \MarsRover\ problem.  For $x \in [-2,2]$, the
rover automatically takes a picture and receives a reward quadratic in
$x$.  For $V^1$, the rover may move up to 10 units in
either direction, reaching the full reward of $4$ up
to $x = \pm 10$ and non-zero reward up to $x = \pm 12$. 
For $V^2$, the rover can move up to 20 units in two time steps,
allowing it to achieve non-zero reward up to $x = \pm 12$.}
\label{fig:opt_graph}
%\vspace{-2mm}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t!]
\centering
%\subfigure{
\includegraphics[width=0.5\textwidth]{Figures1/v2_mr_dd.pdf}
%\vspace{-3mm}
\caption{\footnotesize Optimal value function $V^1$ for the
\MarsRover\ problem represented as an extended algebraic decision
diagram (XADD).  Here the solid lines represent the $\true$ branch for
the decision and the dashed lines the $\false$ branch.  To evaluate
$V^1(x)$ for any state $x$, one simply traverses the diagram in a
decision-tree like fashion until a leaf is reached where the
expression provides the value.  The second expression in parentheses
in each leaf provides the optimal action \emph{policy} for $d$ as a
function of the state that allows one to obtain $V^1(x)$.  This
closed-form policy can be derived as a byproduct of symbolic dynamic
programming as we discuss later.}
\label{fig:opt_val_pol}
%\vspace{-3mm}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To get a sense of the form of the optimal solution to problems such as
the \MarsRover, we present the 0-, 1-, and 2-step time horizon solutions
for this problem in Figure~\ref{fig:opt_graph}; further, in symbolic form,
we display both the 1-step time horizon value function (the 2-step is
too large to display) and corresponding optimal policy in
Figure~\ref{fig:opt_val_pol}.  Here we see that the piecewise 
nature of the transition and reward function lead to
piecewise structure in the value function and policy.  And 
despite their striking simplicity, we are not aware of any exact
solution method to the above \MarsRover\ variant that can produce
such an optimal closed-form result.

To this end, we extend the symbolic dynamic programming (SDP) framework
of~\cite{sanner_uai11} to the case of continuous actions to obtain the
\emph{optimal closed-form} value function and policy for CSA-MDPs with
discrete noise, piecewise linear dynamics, and piecewise quadratic
rewards.  As the fundamental technical contribution of the paper, we
show how the \emph{continuous action maximization} step in the dynamic
programming backup can be evaluated optimally and symbolically and how
it can be efficiently realized in the extended algebraic decision diagram (XADD)
we use to perform all SDP operations.  This
allows us to obtain the \emph{first} algorithm to derive exact closed-form
solutions to this class of CSA-MDPs along with a closed-form
representation of the optimal policy
(cf. Figure~\ref{fig:opt_val_pol}).  
%- extension to XADDs
%- solutions for multivariate actions and quadratic reward
%- automatic derivation of the policy
%- first exact solution to variants of multi-inventory control
%  problem, water reservoir, and quadratic reward problems with
%  piecewise dynamics
We empirically evaluate the time and space required to compute exact
solutions to the \MarsRover\ problem for different time horizons, as
well as \WaterReservoir\ and multivariate action \InventoryControl\
studied in operations research showing the first exact solutions for
these problems can be practically computed for reasonable horizons.

\section{Continuous State and Action MDPs}

\label{sec:dcmdps}

We first introduce continuous state and action Markov decision
processes (CSA-MDPs) and then review their finite-horizon solution via
dynamic programming building on~\cite{sanner_uai11}.  

\subsection{Factored Representation}

In a CSA-MDP, states will be represented by vectors of variables
$(\vec{b},\vec{x}) = ( b_1,\ldots,b_n,x_{1},\ldots,x_m )$.  We assume
that each state variable $b_i$ ($1 \leq i \leq n$) is
boolean$\,$ s.t. $b_i \in \{ 0,1 \}$ and each $x_j$ ($1 \leq j \leq m$) is 
continuous s.t. $x_j \in [L_j,U_j]$ for $L_j,U_j \in
\mathbb{R}; L_j \leq U_j$.  We also assume a finite set of actions $A
= \{ a_1, \ldots, a_p \}$.

A DC-MDP is defined by the following: (1) a state transition model
$P(\vec{b}',\vec{x}'|\cdots,a)$, which specifies the probability of
the next state $(\vec{b}',\vec{x}')$ conditioned on a subset of the
previous and next state (defined below) and action $a$; (2) a reward
function $R(\vec{b},\vec{x},a)$, which specifies the immediate reward
obtained by taking action $a$ in state $(\vec{b},\vec{x})$; and (3) a
discount factor $\gamma, \; 0 \leq \gamma \leq 1$.\footnote{If time is
explicitly included as one of the continuous state variables, $\gamma
= 1$ is typically used, unless discounting by horizon (different from
the state variable time) is still intended.}  
A policy $\pi$
specifies the action $\pi(\vec{b},\vec{x})$ to take in each state
$(\vec{b},\vec{x})$.  Our goal is to find an optimal sequence of
horizon-dependent policies $\Pi^* = (\pi^{*,1},\ldots,\pi^{*,H})$
that maximizes the expected sum of discounted rewards over a horizon
$h \in H; H \geq 0$:\footnote{$H=\infty$ is allowed if an optimal policy has a
finitely bounded value (guaranteed if $\gamma < 1$); for $H=\infty$, 
the optimal policy is independent of horizon, 
i.e., $\forall h \geq 0, \pi^{*,h} = \pi^{*,h+1}$.}
\begin{align}
V^{\Pi^*}(\vec{x}) & = E_{\Pi^*} \left[ \sum_{h=0}^{H} \gamma^h \cdot r^h \Big| \vec{b}_0,\vec{x}_0 \right], \label{eq:vfun_def}
\end{align}
Here $r^h$ is the reward obtained at horizon $h$ following $\Pi^*$ where 
we assume starting state $(\vec{b}_0,\vec{x}_0)$ at $h=0$.
 
DC-MDPs as defined above are naturally factored~\cite{boutilier99dt}
in terms of state variables $(\vec{b},\vec{x})$; as such transition
structure can be exploited in the form of a dynamic Bayes net
(DBN)~\cite{dbn} where the individual conditional probabilities
$P(b_i'|\cdots,a)$ and $P(x_j'|\cdots,a)$ condition on a subset of the
variables in the current and next state.  We disallow \emph{synchronic
arcs} (variables that condition on each other in the same time slice) 
within the binary $\vec{b}$ and continuous variables $\vec{x}$, 
but we allow synchronic arcs from $\vec{b}$ to $\vec{x}$ (note that
these conditions enforce the directed acyclic graph requirements of
DBNs).
%from variables in $\vec{b}$ to each other and to $\vec{x}$.
%variables to condition on binary
%them between  to enforce directed graph
%properties for \emph{synchronic arcs} among binary variables 
%, we assume a total ordering over
%binary and continuous variables and let $\vec{b}_{<i}$
%($\vec{x}_{<j}$) represent all variables lower than $b_i$ ($x_j$) in
%the ordering; furthermore we asssume and that all 
%$\vec{b}$ come before $\vec{x}$.  
We write the joint transition model as
\begin{align}
P(\vec{b}',&\vec{x}'|\vec{b},\vec{x},a) = \label{eq:dbn} \\
& \prod_{i=1}^n P(b_i'|\vec{b},\vec{x},a) \prod_{j=1}^m P(x_j'|\vec{b},\vec{b}',\vec{x},a) \nonumber 
\end{align}
where $P(b_i'|\vec{b},\vec{x},a)$ may condition on a subset of
$\vec{b}$ and $\vec{x}$ and likewise 
$P(x_j'|\vec{b},\vec{b}',\vec{x},a)$ may condition on a subset of
$\vec{b}$, $\vec{b}'$, and $\vec{x}$.

We call the conditional
probabilities $P(b_i'|\vec{b},\vec{x},a)$ for \emph{binary} variables
$b_i$ ($1 \leq i \leq n$) conditional
probability functions (CPFs) --- not tabular enumerations,
because in general these functions
can condition on both discrete and continuous state.
For the \emph{continuous} variables $x_j$
($1 \leq j \leq m$), we represent the CPFs 
$P(x_j'|\vec{b},\vec{b'},\vec{x},a)$ with \emph{conditional stochastic
 equations} (CSEs).  For the solution provided here, we
only require two properties of these CSEs: (1) they are
\emph{Markov}, meaning that they can only condition on the previous
state, and (2) they are \emph{deterministic} meaning that the next state must
be uniquely determined from the previous state (i.e., $x_1' = x_1 +
x_2^2$ is deterministic whereas $x_1'^2 = x_1^2$ is not because $x_1'
= \pm x_1$).\footnote{While the \emph{deterministic} 
requirement may seem to conflict 
with the label of \emph{stochastic}, we note that stochasticity enters through
the conditional component, to be discussed in a moment.}
Otherwise, we allow for arbitrary functions in these
Markovian, conditional deterministic equations as in the following
example:
\vspace{-3mm}

{\footnotesize
\begin{align}
P(x_1' | \vec{b},\vec{b}',\vec{x},a) = \delta\left[ x_1' - 
\begin{cases}
b_1' \land x_2^2 \leq 1 : & \exp(x_1^2 - x_2^2) \\
\neg b_1' \lor  x_2^2 > 1 : & x_1 + x_2 \\
\end{cases}
\right] \label{eq:ex_csde}
\end{align}}
Here %the next-state of $x_1'$ is independent of the action $a$;
the use of the Dirac $\delta[\cdot]$ function ensures that this is a
conditional probability function that integrates to 1 over $x_1'$
in this case.  But in more intuitive terms, one can see that this
$\delta[\cdot]$ encodes the deterministic transition 
equation $x_1' = \ldots$ where $\ldots$ is the conditional portion 
of~\eqref{eq:ex_csde}.
In this work, we require all CSEs in the transition
function for variable $x_i'$ to use the $\delta[\cdot]$ as shown in this
example.

It will be obvious that CSEs in the form of \eqref{eq:ex_csde} are
\emph{conditional  equations}; they are furthermore \emph{stochastic}
because they can condition on boolean random variables in the same time slice
that are stochastically sampled, e.g., $b_1'$ in
\eqref{eq:ex_csde}.  Of course, these CSEs are restricted in that
they cannot represent general stochastic noise (e.g., Gaussian noise),
but we note that this representation effectively allows modeling of
continuous variable transitions as a mixture of $\delta$ functions,
which has been used heavily in previous exact DC-MDP
solutions~\cite{feng04,li05,hao09}.  Furthermore, we note that our
representation is more general than~\cite{feng04,li05,hao09} in that
we do not restrict the  equation to be linear, but rather
allow it to specify \emph{arbitrary} functions (e.g., nonlinear) as
demonstrated in~\eqref{eq:ex_csde}.

We allow 
the reward function $R_a(\vec{b},\vec{x})$
to be \emph{any} arbitrary function of the current state
for each action $a \in A$, for example:
\begin{align}
R_a(\vec{b},\vec{x}) = \begin{cases}
x_1^2 + x_2^2 \leq 1 : & 1 - x_1^2 - x_2^2  \\
x_1^2 + x_2^2 > 1 : & 0 \\
\end{cases} \label{eq:simple_reward}
\end{align}
or even 
\begin{align}
R_a(\vec{b},\vec{x}) & = 10 x_3 x_4 \exp(x_1^2 + \sqrt{x_2}) \label{eq:expr_reward}
\end{align}
While our DC-MDP examples throughout the paper will demonstrate the
full expressiveness of our symbolic dynamic programming approach,
we note that there are computational advantages to be had when
the reward and transition case conditions and 
functions can be restricted, e.g., to
polynomials.  We will return to this issue later.

\subsection{Solution Methods}

\label{sec:soln}

Now we provide a continuous state generalization of {\it
value iteration}~\cite{bellman}, which is a dynamic programming
algorithm for constructing optimal policies.  It proceeds by
constructing a series of $h$-stage-to-go value functions
$V^h(\vec{b},\vec{x})$.  Initializing $V^0(\vec{b},\vec{x})$ 
(e.g., to $V^0(\vec{b},\vec{x}) = 0$) 
we define the quality of taking action $a$ in state
$(\vec{b},\vec{x})$ and acting so as to obtain $V^{h}(\vec{b},\vec{x})$ 
thereafter as the following:
\vspace{-3mm}

{\footnotesize
\begin{align}
& Q_a^{h+1}(\vec{b},\vec{x}) = R_a(\vec{b},\vec{x}) + \gamma \cdot \label{eq:qfun} \\ 
& \sum_{\vec{b}'} \int_{\vec{x}'} \left( \prod_{i=1}^n P(b_i'|\vec{b},\vec{x},a) \prod_{j=1}^m P(x_j'|\vec{b},\vec{b}',\vec{x},a) \right) V^h(\vec{b}',\vec{x}') d\vec{x}' \nonumber
\end{align}}

Given $Q_a^h(\vec{b},\vec{x})$ for each $a \in A$, we can proceed
to define the $h+1$-stage-to-go value function as follows:
\begin{align}
V^{h+1}(\vec{b},\vec{x}) & = \max_{a \in A} \left\{ Q^{h+1}_a(\vec{b},\vec{x}) \right\} \label{eq:vfun}
\end{align}

If the horizon $H$ is finite, then the optimal value function is
obtained by computing $V^H(\vec{b},\vec{x})$ and the optimal
horizon-dependent policy $\pi^{*,h}$ at each stage $h$ can be easily
determined via 
$\pi^{*,h}(\vec{b},\vec{x}) = \argmax_a Q^h_a(\vec{b},\vec{x})$.  
If the horizon 
$H = \infty$ and the optimal policy has finitely bounded value, 
then value iteration can terminate at horizon $h+1$ if 
$V^{h+1} = V^{h}$; then 
$\pi^*(\vec{b},\vec{x}) = \argmax_a Q^{h+1}_a(\vec{b},\vec{x})$.

Of course this is simply the \emph{mathematical} definition.  In the
discrete-only case, we can always compute this in tabular form;
however, how to compute this for DC-MDPs with reward and transition
function as previously defined is the objective of the symbolic
dynamic programming algorithm that we define next.

\section{Symbolic Dynamic Programming}

As it's name suggests, symbolic dynamic programming (SDP)~\cite{fomdp}
is simply the process of performing dynamic programming (in this case
value iteration) via symbolic manipulation.  While SDP as defined
in~\cite{fomdp} was previously only used with piecewise
constant functions, we now generalize the representation to work with
general piecewise functions needed for DC-MDPs in this paper.  

Before we define our solution, however, we must formally define our
case representation and symbolic case operators.

\subsection{Case Representation and Operators}

Throughout this paper, we will assume that all symbolic functions
can be represented in \emph{case} form as follows:
{%\footnotesize 
\begin{align*}
f = 
\begin{cases}
  \phi_1: & f_1 \\ 
 \vdots&\vdots\\ 
  \phi_k: & f_k \\ 
\end{cases}
\end{align*}
}
Here the $\phi_i$ are logical formulae defined over the state
$(\vec{b},\vec{x})$ that can include arbitrary logical ($\land,\lor,\neg$)
combinations of (a) boolean variables in $\vec{b}$ and (b) 
inequalities ($\geq,>,\leq,<$), equalities ($=$), or disequalities ($\neq$)
where the left and right operands can be \emph{any} function of one or more 
variables in $\vec{x}$.  
Each $\phi_i$ will be disjoint from the other $\phi_j$ ($j \neq i$); 
however the $\phi_i$ may not exhaustively cover the state space, hence
$f$ may only be a \emph{partial function} and may be undefined for some
state assignments.
%\footnote{In the context of SDP, states whose value
%at horizon $h$ is undefined correspond to states that cannot reach any
%defined state of the reward in horizon $h$.}
The $f_i$ can be \emph{any} functions of the state
variables in $\vec{x}$.  

As concrete examples, consider the transition representation for
KNAPSACKin Ex.~\ref{ex:knapsack}, the optimal value function for
KNAPSACKfrom~\eqref{eq:vfun_knapsack}, or any of
\eqref{eq:ex_csde}, \eqref{eq:simple_reward}, or \eqref{eq:expr_reward}.

\emph{Unary operations} such as scalar multiplication $c\cdot f$ (for
some constant $c \in \mathbb{R}$) or negation $-f$ on case statements
$f$ are straightforward; the unary operation is simply applied to each
$f_i$ ($1 \leq i \leq k$). Intuitively, to perform a \emph{binary
  operation} on two case statements, we simply take the cross-product
of the logical partitions of each case statement and perform the
corresponding operation on the resulting paired partitions.  Letting
each $\phi_i$ and $\psi_j$ denote generic first-order formulae, we can
perform the ``cross-sum'' $\oplus$ of two (unnamed) cases in the
following manner:

{\footnotesize 
\begin{center}
\begin{tabular}{r c c c l}
&
\hspace{-6mm} 
  $\begin{cases}
    \phi_1: & f_1 \\ 
    \phi_2: & f_2 \\ 
  \end{cases}$
$\oplus$
&
\hspace{-4mm}
  $\begin{cases}
    \psi_1: & g_1 \\ 
    \psi_2: & g_2 \\ 
  \end{cases}$
&
\hspace{-2mm} 
$ = $
&
\hspace{-2mm}
  $\begin{cases}
  \phi_1 \wedge \psi_1: & f_1 + g_1 \\ 
  \phi_1 \wedge \psi_2: & f_1 + g_2 \\ 
  \phi_2 \wedge \psi_1: & f_2 + g_1 \\ 
  \phi_2 \wedge \psi_2: & f_2 + g_2 \\ 
  \end{cases}$
\end{tabular}
\end{center}
}
\normalsize

Likewise, we can perform $\ominus$ and $\otimes$ by,
respectively, subtracting or multiplying partition values (as opposed
to adding them) to obtain the result.  Some partitions resulting from
the application of the $\oplus$, $\ominus$, and $\otimes$ operators
may be inconsistent (infeasible); we may simply discard such 
partitions as they are irrelevant to the function value.

For SDP, we'll also need to perform maximization, restriction,
and substitution on case statements.  
\emph{Symbolic maximization} is fairly straightforward
to define:
\vspace{-5mm}

{\footnotesize
\begin{center}
\begin{tabular}{r c c c l}
&
\hspace{-9mm} $\max \Bigg(
  \begin{cases}
    \phi_1: & f_1 \\ 
    \phi_2: & f_2 \\ 
  \end{cases}$
$,$
&
\hspace{-4mm}
  $\begin{cases}
    \psi_1: & g_1 \\ 
    \psi_2: & g_2 \\ 
  \end{cases} \Bigg)$
&
\hspace{-4mm} 
$ = $
&
\hspace{-4mm}
  $\begin{cases}
  \phi_1 \wedge \psi_1 \wedge f_1 > g_1    : & f_1 \\ 
  \phi_1 \wedge \psi_1 \wedge f_1 \leq g_1 : & g_1 \\ 
  \phi_1 \wedge \psi_2 \wedge f_1 > g_2    : & f_1 \\ 
  \phi_1 \wedge \psi_2 \wedge f_1 \leq g_2 : & g_2 \\ 
  \phi_2 \wedge \psi_1 \wedge f_2 > g_1    : & f_2 \\ 
  \phi_2 \wedge \psi_1 \wedge f_2 \leq g_1 : & g_1 \\ 
  \phi_2 \wedge \psi_2 \wedge f_2 > g_2    : & f_2 \\ 
  \phi_2 \wedge \psi_2 \wedge f_2 \leq g_2 : & g_2 \\ 
  \end{cases}$
\end{tabular}
\end{center}
}
One can verify that the resulting case statement is still
within the case language defined previously.  At first
glance this may seem like a cheat and little is gained
by this symbolic sleight of hand.  However, simply
having a case partition representation that is closed under 
maximization will facilitate the closed-form regression 
step that we need for SDP.\ \  Furthermore, the 
XADD that we introduce later will be able to exploit the 
internal decision structure of this
maximization to represent it much more compactly.

The next operation of \emph{restriction} is fairly simple: in this
operation, we want to restrict a function $f$ to apply only in cases
that satisfy some formula $\phi$, which we write as $f|_{\phi}$.  
This can be done by simply appending $\phi$ to each case partition
as follows:
{\footnotesize
\begin{center}
\begin{tabular}{r c c l}
&
\hspace{-6mm} 
  $f = \begin{cases}
    \phi_1: & f_1 \\ 
   \vdots&\vdots\\ 
    \phi_k: & f_k \\ 
  \end{cases}$
&

&
\hspace{-2mm}
  $f|_{\phi} = \begin{cases}
    \phi_1 \land \phi : & f_1 \\ 
   \vdots&\vdots\\ 
    \phi_k \land \phi : & f_k \\ 
  \end{cases}$
\end{tabular}
\end{center}
}
Clearly $f|_{\phi}$ only applies when $\phi$ holds and is
undefined otherwise, hence $f|_{\phi}$ is a partial function
unless $\phi \equiv \top$.

The final operation that we need to define for case
statements is substitution.  \emph{Symbolic substitution} simply takes
a set $\sigma$ of variables and their substitutions, e.g., 
$\sigma = \{ x_1' / (x_1 \hspace{-.8mm} + \hspace{-.8mm} x_2), x_2' / x_1^2 \hspace{-.3mm} \exp(x_2) \}$ where
the LHS of the $/$ represents the substitution variable and the
RHS of the $/$ represents 
the expression that should be substituted in its place.  No variable
occurring in any RHS expression of $\sigma$ can also occur in any 
LHS expression of $\sigma$.
We write the substitution of a non-case function $f_i$ with $\sigma$ 
as $f_i\sigma$; as an example, for the $\sigma$ defined previously and 
$f_i = x_1' + x_2'$ then $f_i\sigma = x_1 + x_2 + x_1^2 \exp(x_2)$ as
would be expected.  We can also substitute into case partitions $\phi_j$
by applying $\sigma$ to each inequality operand; as an example, if
$\phi_j \equiv x_1' \leq \exp(x_2')$ then 
$\phi_j \sigma \equiv x_1 + x_2 \leq \exp(x_1^2 \exp(x_2))$.
Having now defined substitution of $\sigma$ for non-case functions $f_i$ and case
partitions $\phi_j$ we can define it for case statements in general:

{\footnotesize
\begin{center}
\begin{tabular}{r c c l}
&
\hspace{-6mm} 
  $f = \begin{cases}
    \phi_1: & f_1 \\ 
   \vdots&\vdots\\ 
    \phi_k: & f_k \\ 
  \end{cases}$
&

&
\hspace{-2mm}
  $f\sigma = \begin{cases}
    \phi_1\sigma: & f_1\sigma \\ 
   \vdots&\vdots\\ 
    \phi_k\sigma: & f_k\sigma \\ 
  \end{cases}$
\end{tabular}
\end{center}
}
\normalsize

One property of substitution is that
if $f$ has mutually exclusive partitions $\phi_i$ ($1 \leq i \leq k$)
then $f\sigma$ must also have mutually exclusive partitions ---
this follows from the logical consequence that 
if $\phi_1 \land \phi_2 \models \bot$
then $\phi_1\sigma \land \phi_2\sigma \models \bot$.
%We will exploit this property next in SDP for DC-MDPs.



\subsection{Symbolic Dynamic Programming (SDP)}

% \ref{sec:dcmdps} \ref{sec:soln} \ref{eq:qfun} \ref{eq:vfun} 

In the SDP solution for DC-MDPs, our objective will be to take
a DC-MDP as defined in Section~\ref{sec:dcmdps}, apply value
iteration as defined in Section~\ref{sec:soln}, and produce
the final value optimal function $V^h$ at horizon $h$ in the form
of a case statement.

For the base case of $h=0$, we note that setting $V^0(\vec{b},\vec{x}) = 0$
(or to the reward case statement, if not action dependent)
%and $R(\vec{b},\vec{x})$ as described in Section~\ref{sec:dcmdps}
is trivially in the form of a case statement.  

Next, $h > 0$ requires the application of SDP.  
Fortunately, given our previously defined
operations, SDP is straightforward and can be divided into four 
steps: 
\begin{enumerate}
\item {\it Prime the Value Function}: Since $V^{h}$ will become
the ``next state'' in value iteration, we setup a substitution
$\sigma = \{ b_1 / b_1', \ldots, b_n / b_n', x_1 / x_1', \ldots, x_m / x_m' \}$
and obtain $V'^{h} = V^{h}\sigma$.
%where $V'^{h}$ is over $(\vec{b}',\vec{x}')$.

\item {\it Continuous Integration}: Now that we have our primed value
function $V'^{h}$ in case statement format defined over next state
variables $(\vec{b}',\vec{x}')$, we first evaluate the integral
marginalization $\int_{\vec{x}'}$ over the continuous variables
in~\eqref{eq:qfun}.  Because the lower and upper integration bounds
are respectively $-\infty$ and $\infty$
and we have
disallowed synchronic arcs between variables in $\vec{x}'$ 
in the transition DBN, we can marginalize out each
$x_j'$ independently, and in any order.  Using 
\emph{variable elimination}~\cite{varelim}, when marginalizing
over $x_j'$ we can factor out any functions independent of $x_j'$ --- 
that is, for $\int_{x_j'}$ in~\eqref{eq:qfun}, 
one can see that initially, 
the only functions that can include $x_j'$ are $V'^{h}$
and $P(x_j'|\vec{b},\vec{b}',\vec{x},a) = \delta[x_j' - g(\vec{x})]$; 
hence, the first marginal over $x_j'$ need only be computed over
$\delta[x_j' - g(\vec{x})] V'^{h}$.

What follows is one of the \emph{key novel insights of SDP} in the context of
DC-MDPs --- the integration 
$\int_{x_j'} \delta[x_j' - g(\vec{x})] V'^{h} dx_j'$ 
simply \emph{triggers the substitution} $\sigma = \{ x_j' / g(\vec{x}) \}$
on $V'^{h}$, that is
\begin{align}
\int_{x_j'} \delta[x_j' - g(\vec{x})] V'^{h} dx_j' \; = \; V'^{h} \{x_j' / g(\vec{x}) \} . \label{eq:one_int}
\end{align}
Thus we can perform the
operation in~\eqref{eq:one_int} repeatedly in sequence \emph{for each}
$x_j'$ ($1 \leq j \leq m$) for every action $a$.  The only
additional complication is that the form of 
$P(x_j'|\vec{b},\vec{x},a)$ is a \emph{conditional} 
equation, c.f.~\eqref{eq:ex_csde}, and represented generically
as follows:
\begin{align}
   P(x_j'|\vec{b},\vec{x},a) = \delta\left[ x_j' = \begin{cases}
    \phi_1: & f_1 \\ 
   \vdots&\vdots\\ 
    \phi_k: & f_k \\ 
  \end{cases} \right] \label{eq:cond_sub}
\end{align}
Hence to perform~\eqref{eq:one_int} on this more general
representation, we obtain that $\int_{x_j'} P(x_j'|\vec{b},\vec{x},a) V'^{h} dx_j'$
\begin{align*}
    = \begin{cases}
    \phi_1: & V'^{h} \{ x_j' = f_1 \} \\ 
   \vdots&\vdots\\ 
    \phi_k: & V'^{h} \{ x_j' = f_k \}  \\ 
  \end{cases}
\end{align*}
In effect, we can read~\eqref{eq:cond_sub} as a \emph{conditional
substitution}, i.e., in each of the different \emph{previous state}
conditions $\phi_i$ ($1 \leq i \leq k$), we obtain a different
substitution for $x_j'$ appearing in $V'^{h}$ (i.e., $\sigma = \{ x_j' / f_i
\}$).  Here we note that because $V'^{h}$ is \emph{already} a case
statement, we can simply replace the single partition $\phi_i$ with the
multiple partitions of $V'^h \{ x_j' / f_i \}|_{\phi_i}$.\footnote{If $V'^h$
had mutually disjoint partitions then we note the restriction and
substitution operations preserve this disjointness.}  This reduces
the \emph{nested} case statement back down to a non-nested case
statement as in the following example:
\begin{align*}
    \begin{cases}
      \phi_1: & 
        \begin{cases}
          \psi_1: & f_{11} \\ 
          \psi_2: & f_{12}  \\ 
        \end{cases} \\
      \phi_2: & 
        \begin{cases}
          \psi_1: & f_{21} \\ 
          \psi_2: & f_{22}  \\ 
        \end{cases} \\
    \end{cases} & \; = \;
        \begin{cases}
          \phi_1 \land \psi_1: & f_{11} \\ 
          \phi_1 \land \psi_2: & f_{12}  \\ 
          \phi_2 \land \psi_1: & f_{21} \\ 
          \phi_2 \land \psi_2: & f_{22}  \\ 
        \end{cases} 
\end{align*}

To perform the full continuous integration, 
if we initialize 
$\tilde{Q}_a^{h+1} := V'^{h}$ for each action $a \in A$, and repeat
the above integrals for all $x_j'$, updating $\tilde{Q}_a^{h+1}$ each time,
then after elimination of all $x_j'$ ($1 \leq j \leq m$), we will have 
the partial regression of $V'^{h}$ for the continuous variables for
each action $a$ denoted by $\tilde{Q}_a^{h+1}$.
\item {\it Discrete Marginalization}: Now that we have our partial
regression $\tilde{Q}_a^{h+1}$ for each action $a$, we proceed
to derive the full backup $Q_a^{h+1}$ from $\tilde{Q}_a^{h+1}$
by evaluating the discrete 
marginalization $\sum_{\vec{b}'}$ in~\eqref{eq:qfun}.
Because we previously disallowed synchronic arcs
between the variables in $\vec{b}'$ 
in the transition DBN, we can sum out each variable $b_i'$ ($1 \leq i \leq n$) 
independently.  Hence, initializing
$Q_a^{h+1} := \tilde{Q}_a^{h+1}$
we perform the discrete regression by applying the following iterative
process \emph{for each} $b_i'$ in any order
for each action $a$:
\begin{align}
Q_a^{h+1} := & \left[ Q_a^{h+1} \otimes P(b_i'|\vec{b},\vec{x},a) \right]|_{b_i' = 1} \nonumber \\
 & \oplus \left[ Q_a^{h+1} \otimes P(b_i'|\vec{b},\vec{x},a) \right]|_{b_i' = 0}.
\end{align}
This requires a variant of the earlier restriction operator $|_v$ that
actually \emph{sets} the variable $v$ to the given value if present.
Note that both $Q_a^{h+1}$ and $P(b_i'|\vec{b},\vec{x},a)$ can be represented
as case statements (discrete CPTs \emph{are} case statements), 
and each operation produces a case statement.
Thus, once this process is complete, we have marginalized over
all $\vec{b}'$ and $Q_a^{h+1}$ is the symbolic representation
of the intended Q-function.
\item {\it Maximization}: Now that we have $Q_a^{h+1}$ in
case format for each action $a \in \{a_1,\ldots,a_p\}$, obtaining
$V^{h+1}$ in case format as defined in~\eqref{eq:vfun} requires
sequentially applying
\emph{symbolic maximization} as defined previously:
\begin{align*}
V^{h+1} & = 
\max(Q_{a_1}^{h+1},\max(\ldots,\max(Q_{a_{p-1}}^{h+1},Q_{a_p}^{h+1})))
\end{align*}
\end{enumerate}
By induction, because $V^0$ is a case statement and applying
SDP to $V^h$ in case statement form produces $V^{h+1}$ in case
statement form, we have achieved our intended
objective with SDP.  On the issue of correctness,
we note that each operation above simply implements one of the
dynamic programming operations in \eqref{eq:qfun} or \eqref{eq:vfun}, 
so correctness simply follows from verifying (a) that each case
operation produces the correct result and that (b) each case operation
is applied in the correct sequence as defined in \eqref{eq:qfun} or 
\eqref{eq:vfun}.  

%To make this concrete, we provide an example of SDP for \Knapsack.

On a final note, we observe that SDP holds for \emph{any} symbolic
case statements; we have not restricted ourselves to rectangular
piecewise functions, piecewise linear functions, or even piecewise
polynomial functions.  As the SDP solution is purely symbolic,
SDP applies to \emph{any} DC-MDPs using bounded symbolic function 
that can be written in case format!  Of course, that is the theory,
next we meet practice.

\section{Extended ADDs (XADDs)}

In practice, it can be prohibitively expensive to maintain
a case statement representation of a value function with explicit
partitions.  Motivated by the SPUDD~\cite{spudd} algorithm which
maintains compact value function representations for finite discrete
factored MDPs using algebraic decision diagrams (ADDs)~\cite{bahar93add},
we extend this formalism to handle continuous variables in a data
structure we refer to as the XADD.  An example XADD for the optimal
KNAPSACK value function from~\eqref{eq:vfun_knapsack} is provided
in Figure~\ref{fig:knapsack_vfun}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
\begin{center}
\fbox{$\qquad$}
%\includegraphics[width=0.4\textwidth]{knapsack2.pdf}
\end{center}
\vspace{-3mm}
\caption{%\footnotesize 
The optimal value function for KNAPSACK
as a decision diagram: 
the \emph{true} branch is solid, the \emph{false}
branch is dashed.} \label{fig:knapsack_vfun}
\vspace{-3mm}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In brief we note that an XADD is like an ADD except that (a) the decision
nodes can have arbitrary inequalities, equalities, or disequalities (one
per node) and (b) the leaf nodes can represent arbitrary functions.
The decision nodes still have a fixed order from root to leaf
and the standard ADD
operations to build a canonical ADD (\textsc{Reduce}) and 
to perform a binary operation on two ADDs (\textsc{Apply}) 
still applies in the case of XADDs.

While exact solutions using symbolic dynamic
programming are possible in principle for arbitrary symbolic CSE transition
and reward functions, we note that it is much more difficult to
devise a canonical and compact form for representations 
such as~\eqref{eq:expr_reward}
in comparison to~\eqref{eq:simple_reward}.
Hence while we have used general examples throughout the paper
to demonstrate the expressiveness of our approach, we will restrict
XADDs to use \emph{polynomial} functions only.  We note the main advantage
of this for the XADD is that we can put the leaf and decision nodes
in a \emph{unique, canonical} form, which allows us to minimize 
redundancy in the XADD representation of a case statement.

It is fairly straightforward for XADDs to support all case operations
required for SDP.  Standard operations like unary multiplication,
negation, $\oplus$, and $\otimes$ are implemented exactly as they
are for ADDs.  The fact that the decision nodes have internal structure
is irrelevant, although this means that certain paths in the XADD
may be inconsistent or infeasible (due to parent decisions).  To
remedy this, when the XADD has only linear decision 
nodes, we can use the feasibility checkers of
a linear programming solver (e.g., as also done in~\cite{penberthy94}) 
to prune unreachable nodes in the XADD; later we show results demonstrating
impressive reductions in XADD size using this style of pruning.

The only two XADD operations that pose difficulty are substitution
and maximization.  In principle substitution is simple, the only
caveat is that substitutions modify the decision nodes and hence
decision nodes may become unordered.  We can use the 
recursive application of ADD binary operations $\otimes$ and $\oplus$ 
as given in Algorithm~\ref{fig:correct} to correctly reorder the
nodes in an XADD $F$ after substitution.  A related reordering
issue occurs during XADD maximization; because XADD maximization
can introduce new decision nodes (which occurs at the leaf when
two leaf functions are compared) and these decision nodes may
be out of order w.r.t.\ the diagram, reordering as defined
in Algorithm~\ref{fig:correct} must also be applied after
maximization.  

On a final note, we mention that an implementation of SDP using case
statements without any attempt to merge and simplify cases often
cannot get past the first or second SDP iteration; as our results show
next, XADDs allow SDP to scale to much longer horizons in practice.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\incmargin{1em}
%\linesnumbered
\begin{algorithm}[t!]
\SetKwFunction{getCanonicalNode}{{\sc GetCanonicalNode}}
\SetKwFunction{reduce}{{\sc Reorder}}
\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}

\Input{$F$ (root node for possibly unordered XADD)}
\Output{$F_r$ (root node for an ordered XADD)}
\BlankLine
\Begin{
   //if terminal node, return canonical terminal node\\
   \If{F is terminal node}
   {
   \Return{canonical terminal node for polynomial of $F$}\;
   }
   //nodes have a $\mathit{true}$ \& $\mathit{false}$ branch and $\mathit{var}$ id\\
   \If{$F \rightarrow F_r$ is not in Cache}
   {
    $F_{\mathit{true}}$ = \reduce{$F_{\mathit{true}}$} $\otimes \; \mathbb{I}[F_\mathit{var}]$ \;
    $F_{\mathit{false}}$ = \reduce{$F_{\mathit{false}}$} $\otimes \; \mathbb{I}[\neg F_\mathit{var}]$\;
    $F_r = F_{\mathit{true}} \oplus F_{\mathit{false}}$\;
    insert $F \rightarrow F_r$ in Cache\;
   } 
   \Return{$F_r$}\;
}
\caption{{\sc Reorder}(F)  \label{fig:correct}}
\end{algorithm}
\decmargin{1em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Empirical Results}

\newpage

\section{Introduction}


Markov decision processes (MDPs) have been a theoretical model for many planning problems in the recent years. Although the most common solution to MDPs is to use value iteration or policy iteration, they can quickly fall into Bellman's curse of dimensionality problem for large state – action spaces ~\cite{puterman}. In many problems this can be avoided by modelling the problem with continuous state – action spaces which is also a closer resemblance to real-world problems. 

Examples of problems in the continuous state and action domain can include Robotics, Operation Research and Management problems. The OR (Operation Research) literature has used MDPs to model many problems such as Inventory Control [] or Water Reservoir management []. These are examples where the state space can be modelled by discrete and continuous Markov decision processes and where the actions can also be considered continuous.

Most existing solutions concentrate on approximation techniques such as sampling [] that requires computational effort while providing no guarantee to the optimal solution, or other methods such as discretizing the state-action space[] that has scalability problems for large domain sizes. Meanwhile little effort has been devoted to exact solutions using the continuous variables beyond the piecewise rectilinear value function setting which contributes to a small subset of the problems in this domain. \\
In the previous paper [], the solution to continuous state MDPs was investigated, here we introduce a new model for Discrete and continuous state and Action MDPs (DCA-MDPs) and propose a new algorithm for value iteration in a Symbolic Dynamic Programming(SDP) format. Although the exact solution presented in [] covers the discrete and continuous state domain, it could not be used for planning in continuous action spaces. 

We exploit the algebraic properties of the maximization operator for continuous actions and develop disjoint case partitions that can be used to factor the actions in the Q-value to obtain the next value function. This symbolic maximization approach allows us to go beyond the linear function domain and apply the technique for non-linear reward and transition functions. Our method is scalable to higher horizons as well as being applicable to multi-variate continuous action domains. We represent the reward function as the function that is used to reward and penalize the agent based on the action costs. Both the reward and the transition function are represented using First-order logic with arbitrary piecewise symbol functions. These will be explained more deeply in section 2. This section also explains the main technique for applying continuous actions to any arbitrary function in depth. 

% Do we need any specific mention for the UB,LB, root of action here? 
The practical implementation of case statements of section 2 is the extended ADDs (XADDs) structure, which is discussed in section 3. In section 4 the symbolic value iteration algorithm for continuous action selection is presented and the approach of maximizing the action variables is examined by visual examples. Empirical results on various problems such as the Mars Rover, Inventory control and Water reservoirs are then demonstrated in section 5. We present some of the related works in section 6 and conclude with a discussion on the future work in the last section. 

\section{Case Representation}

To work in the symbolic space with lifted solutions, we need a representation for the symbolic functions and logical operators. Here we define such a notation along with the required operations for the SDP algorithm in the next sections. We consider the following simple example adopted from [] that is used throughout this paper:

\begin{example*}
[\MarsRoverNL] In a general \MarsRover\ domain, a rover is supposed to approach one or more target points and take images of these points.We consider the continuous non-linear example where the coordinate of the rover is a continuous variable $x$ and the amount of movement is also a continuous action $a$. A boolean variable $b$ is used for indicating whether the rover has already taken a picture at that point. The rover will move from any distance towards the target position of taking a picture only limited by the speed of moving. This constraints the rover to move more than 10 steps at a time. We consider the \MarsRoverNL  to have a non-linear transition or reward function.

\end{example*}

The \textit{case} notation can present any symbolic function in the following format: 

\begin{equation}
f = 
\begin{cases}
  \phi_1: & f_1 \\ 
 \vdots&\vdots\\ 
  \phi_k: & f_k \\ 
\end{cases}
\end{equation}

Where the $\phi_i$ are logical formulae defined over the state and action space $(\vec{b},\vec{x},\vec{a})$ that can include arbitrary logical ($\land,\lor,\neg$) combinations of the boolean variable $\vec{b}$ and can also contain any sort of
inequalities (such as $\geq,>,\leq,<$) where the left and right operands can be \emph{any} function of the continuous variables $\vec{x},\vec{a}$. 
The $f_i$ can be \emph{any} functions of the state action variables.  
\\
Each of the case statements introduce a partitioning on the domain where each partition is mutually exclusive and disjoint from the other partitions since otherwise they can be converted into more partitions with this property using logical operators: 
{%\footnotesize 
\begin{equation}
\begin{cases}
\begin{array}{c}
\varphi_{1}:f_{1}\\
\varphi_{2}:f_{2}
\end{array} & \simeq\begin{cases}
\begin{array}{c}
\varphi_{1}\wedge\varphi_{2}:mutal\\
\varphi_{1}\wedge\neg\varphi_{2}\\
\neg\varphi_{1}\wedge\varphi_{2}\\
\neg\varphi_{1}\wedge\neg\varphi_{2}:nondisjoint
\end{array}\end{cases}\end{cases}
\end{equation}
}
The reward and transition functions below can show the case statements in more details:

\begin{example*}[\MarsRoverNL]
We define an immediate negative reward for the cost of moving the rover ( e.g. fuel costs) outside the target area or has already taken a picture, and a non-linear reward to allow a rover to move progressively more slowly as it approaches a target position in order to reach the position with high accuracy. The transition to the next state is defined by the current rover location and the amount it moved, limited to the maximum movement bound.
The transition function of the location and reward for \MarsRoverNL\ are displayed below:
\end{example*}

{\footnotesize
\begin{align*}
x' & = \begin{cases}
a \leq 10 \wedge a\geq -10 : & x + a \\
\neg (a \leq 10 \wedge a\geq -10 ) : & x \\
\end{cases}\\
R & = \begin{cases}
x \leq 2 \wedge x \geq -2 \wedge b: & -10 \\
x \leq 2 \wedge x \geq -2 \wedge \neg b: 25 *(4 - x^2)  \\
\neg (x \leq 2 \wedge x \geq -2): &  -10 \\
\end{cases}
\end{align*}}

Having defined the function notation in case statements, we now consider applying logical operators on a function. 
If the operator is \textit{Unary} such as scalar multiplications (e.g. $0.5 \cdot a$) or negation $-f$ it is simply applied to each of the disjoint partitions. 
\\ For \textit{Binary} operations, the cross-product of the logical partitions is considered and then the operator is applied to the result. The reward function of \MarsRoverNL is an 
example for binary $\oplus$ where the logical AND ($\wedge$) of logical formulas is taken along with their negation, to form 4 partitions with different function definitions. As stated in (2) the last partition can be partitioned into two disjoint partitions.
\begin{align*}
\begin{cases}
x > 2 : &  -10 \\
x < -2 : &  -10 \\
\end{cases}
\end{align*}

Likewise, $\ominus$ and $\otimes$ are applied by subtracting or multiplying partition values respectively.  
Apart from the logical operators, we also need to perform other operations such as restriction, substitution and maximization on case statements to be able to completely explain the SDP algorithm. 

In \emph{restriction} a function $Q$ is restricted to cases that satisfy some formula $\phi$ (e.g. the binary variable $d_i$ then function can be restricted to either the true or false branch $Q|_{d_i}$). Restriction can be performed by appending  the logical formula (e.g. of \MarsRoverNL sets $b_i=0 or b_i=1$) to each case partition:
{\footnotesize
\begin{center}
\begin{tabular}{r c c l}
&
\hspace{-6mm} 
  $Q = \begin{cases}
    \phi_1: & f_1 \\ 
   \vdots&\vdots\\ 
    \phi_k: & f_k \\ 
  \end{cases}$
&

&
\hspace{-2mm}
  $Q|_{b_i = 0} = \begin{cases}
    \phi_1 \land \neg b_i : & Q_1 \\ 
   \vdots&\vdots \\ 
    \phi_k \land \neg b_i : & Q_k \\ 
  \end{cases}$
\end{tabular}
\end{center}
}
%This operator will be used in computing the values of the binary demands in the next sections.\\

\emph{Symbolic substitution} takes a set $\sigma$ of variables and their substitutions, e.g.
 
{\footnotesize
\begin{center}
$\sigma = \{ b_1 / b_1', \ldots, b_n / b_n', x_1 / x_1', \ldots, x_m / x_m' \}$ 
\end{center}
}
where the LHS of the $/$ represents the substitution variable and the RHS of the $/$ represents 
the expression that should be substituted in its place.
While the example substitutes the next state (primed) with the current state, we can use substitution again on the transition functions of the variables to build the next state:
{\footnotesize
\begin{center}
 $\sigma = \{ b'/(b:0.7,\neg b:0.3),$\\$ x'/(b \wedge a\leq 10: x + a ) \}$
\end{center}
}
For each partition, we perform substitution on the logical part $\phi$ and the functional part $f$. For the logical part of case partitions substitution occurs by applying $\sigma$ to each inequality operand. For example if $\phi_i: b' \wedge x'>200$ after substituting the result is $\phi_i\sigma : b \wedge x + a > 200 $ where with $b$ was chosen with probability 0.7. 

As for the function values, they are again substituted in a similar fashion; for the second partition of the reward function:  $r_2 = 25 * (4 - x^2)$  then  
\begin{equation}
r_2\sigma = 100 - 25*x^2 - 25* a^{2} + 50*a*x 
\end{equation}
The partitions remain mutually exclusive and disjoint after the substitution due to logical consequence of applying $\sigma$. Substitution is defined generally in the following:
%is this required? 
{\footnotesize
\begin{equation}
\begin{tabular}{r c c l}
&
\hspace{-6mm} 
  $f = \begin{cases}
    \phi_1: & f_1 \\ 
   \vdots&\vdots \\ 
    \phi_k: & f_k \\ 
  \end{cases}$
&

&
\hspace{-2mm}
  $f\sigma = \begin{cases}
    \phi_1\sigma: & f_1\sigma \\ 
   \vdots&\vdots \\ 
    \phi_k\sigma: & f_k\sigma \\ 
  \end{cases}$
\end{tabular}
\end{equation}
}

\emph{Symbolic maximization} is defined by considering all combinations of the logical formulas $\phi_i$ and $\psi_i$ and taking the maximum according to the function values $f$ and $g$:
\vspace{-5mm}

{\footnotesize
\begin{center}
\begin{tabular}{r c c c l}
&
\hspace{-9mm} $\max \Bigg(
  \begin{cases}
    \phi_1: & f_1 \\ 
    \phi_2: & f_2 \\ 
  \end{cases}$
$,$
&
\hspace{-4mm}
  $\begin{cases}
    \psi_1: & g_1 \\ 
    \psi_2: & g_2 \\ 
  \end{cases} \Bigg)$
&
\hspace{-4mm} 
$ = $
&
\hspace{-4mm}
  $\begin{cases}
  \phi_1 \wedge \psi_1 \wedge f_1 > g_1    : & f_1 \\ 
  \phi_1 \wedge \psi_1 \wedge f_1 \leq g_1 : & g_1 \\ 
  \phi_1 \wedge \psi_2 \wedge f_1 > g_2    : & f_1 \\ 
  \phi_1 \wedge \psi_2 \wedge f_1 \leq g_2 : & g_2 \\ 
  \phi_2 \wedge \psi_1 \wedge f_2 > g_1    : & f_2 \\ 
  \phi_2 \wedge \psi_1 \wedge f_2 \leq g_1 : & g_1 \\ 
  \phi_2 \wedge \psi_2 \wedge f_2 > g_2    : & f_2 \\ 
  \phi_2 \wedge \psi_2 \wedge f_2 \leq g_2 : & g_2 \\ 
  \end{cases}$
\end{tabular}
\end{center}
}

According to this definition, maximization of any finite number of functions can be performed by taking a pair of the functions and iteratively building the resulting maximization. For the finite number of discrete actions the following maximization results in the value function: 
\begin{align}
V & 
= \max(Q_{a_1},\max(\ldots, \max (Q_{a_{p-1}},Q_{a_p})))
\end{align}
Apart from this maximization, we also have to consider maximizing a function (such as the Q-function) for continuous variables below.

\subsection{Maximization of continuous actions}


In the previous section, we covered the operations required for the SDP algorithm in section 4 except for maximizing a function (such as the Q-function) for an infinite number of actions ($\Delta$) in our continuous case. 

To compute a maximum of a function over the continuous variable $\Delta$, $\max_{\Delta} f $ for $f$
in the case format (1), we can rewrite it in the following equivalent form using the mutually exclusive and disjoint property of partitions (3):
\begin{equation}
\max_{\bigtriangleup}\sum_{i}\varphi_{i}f_{i}=\sum_{i}\max_{\bigtriangleup}\varphi_{i}f_{i}
\end{equation}

Hence we can compute the maximum separately for each case partition (producing a case statement) and then $\sum$ the results using $\oplus$.
To continue with maximization in one partition, we use our \MarsRoverNL example in one of the branches of the first iteration for continuous maximization that covers this technique completely. This branch is obtained from replacing the transition function of each variable in the reward function:

{\footnotesize
\begin{align*}
if (\neg b \wedge (x > 2) \wedge (\Delta <=10) \wedge (\Delta >=-10) \\
\wedge (x+\Delta <=2) \wedge (x+\Delta >=-2)) then \\ 
(90 + (-50 * \Delta * x) + (-25 * \Delta ^2) + (-25 * x^2)) \\ 
\end{align*}
}

We also consider some mathematical definitions for the technique of maximizing the continuous action: 
\begin{enumerate}
\item A real-valued function f defined on the real domain is said to have a global (or absolute) maximum point at the point x, if for all x :$ f(x^{*}) \geq f(x) $. The value of the function at this point is the maximum of the function.
\item Boundedness theorem states that a continuous function f in the closed interval (a,b) is bounded on that interval. That is, there exist real numbers m and M such that:
\begin{align*}
\forall x \in \{a,b\} : m \leq f(x) \leq M 
\end{align*}
\item The first partial derivatives as to x (the variable to be maximized) are zero at the maximum. The second partial derivatives are negative. These are only necessary, not sufficient, conditions for a maximum because of the possibility of saddle points.
\end{enumerate}

Based on these definitions, the maximum value of a function is defined at the boundaries and roots of that function according to the continuous variable $\Delta$. If this function were linear and a polynomial of a certain degree k, then the maximum for that function would occur on any of the (k-1) roots of the function or at the lower and upper bounds defined for it.
\\We can write the explicit functions for these bounds in  \emph{piecewise linear case form} for the respective lower and upper bounds $LB$, $UB$ and possible roots of our example as shown below. The maximum of all the lower bounds is obtained by taking a maximum over this set and the maximum of the upper bounds is obtained by taking a minimum over the set of UB.

{\footnotesize
\begin{center}
\begin{tabular}{r c c c l}
&
\hspace{-9mm} $LB:= \max \Bigg(
  \begin{cases}
\Delta >= -10 \\ 
\Delta >= -2 -x\\ 
  \end{cases}$
$=$
&
\hspace{-4mm}
$  \begin{cases}
x \leq 8: & -2 -x \\ 
x > 8: &-10\\ 
  \end{cases}$
&
\hspace{-4mm} 
\\
&
\hspace{-4mm}
$UB:= \max \Bigg(
  \begin{cases}
\Delta <= 10 \\ 
\Delta <= 2 -x\\ 
  \end{cases}$
$=$
&
\hspace{-4mm}
$  \begin{cases}
x \geq -8: & 2 -x \\ 
x < -8: &10\\ 
  \end{cases}$
&
\hspace{-4mm} 
\\
&
\hspace{-4mm}
$Roots:= 
  \begin{cases}
- 50* \Delta -50*x =0 := -x \\ 

  \end{cases}$
\end{tabular}
\end{center}
}

A lower bound on action $\Delta$ occurs when the inequalities of $\geq ,>$ have action $\Delta$ on their LHS; then any expression on the RHS is considered a lower bound on action $\Delta$. Similar to this, an upper bound occurs for inequalities of $\leq , <$ and the RHS expression is considered an upper bound for action $\Delta$. 
The roots of the function are also defined by taking the first derivative of $f$ with respect to $\Delta$.

Having the UB,LB and root of each case partition, we can maximize the value of each case function according to the bounds. We want to factor out the action variable $\Delta$ and have a function that is $\Delta$-free (we show later that this is equal to maximizing the Q-function values in order to obtain the value function). This means replacing the continuous action with the possible maximum points; LB,UB and roots. Replacing the action with a constant, variable or another function is equal to applying the \emph{substitute} operator (2). The result of the maximum defines a case statement itself, which implies maximization over the two values obtained after the substitution of the LB and UB in the function definition of the case. The same is applied to the second case statement with the maximization to hold for the root function as well: 

\begin{align*}
\max_{\Delta}
\begin{cases}
(90 + (-50 * UB * x) + (-25 * UB ^2) + (-25 * x^2)) \\ 
(90 + (-50 * LB * x) + (-25 * LB ^2) + (-25 * x^2)) \\ 
(90 + (-50 * Roots * x) + (-25 * Roots ^2) + (-25 * x^2))\\ 
  \end{cases}
\end{align*}

All different combinations of the three maximums have to be considered for the above equation. For example for the branch of $(10 <=x <= 12)$ we have:
\begin{align*}
\begin{cases}
(100 + (-20 * x) + (1 * x * x)) >= 0) : 90\\
else : (-2410 + (500 * x) + (-25 * x * x))\\
( 96.4 + (-20 * x) + (1 * x * x)>= 0): 0\\
else ( -2410 + (500 * x) + (-25 * x * x))\\
  \end{cases}
\end{align*}
The result is a normal symbolic maximization as defined in the previous section. 
To enforce correctness of bounds symbolically we must have $LB \leq ROOT \leq UB$, this means adding the pair of constraints for the bounds to the final result of maximization, as well as all the constraints that did not effect the $\Delta$ boundaries (i.e, $x<=2$):

\begin{align*}
\max_{\Delta}
\begin{cases}
(90 + (-50 * UB * x) + (-25 *UB ^2) + (-25 * x^2))\\ 
(90 + (-50 * LB * x) + (-25 *LB ^2) + (-25 * x^2))\\ 
(90 + (-50 * Roots * x) + (-25 *Roots ^2) + (-25 * x^2))\\ 
  \end{cases}
\end{align*}

The last statements of the last two lines can be removed as a tautology, but the other constraints must remain.

\section{Extended ADDs (XADDs)}

In practice, a case statement representation of a value function with explicit
partitions is computationally expensive.  
Extended Algebric decision diagrams (XADDs) were introduced in [] and will be used 
as the development tool throughout this work. XADDs were motivated by the SPUDD ~\cite{spudd} algorithm which
maintains compact value function representations for finite discrete
factored MDPs using algebraic decision diagrams (ADDs) ~\cite{bahar93add},
and was extended to handle continuous variables.  An example XADD for the optimal
\InventoryControl value function is provided
in Figure~\ref{fig:rover_R}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
\begin{center}
\includegraphics[width=0.33\textwidth]{Figures1/roverR.pdf}
\end{center}
\vspace{-3mm}
\caption{%\footnotesize 
The optimal value function for horizon one of \InventoryControl\ 
as a decision diagram: 
the \emph{true} branch is solid, the \emph{false}
branch is dashed.} \label{fig:rover_R}
\vspace{-3mm}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

In an XADD the decision nodes can have arbitrary inequalities (one
per node) and the leaf nodes can represent arbitrary functions.
Standard ADD operations to build a canonical ADD and 
to perform a binary operation on two ADDs applies in the case of XADDs.

It is fairly straightforward for XADDs to support all case operations defined in the previous section. 
Binary operations or restriction is done similar to the ADD case while maximization and substitution 
requires reordering of the decision nodes since the former introduces new nodes and the latter modifies
the decision nodes. 

For the maximization of continuous variables, we consider two separate XADDs for the 
upper and lower bounds on the continuous action and then apply the maximum on these two functions. 
This operation is shown in detail in the next section. 

In general, the fact that the decision nodes have internal structure is irrelevant, but means that certain 
paths in the XADD may be inconsistent or infeasible (due to parent decisions).  To further simplify our settings, 
when the XADD has only linear decision nodes, we can use the feasibility checkers of a linear programming solver 
to prune unreachable nodes in the XADD following the approach of []. Using the XADD structure the value
iteration algorithm can scale to much longer horizons compared to the original SDP algorithm. In the next section we introduce the model we use for our SDP algorithm.

\section{Discrete and Continuous State-Action MDPs}

% I have defined action to be continuous only
We assume that a general class of fully observable Markov decision processes with discrete and continuous states and actions can model our domain problems.
The state space is represented by a vector $(\vec{b},\vec{x}) = ( b_1,\ldots,b_n,x_{1},\ldots,x_m )$ consisting of Boolean variables
 $b_i$ ($1 \leq i \leq n$) such that $b_i \in \{ 0,1 \}$ and continuous Real variables $x_j$ ($1 \leq j \leq m$) s.t. $x_j \in [L_j,U_j]$ and $L_j,U_j \in
\mathbb{R}; L_j \leq U_j$.  The action space contains a finite number of continuous actions $A
= \{ a_1, \ldots, a_p \}$ where each $a_{\Delta} \in [L_{\Delta},U_{\Delta}]$ and $L_{\Delta},U_{\Delta} \in
\mathbb{R}; L_{\Delta} \leq U_{\Delta}$.

The transition function is defined as the probability of the next state given the current state and action assuming the Markov property.  Considering the Dynamic Bayesian Network model for modelling each state variable and disallowing synchronic arcs in between them,  results in the following definition for the transition function:   
\begin{align}
P(\vec{b}',&\vec{x}'|\vec{b},\vec{x},a_{\Delta}) = \label{eq:dbn} \\
& \prod_{i=1}^n P(b_i'|\vec{b},\vec{x},a_{\Delta}) \prod_{j=1}^m P(x_j'|\vec{b},\vec{b}',\vec{x},a_{\Delta}) \nonumber 
\end{align}

Here the first probability for binary discrete variables can be represented by conditional probability tables (CPTs). The second probability for continuous variables is represented using conditional stochastic equations (CSEs) wthat are Markovian and deterministic with the definition of any arbitrary (e.g. non-linear) function with continuous actions and states such as:
\vspace{-3mm}
{\footnotesize
\begin{align}
P(x_1' | \vec{b},\vec{b}',\vec{x},a_{\Delta}) = \delta\left[ x_1' - 
\begin{cases}
b_1' \land a_{\Delta} \leq 1 : & \exp(x_1^2 - x_2^2) \\
\neg b_1' \lor  x_2^2 > 1 : & x_1 + x_2 +a_{\Delta} \\
\end{cases}
\right] \label{eq:ex_csde}
\end{align}}
Using the Dirac function the conditional probability is guaranteed to integrate to 1 over the variable, furthermore the probability is stochastic since it conditions on Boolean random variables as well as sampling them stochastically at the same time. \\
The reward function represents the immediate reward of taking an action in a state and it can take the form of any arbitrary function such as:
\begin{align}
R_{a_{\Delta}}(\vec{b},\vec{x}) = \begin{cases}
x_1^2 + x_2^2 \leq 1 : & 1 - x_1^2 - x_2^2  \\
x_1^2 + x_2^2 > 1 : & 2 * a_{\Delta}^2 \\
\end{cases} \label{eq:simple_reward}
\end{align}
The policy $\pi$ specifies which action $a_{\Delta}$ to take in the current state  $(\vec{b},\vec{x})$ and the goal is to find an optimal sequence of policies
$\Pi^* = (\pi^{*,1},\ldots,\pi^{*,H})$ that maximizes the expected sum of discounted reward over a horizon $h \in H $: \\
\begin{align}
V^{\Pi^*}(\vec{x}) & = E_{\Pi^*} \left[ \sum_{h=0}^{H} \gamma^h \cdot r^h \Big| \vec{b}_0,\vec{x}_0 \right], \label{eq:vfun_def}
\end{align}
where $\gamma$ is the discount factor between 0 and 1, and  $r^h$ is the reward in horizon $h$ assuming the state state is at $h=0$.\\
This optimal policy can be obtained using the value iteration algorithm. Starting  with an initial value function, the value of taking an action in the current state is defined using the Q-function:  

\vspace{-3mm}

{\footnotesize
\begin{align}
& Q_{a_{\Delta}^{h+1}}(\vec{b},\vec{x}) = R_{a_{\Delta}}(\vec{b},\vec{x}) + \gamma \cdot \\ 
& \sum_{\vec{b}'} \int_{\vec{x}'} \left( \prod_{i=1}^n P(b_i'|\vec{b},\vec{x},a_{\Delta}) \prod_{j=1}^m P(x_j'|\vec{b},\vec{b}',\vec{x},a_{\Delta}) \right) V^h(\vec{b}',\vec{x}') d\vec{x}' \nonumber
\end{align}
}

Now given the Q-function for each action, the h+1-stage-to-go value function is defined as (3): 
\begin{align}
V^{h+1}(\vec{b},\vec{x}) & = \max_{a \in A} \left\{ Q^{h+1}_a(\vec{b},\vec{x}) \right\} \label{eq:vfun}
\end{align}

If the horizon is finite then the optimal value function continues to the last horizon but if the problem has infinite horizon then a termination criteria is defined, in most problems if the value function of two successive iterations are equal, the algorithm terminates. In the next section we define the symbolic dynamic programming algorithm as the solution to DCA-MDPs.

\section{Symbolic Dynamic Programming (SDP)}

In this section we present our exact algorithm for obtaining the optimal policy for continuous action and states. This algorithm avoids approximating as well as enumerating the state and action. The Continuous state-action dynamic programming (CSA-DP) algorithm in Algorithm 1, implements value iteration for DCA-MDPs producing a sequence of value functions $V^0,V^1,...$ until convergence and uses the XADD structure to efficiently represent state-action partitions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\incmargin{1em}
%\linesnumbered
\begin{algorithm}[t!]
\SetKwFunction{getCanonicalNode}{{\sc GetCanonicalNode}}
\SetKwFunction{reduce}{{\sc Reorder}}

%\SetKwInOut{Input}{input}
%\SetKwInOut{Output}{output}

%\Input{$F$ (DCA-MDP: discrete-continuous states, continuous actions, transition and reward function)}
%\Output{$F_r$ (Value function $V^h$ and optimal policy $\pi^*$)}
\BlankLine
\Begin{
   \textbf{if} R is action independent \textbf{then} set $V^0=R$     \textbf{else} $V^0 = 0$\\
   \While {$h < \mathit{maxHorizon}$}{
      	\ForEach { action variable $\Delta$ in A }{
      		Prime Value-function($V^{'h}$)using substitute\\
      		\ForEach {continuous state variable in S}
      		
      		{
      			Perform Continuous integration: 
      			\begin{align}
      			\tilde{Q}_{a_{\Delta}^{h+1}} :=\int_{x_j'} \delta[x_j' - g(\vec{x})] V'^{h} dx_j' \; \\
      			      			= \; V'^{h} \{x_j' / g(\vec{x}) \}
      			\end{align}
      		}
      		
      		\ForEach {discrete state variable in S }
      		
      		{
      			Perform Discrete marginalization: 
      			\begin{align}
      			\tilde Q_{a_{\Delta}^{h+1}} := \left[\tilde Q_{a_{\Delta}^{h+1}} \otimes P(b_i'|\vec{b},\vec{x},a) \right]|_{b_i' = 1} \nonumber \\
      		   	 \oplus \left[\tilde Q_{a_{\Delta}^{h+1}} \otimes P(b_i'|\vec{b},\vec{x},a) \right]|_{b_i' = 0}
      		   	\end{align}
      		}
      		Multiply discount factor $\gamma$
      		\begin{align*}
      		\tilde Q_{a_{\Delta}^{h+1}} = \tilde Q_{a_{\Delta}^{h+1}} \otimes \gamma
      		\end{align*}
      		Add Reward function to Q-function:
      		\begin{align*}
      		\tilde Q_{a_{\Delta}^{h+1}} = \tilde Q_{a_{\Delta}^{h+1}} \oplus R
      		\end{align*}
      		
      		\ForEach {case partition in $\tilde Q_{a_{\Delta}^{h+1}}$ }
      		{
      			 1- Compute bounds on $\Delta$: (UB, LB, Roots)
      			
      			 2- Evaluate $\tilde Q_{a_{\Delta}^{h+1}}$ using bounds
      			(Substitute and take maximum)
      			
      			 3- Add bound constraints ()$LB\leq Roots\leq UB$) 
      		}
      	}
      	
      	Compute new Value function: 
      	\begin{align}
      	V^{h+1} & = 
      	\max(Q_{a_1}^{h+1},\max(\ldots,\max(Q_{a_{p-1}}^{h+1},Q_{a_p}^{h+1})))
      	\end{align}
      }
}
\caption{{\sc CSA-DP} }
\end{algorithm}
\decmargin{1em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We now describe each section of the algorithm using the \MarsRoverNL example. 

\begin{enumerate}
\item The symbolic value iteration algorithm works with the XADD structure for efficiency. This means that each of the i-stage-to-go value functions ($V^i$) is set to be an XADD. In most problems in this domain, the reward function defines action costs as a penalty to avoid the agent of acting more than required. For this reason we define the first value function equal to zero $V^0=0$ which is a tree with only one leaf node [0]. If the reward function of the DCA-MDP is independent of the actions, then the first value function can be equal to this reward $V^0=R$.\\

\item Next, for $h > 0$ the algorithm iterates over the horizon until a convergence criteria is met or in our case the maximum number of iterations is reached. The next parts of the algorithm is performed for each horizon. For each horizon we take a continuous action $\Delta$ and perform the following steps in sequence.\\

\item The first step of value iteration for horizon $h$, action $\Delta$ is to prime the current state so it becomes the next state. This can be performed by substituting the state variables with their primed versions. Given as an example in section 2 , we set
{\footnotesize
\begin{align*}
\sigma = \{ b_1 / b_1', \ldots, b_n / b_n', x_1 / x_1', \ldots, x_m / x_m' \}
\end{align*}
}
and obtain $V'^{h} = V^{h}\sigma$.

\item Next, we need to consider the back-up step of the Bellman equation of ~\eqref{eq:qfun} restated here: 
 \vspace{-3mm}
 
 {\footnotesize
 \begin{align}
 & Q_a^{h+1}(\vec{b},\vec{x}) = R_a(\vec{b},\vec{x}) + \gamma \cdot \label{eq:qfun} \\ 
 & \sum_{\vec{b}'} \int_{\vec{x}'} \left( \prod_{i=1}^n P(b_i'|\vec{b},\vec{x},a) \prod_{j=1}^m P(x_j'|\vec{b},\vec{b}',\vec{x},a) \right) V^h(\vec{b}',\vec{x}') d\vec{x}' \nonumber
 \end{align}}
 
We first evaluate the integral marginalization $\int_{\vec{x}'}$ over the continuous variables in the algorithm. For each of the continuous variables $x'_j$, the only functions dependent on this variable are $V'^{h}$
and $P(x_j'|\vec{b},\vec{b}',\vec{x},a) = \delta[x_j' - g(\vec{x})]$; 
hence, marginal over $x_j'$ need only be computed over
their product. Also according to [] this integration is equal to substituting $\sigma = \{ x_j' / g(\vec{x}) \}$
on $V'^{h}$ as shown in (11) and (12) in the algorithm.

This substitution is performed for each $x_j'$ ($1 \leq j \leq m$) updating $\tilde{Q}_a^{h+1}$ each time,
then after elimination of all $x_j'$ the partial regression of $V'^{h}$ for the continuous variables for
each action $a$ is denoted by $\tilde{Q}_a^{h+1}$. Substitution preserves the disjoint partition property, so given that the previous value function is also a case statement, substituting it with the transition function which is again in the form of a case statement, produces $\tilde{Q}_a^{h+1}$ as a case statement that can also be presented by an XADD.
% need to mention conditional substitution here??? 

\item The next step is to complete the backup step using $\tilde{Q}_a^{h+1}$. 
For this we consider marginalization of discrete variables $\sum_{\vec{d}'}$ in~\eqref{eq:qfun}.
The discrete regression applies the following iterative process for each $d_i'$ according to (13).
As explained in an example for the restriction operator $|_v$ the variable v is set to the given value if present.
Both $Q_a^{h+1}$ and $P(b_i'|\vec{b},\vec{x},a)$ are represented
as case statements and restriction preserves this property, thus after marginalizing over all $\vec{b}'$ , the resulting $ \tilde{Q_a^{h+1}}$ is the symbolic representation
of the intended Q-function.


\item Now that we have $Q_a^{h+1}$ in
the case format we perform continuous maximization for action $\Delta$ as explained in section (2.1). Figure 2 also explains this step visually using XADDs from the \MarsRoverNL problem. We take each case statement which are disjoint partitions on the state-action space independently and keep a running maximum for the result. In Figure 2, we shown 2 of the ??  branches and consider the steps of our algorithm on them.
For each such partition, we first build the bounds on $\Delta$ based on the nodes: 
\begin{enumerate}
\item Consider two XADDs for the UB, LB and the root
\item For each decision node , isolate the action $\Delta$ variable on the LHS and take everything to the RHS.
\item Divide the RHS by the coefficent of $\Delta$
\item Flip the inequality sign if the false branch of the parent decision is considered or the coefficient of $\Delta$ is negative. 
\item Consider the RHS as the UB if inequality is ($<, \leq$), else add RHS to the LB.
\item Perform Symbolic Minimum on the upper bound XADD and Symbolic Maximum on the lower bounds
\item Take first derivative of the Leaf of this case partition and equal that to zero
\item If a value (constant or variable) appears for action $\Delta$ this is the root value.(XADD with single node)
\item The two resulting XADDs now have the UB, LB and roots for this branch
\end{enumerate}


Figure 2 shows the XADDs for the UB,LB and root. Having obtained the XADDs for the bounds, we now evaluate the bounds on the Q-function value at the leaf. This is equal to substituting the bounds in the leaf for $\Delta$.
% add substitute formula? done it before

We then take the maximum of the bounds with regard to the new XADDs. This is equal to the Symbolic maximum over three different functions which is an XADD itself: 
% add maximum function?already done it with examples

In the last step, we add the constaints for the bounds as well as the action-independent decisions of this partition to the resulting XADD. Figure 2 shows the end result XADD for both partitions. The running maximum which is an initially empty XADD is now used to take the maximum of each branch after they are produced. By the time all the case statements of $ \tilde{Q_a^{h+1}}$ have been processed in this algorithm, the running maximum will contain the maximum over all the partitions which is independent of action $\Delta$ now, or the  $V_{\Delta}^{h+1}$

This algorithm is performed for each of the continuous actions in $a \in \{a_1,\ldots,a_p\}$ separately. Obtaining the overall $V^{h+1}$ in case format as defined in ~\eqref{eq:vfun} requires sequentially applying \emph{symbolic maximization} as defined in (14).
\end{enumerate}

We illustrate the maximization algorithm using the \MarsRoverNL example in Figure ~\ref{fig:rover_algorithm}. The non-linear branch of the value function of iteration one is illustrated as in section 2.1. 
The other branches follow the algorithm to obtain the final result of Figure ~\ref{fig:rover_V2}. The pruning approach of [] is used to prune the branches as well as a linearization approach. In this method we take all non-linear decision nodes and convert them into their equivalent linear nodes. As an example of the \MarsRoverNL we have the following:

{\footnotesize 
\begin{center}
\begin{tabular}{r c c c l}
&
$\begin{cases}
(96.4 + x^2 -20*x>=0): 0 \\ 
\neg(96.4 + x^2 -20*x): -2410 - 500*x - 25*x^2\\ 
  \end{cases}$
\\ \\
&
  $\begin{cases}
(-11.897366596101026 + (1 * x) >= 0): 0\\ 
(-8.102633403898974 + (1 * x) <= 0) \wedge \\
\neg(-11.897366596101026 + (1 * x) >= 0):0\\ 
(-8.102633403898974 + (1 * x) >= 0) \wedge \\
\neg(-11.897366596101026 + (1 * x)<= 0):\\
 -2410 - 500*x - 25*x^2\\ 
  \end{cases}$
\end{tabular}
\end{center}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CSA-DP sketch of algorithm using marsrover example
\begin{figure*}[t]
\begin{center}
\includegraphics[width=0.7\textwidth]{Figures1/maximum_cont_action.pdf}
\end{center}
\vspace{-3mm}
\caption{%\footnotesize 
Continuous maximization in CSA-DP using one branch of \MarsRoverNL example\
} \label{fig:rover_algorithm}
\vspace{-3mm}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% V2 for marsrover
\begin{figure*}[t]
\begin{center}
\includegraphics[width=0.3\textwidth]{Figures1/v2.pdf}
\end{center}
\vspace{-3mm}
\caption{%\footnotesize 
The final value function for the second horizon of the \MarsRoverNL example\
} \label{fig:rover_V2}
\vspace{-3mm}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Empirical Results}

We implemented the CSA-DP algorithm using the XADDs and tested it on several domains. The first which has been used as the example throughout the paper is the \MarsRoverNL, also we have two problems from the OR literature; \InventoryControl and \WaterReservoir. We give the empirical results for these domains next.
\subsection{\MarsRoverNL}
The problem specifications and some parts of the solution have already been presented in the previous sections. Here we take a closer look at the exact value function for three different horizons. 

This plot visualizes non-linear piecewise boundaries which occurs due to non-linear leaves in the XADD ;equivalent to non-linear partitions of the state space. It shows that the Rover can move non-linearly (quadratic) around the target radius (i.e, [-2,2]) progressively to get to the exact point with high precision. It then achieves a constant optimal value after the radius and before reaching the action boundaries in each iteration (i.e, the rover can move at most between [-10,10] in each iteration). After the boundaries the rover moves according to a non-linear function thus achieving a non-linear value which was demonstrated in Figure~\ref{fig:rover}. 

Every step of the algorithm allows the Rover to move from further distances towards the radius, hence the constant step and the non-linear curvature at the boundaries occur iteratively. This example clearly demonstrated the power of the CSA-DP algorithm for non-linear functions.

We note that these results were generated  with the XADD pruning which consists of linear pruning using the LP solver and non-linear pruning using the linearization method.
Without the pruning, even in low horizons, because of infeasible branches, time and space grows exponentially. We have also used the general pruning method of XADDs as in [] for the other domain problems.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 2d plots for mars rover to show non-linear movements
\begin{figure*}[t]
\begin{center}
\includegraphics[width=0.37\textwidth]{Figures1/rover.pdf}
\end{center}
\vspace{-3mm}
\caption{%\footnotesize 
The optimal value function for horizon 1-4-8 of \MarsRoverNL\ 
} \label{fig:rover}
\vspace{-3mm}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\InventoryControl} 
This domain problem has become a benchmark problem for optimization in the OR literature and applies to our general DCA-MDP setting. Business firms often deal with the problem of deciding  the amount of product to order in a time period such that customer demands are satisfied. The firm's warehouse will keep an inventory of this product to deal with different customer demand levels. Each month, the firm must decide on the amount of products to order based on the current stock level.
The order should not be too high since keeping the inventory is expensive, nor should it be too low in which case it will be penalized for being unable to meet customer demands and leading to loss of customers. The optimization problem faced by the firm is to find an optimal order policy that maximizes the profit.

We present a simple formulation of this problem where the capacity of the inventory is C units of each product and customer orders not satisfied in this month are backlogged for the next month, so inventory can take negative values. We consider two cases, a one product inventory with one order action and the other with two products that needs two different orders.

We take two continuous state variable $x_1,x_2 \in [-1000,C]$ indicating the current inventory quantity into account, with the total inventory capacity of 800, and a stochastic boolean state variable for customer demand level $d$ where $d=0$ is low demand levels (50) and $d=1$ is high demand levels (150) according to some probability. \\
The continuous action variable is the order quantity $a_1,a_2 \in [0,C]$ which can at most take the value of the maximum inventory capacity. \\

We define an immediate negative reward for the cost of producing an order and the storage cost of holding the products in the inventory and also a positive reward for fulfilling the customer demand whenever there are enough stocks in the inventory. The transition for one of the state variables and reward function are defined below:
%\ because of backlogging, the customer demand is always taken from the current stock level, leaving it negative
%\ consider the next state as the current state and orders minus the customer orders
%\ customer demands are boolean of low and high that change in every iteration
%\ 

{\footnotesize
\begin{align*}
x'_1 & = \begin{cases}
d  \wedge (x_1 + a_1 + x_2 - 150 \leq 800) : & x_1 + a_1 - 150 \\
d  \wedge (x_1 + a_1 + x_2 - 150 \geq 800) : & x_1 - 150  \\
\neg d \wedge (x_1 + a_1 + x_2 - 150 \leq 800): & x_1 + a_1 - 50    \\
\neg d \wedge (x_1 + a_1 + x_2 - 150 \geq 800): & x_1 - 50    \\
\end{cases}\\
d' & =  \begin{cases}
d     : &(0.7)\\
\neg d: &(0.3)\\
\end{cases}\\
R & = \begin{cases}
(x_1 + x_2 \geq 900) \wedge d\\
150 - 0.5\cdot a_1 -0.4 \cdot a_2 - 0.1\cdot(x_1 + x_2) \\
(x_1 + x_2 \leq 900) \wedge d\\
(150 - (x_1+x_2)) - 0.5\cdot a_1 -0.4 \cdot a_2 - 0.1\cdot(x_1 + x_2) \\
(x_1 + x_2 \geq 300) \wedge \neg d\\
50 - 0.5\cdot a_1 -0.4 \cdot a_2 - 0.1\cdot(x_1 + x_2) \\
(x_1 + x_2 \leq 300) \wedge \neg d\\
(50 - (x_1+x_2)) - 0.5\cdot a_1 -0.4 \cdot a_2 - 0.1\cdot(x_1 + x_2) \\
\end{cases}
\end{align*}}

% write description in case we have to take this huge case out! 
The transition for the continuous actions partitions based on the maximum capacity of the inventory (for both products), and only adds the orders if the current total capacity (with respect to the orders of that product and the stocks available for both products) are less than this maximum capacity. 

The demand variable is transitioned stochastically and the reward function is based on the demand levels and the current stock in inventory. If the current stock is larger than the total inventory, we get the reward for fulfilling the demand $(e.g 150)$, if the demand is high and the inventory is not high enough, then the reward is $(e.g (150 - (x_1+x_2)))$, in both cases the action costs and holding costs are also added. This allows the inventory to stock as many products as possible while not exceeding the capacity of the inventory. 

We plot the results of comparing a 1-product \InventoryControl problem with a multi-dimensional one. Figure~\ref{fig:invC} compares the time and nodes for different iterations for these two problem instances and a third comparisoon for the effect of not pruning on the 1D instance. This demonstrates the impact of having multiple constraints and action variables on the problem size which requires much more state-action partitions. The time and space have increased from the first iteration up to the third iteration for the 2D problem size, but then dropped for the next horizons due to pruning the XADD in our algorithm. As more constraints got added in for horizon 4, they cancelled the effects of some of the previous branches because of infeasibility and the pruning operation allows the XADD to grow smaller in space and requiring almost a constant time depending on the constraints added in each horizon. Without considering pruning, even the 1 product problem instance quickly falls into the curse of dimensionality problem and as the figure shows, time and space grow non-linearly after the second iteration. 
% any need for 3d plot? For the 1 or 2 product case?
% any thing else we need from this example?
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%figure5 : time-iteration and space-iteraton for 1d-2d-noPrune inventory
\begin{figure*}[t]
\centering
%\subfigure{
\includegraphics[width=0.4\textwidth]{Figures1/space1-2.pdf}
\hspace{5mm}
\includegraphics[width=0.4\textwidth]{Figures1/time1-2.pdf}
%}
%\vspace{-3mm}
\caption{%\footnotesize 
Space (\# XADD nodes in value function) and
time for different horizons of CSA-DP on \InventoryControl\ 
Comparing 1D, 2D and no-pruning}
\label{fig:invC}
%\vspace{-3mm}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{\WaterReservoir}The problem of \WaterReservoir needs to make an optimal decision on how much and when to discharge water from water reservoirs to maximize hydroelectric energy productions while considering environment constraints such as irrigation requirements and flood prevention. 
A multi-reservoir system is more desirable than the single reservoir problem due to its ability in controlling various environment parameters such as flooding.  In these systems, the inflow of downstream reservoirs are effected by the outflow of their upstream reservoirs. In the OR literature, this case  is considered much more complex and for the sake of simplicity mainly the single case in considered. For multi-reservoirs the main problem that leads to approximations to DP methods or using other simplifications is the curse of dimensionality. In this domain the discharge action is considered as a discrete action  and so are the energy demands. This causes the state space to grow exponentially in case of multiple states and actions[].
Using our method for continuous action value iteration, we show that this problem can be handled efficiently and is scalable to multi-reservoir problems. Consider a two-level reservoir with the outflow of the second reservoir added to the input of the first reservoir. The state space is the level of water in both reservoirs as well as the energy demands and inflow (such as rainfall or streams) to the reservoirs $(l1,l2,i,e)$. We consider the water levels as continuous variables, the inflows to both reservoirs are the same which depends on a high-low sessions of rainfall. The energy demands e can be considered both continuous or discrete for different customer needs. 

We want to prevent the upstream reservoir ($l2$) from reaching low water levels, while avoiding flooding which is caused by high water levels. A constant amount of water is discharged from the downstream reservoir($l1$) to meet customer electricity demands. The inflow to both reservoirs is assumed to be the same (same amount of rainfall) 

The reward is assigned based on preventing the flood. If water levels prevent flooding, a reward is given for the \textit{time} the reservoir selects the action of discharging water from one reservoir to the other($l2 to l1$), or the choice of not discharging at that time, depending on the water levels. The elapsed time is used as the reward value if the constraints are met, in this case the system should choose to perform an action (drain or no drain) as latest as possible. 
This elapsed time is added to the current time at each transition. The water levels are transitioned based on the previous levels and the amount of discharge and inflow multiplied by the elapsed time. This means that higher elapsed time is desired to maximize the reward, while lower ones ensure flood prevention.

In order to demonstrate the effectiveness of planning optimally with our model, we use this continuous-time example of the multi-reservoir where the elapsed time is defined as the action parameter to drain a reservoir. The goal is to show a form of value function refinement that is generated using this CSA-DP algorithm. The optimal value function will obtain higher values for higher horizons, while the value for lower horizons were less than the current value function. This is demonstrated in Figure~\ref{fig:reservoir} where three value functions have been presented in different horizons. 

Here flood prevention is ensured in each step by the linear piecewise boundaries on the very high and very low levels, the action of discharging occurs here either at time 0 for very critical points or at a time depending on the water levels later during the iteration (i.e, $a = -8.18 + 0.0018*l1$ if one of the water levels is above the critical level in the third iteration). As for the constant section, this piece is the safe range and the action of discharge (or not discharge) is left till the latest possible time thus achieving higher rewards. As the iteration number increases, the future discounted reward goes higher than the previous horizon. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%figure6 : 3d plots of refinement planning for 3 horizons
\begin{figure*}[t]
\centering
\includegraphics[width=0.33\textwidth]{Figures1/ref1.pdf}
\includegraphics[width=0.33\textwidth]{Figures1/ref3.pdf}
\includegraphics[width=0.33\textwidth]{Figures1/ref6.pdf}
\caption{%\footnotesize 
Exact optimal value function for \WaterReservoir domain
}
\label{fig:reservoir}
%\vspace{-3mm}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Work}
Markov Decision Processes have been used as the framework of many planning problems that consider uncertainty with continuous resources ~\cite{bresina02}. Many of the previous works have considered continuous state spaces and built exact or approximate Dynamic Programming algorithms to solve the MDP problem. Among these works, the most relevent work to our work is ~\cite{sanner11} which considers solving the MDP for discrete and continuous state variables. 
Here we offer continuous actions such as time or movement for our domain. The theoretical framework of DCA-MDP is similar to ~\cite{Rachelson08} for considering continuous state and action spaces and the mathematical proofs for extending the MDP framework are provided here, but we presented practical application of this framework using XADDs and the CSA-DP algorithm. 

Most work in the continuous action area consider time-dependent models (TMDP) and consider time as a continuous action and solve the MDP problem. In ~\cite{boyan00} time is considered as a continuous variable and the exact solutions exist, but they are limited to the piecewise linear value function setting. 
Sampling ~\cite{Weinstein2010} and approximating ~\cite{pegasus} the continous action space is the most common solution. 

\section{Conclusions}


\bibliography{exact_sdp}
\bibliographystyle{aaai}
\end{document}
